---
up: "[[2024-W43]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20241025095801
modified: 20241028090504
aliases:
  - Friday - October 25th 2024
linter-yaml-title-alias: Friday - October 25th 2024
title: Friday - October 25th 2024
id: 10
week: "[[2024-W43]]"
yearly: "[[2024]]"
quarterly: "[[2024-Q4]]"
monthly: "[[2024-10]]"
daily: "[[2024-10-25]]"
month: "October"
weekday: Friday
---

# Friday - October 25th 2024

## Memos Personal

## Memos Work

## Working On

https://medallia.slack.com/archives/G8YR81G49/p1729866750247909?thread_ts=1729780500.369299&cid=G8YR81G49

```
# NFS Configuration
## These values can be overridden per instance in overlay values.yaml
## If not provided, values from allDC configuration will be used
## workDirNFSServer - The IP address of the NFS server for shared work directory
## workDirNFSPath - The base path on the NFS server for shared work directory
## feedDirNFSServer - The IP address of the NFS server for feed directory
## Example:
## workDirNFSServer: 10.42.37.4
## workDirNFSPath: /sharedworkdir2
## feedDirNFSServer: 10.42.37.4
```

---

---

Bounce node state: Waiting for pod termination

- Estimated max startup time: 1h
- Elapsed time: 7m 12s
- Remaining time: 52m 47s
- Current State: Waiting for pod termination
- State Type: Process State

It's not relevant what the startup

---

Should be getting the express admin status of all the nodes before the pod even goes down.

---

If the flag is used to watch the remaining node bounce, it should not say complete if the monitoring fails

Only if that is false.

---

```
varsamisktest-fe1                                    2/2     Terminating   0          9m9s    10.197.34.121    den-r15-u31   <none>           <none>
```

- Make sure that we catch if it is stuck in terminating. Might need to notify the operator to delete the pod in crio.

---

Task Retry and State Persistence  
Current limitation: Restarting a failed task restarts the entire process  
Proposed solution:  
Store state information in VARS  
Allow tasks to resume from last known state

---

Task Behavior Options  
Suggestion: Add parameter for task submission  
"Fire and forget" (default)  
"Wait for ready state"      
Rationale: Different use cases for bouncing nodes

---

[express/express/src/main/java/express/web/admin/platform/NodesPumpProgressCommand.java at master · Express/express](https://github.medallia.com/Express/express/blob/master/express/src/main/java/express/web/admin/platform/NodesPumpProgressCommand.java)

```
getSyncState
```

---

```
2024-10-25 15:19:32,436 DEBUG tunnel stopped kubectl proxy on port 63704
2024-10-25 15:19:32,439 INFO step_bounce_node Retrieved cluster state:
- varsamisktest-fe1: SYNCHRONIZED
- varsamisktest-fe2: SYNCHRONIZED
- varsamisktest-fe3: SYNCHRONIZED
- varsamisktest-fe4: SYNCHRONIZED
2024-10-25 15:19:32,440 DEBUG abstract_step updating step state to: PutStatusBody(stepName='bounce_node', stepState='RUNNING', updatedAt='2024-10-25T20:19:32.440334Z', stepNameExtra='', stepTargets=[], stepStateDetail=['Monitoring cluster state for node: varsamisktest-fe1', 'Current cluster state:', '➡️ varsamisktest-fe1: SYNCHRONIZED', '   varsamisktest-fe2: SYNCHRONIZED', '   varsamisktest-fe3: SYNCHRONIZED', '   varsamisktest-fe4: SYNCHRONIZED'])
```

```
024-10-25 15:20:05,186 INFO tunnel stopping kubectl tunnel to pod/varsamisktest-be in tenant-123634-prod
2024-10-25 15:20:05,187 DEBUG tunnel stopped kubectl proxy on port 63759
2024-10-25 15:20:05,190 INFO step_bounce_node Retrieved cluster state:
- varsamisktest-fe1: DOWN
- varsamisktest-fe2: SYNCHRONIZED
- varsamisktest-fe3: SYNCHRONIZED
- varsamisktest-fe4: SYNCHRONIZED
2024-10-25 15:20:05,190 DEBUG abstract_step updating step state to: PutStatusBody(stepName='bounce_node', stepState='RUNNING', updatedAt='2024-10-25T20:20:05.190500Z', stepNameExtra='', stepTargets=[], stepStateDetail=['Monitoring cluster state for node: varsamisktest-fe1', 'Current cluster state:', '➡️ varsamisktest-fe1: DOWN', '   varsamisktest-fe2: SYNCHRONIZED', '   varsamisktest-fe3: SYNCHRONIZED', '   varsamisktest-fe4: SYNCHRONIZED'])
```



```

I learned something interesting with the shutdown protection btw

so I thought once you made the mistake, you had no choice but to kill the pod (which is annoying to do with crio)



but I realized after I made the mistake with fe1 the other day, that admin was still up


so I was able to just go in there and run admin command to turn it off and let the pod delete



Is there ever a case where after it's been done, you cannot reach admin to undo it (to free the node)?



I think once the server is doing the shutdown, sometimes you cannot reach the servlets so admins are unreachable



ah good to know
```


---


### . Add Graceful Handling for Restarts

Since the container could restart within the allowed threshold, ensure that each restart doesn’t overwrite the startup probe calculation. Kubernetes doesn’t reset startup probe counters unless the container restarts after failing the probe, so the elapsed time should take each restart's start time (`started_at`) as the base.


### Accessing Probe Configuration for Other Probes

For more flexibility, consider retrieving information from other probes readiness. You can do this similarly by accessing `container.readiness_probe` or


Here’s a prompt you could use for Claude to make these updates:

---

**Prompt:**

"I have a Python function, `_get_startup_config_from_pod`, which retrieves startup configuration data for a container in a Kubernetes pod and calculates remaining startup time based on its startup probe.

I'd like to improve this function by focusing on container state rather than pod state, since container restarts don’t reset pod uptime. The function currently calculates elapsed time from the latest `started_at` timestamp, which is the container's most recent restart time, allowing us to handle multiple restarts accurately.

Could you update the `_get_startup_config_from_pod` function to:
1. Retrieve `started_at` from `container_status.state.running.started_at` and store it in `StartupConfig`.
2. Use the `period_seconds` and `failure_threshold` values from `container.startup_probe` to calculate `max_startup_time()` and `remaining_time()`, considering each container restart.
3. Add any necessary error handling or checks for null values if `started_at` or probe information is missing.

Here is the original code for reference:

```python
def _get_startup_config_from_pod(self, pod: V1Pod) -> Optional[StartupConfig]:
    try:
        container = next(c for c in pod.spec.containers if c.name == "express")
        if not container.startup_probe:
            return None

        # Find the express container status to get accurate start time
        container_status = next((c for c in pod.status.container_statuses if c.name == "express"), None)
        if not container_status or not container_status.state.running:
            return None

        return StartupConfig(
            period_seconds=container.startup_probe.period_seconds,
            failure_threshold=container.startup_probe.failure_threshold,
            creation_timestamp=container_status.state.running.started_at
        )
    except StopIteration:
        log.warning("Express container not found in pod spec")
        return None
    except AttributeError as ex:
        log.warning(f"Failed to get container status or startup probe configuration: {ex}")
        return None
    except Exception as ex:
        log.warning(f"Unexpected error getting startup config from pod: {ex}")
        return None
```

Additionally, could you refactor the `StartupConfig` class to include methods for `max_startup_time()` (using `period_seconds * failure_threshold`), `elapsed_time()` (from `started_at`), and `remaining_time()` (subtracting `elapsed_time` from `max_startup_time`)?

Thank you!"


---


- Need to revisit init step of `MecBounceNode`
- Need to split `monitor` step into it's own file to be consistent.
- Need to be able to handle either gitops cluter or regular cluster.
- Need to adjust parameters on the SRE API side



---


```
2024-10-25 18:10:30,443 DEBUG tunnel waiting for local port 58603 to be live
error: unable to forward port because pod is not running. Current status=Pending
```


```
varsamisktest-fe1                                    0/2     Init:1/2    0          53s     10.197.34.121    den-r12-u32   <none>           <none>
```


```
2024-10-25 18:34:55,404 DEBUG abstract_step updating step state to: PutStatusBody(stepName='bounce_node', stepState='RUNNING', updatedAt='2024-10-25T23:34:55.404085Z', stepNameExtra='', stepTargets=[], stepStateDetail=['Monitoring cluster state for node: varsamisktest-fe1', 'Current cluster state:', '➡️ varsamisktest-fe1: DOWN', '   varsamisktest-fe2: SYNCHRONIZED', '   varsamisktest-fe3: SYNCHRONIZED', '   varsamisktest-fe4: SYNCHRONIZED'])
2024-10-25 18:35:17,975 INFO tunnel stopping kubectl tunnel to pod/varsamisktest-fe1 in tenant-123634-prod
2024-10-25 18:35:17,975 DEBUG tunnel stopped kubectl proxy on port 60258
2024-10-25 18:35:17,976 DEBUG misc got exception failed to run admin command 'status' on None with: failed to start kubectl tunnel in time with: Failed to start tunnel in time, sleeping for retry
```

