---
id: 9
up: "[[2023-W49]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20231207000100
modified: 20231208112028
aliases:
  - Thursday - December 7th 2023
linter-yaml-title-alias: Thursday - December 7th 2023
title: Thursday - December 7th 2023
week: "[[2023-W49]]"
yearly: "[[2023]]"
quarterly: "[[2023-Q4]]"
monthly: "[[2023-12]]"
daily: "[[2023-12-07]]"
month: "December"
weekday: Thursday
---

# Thursday - December 7th 2023

## Tasks

%% TCT_TEMPLATED_START 2023-12-07 00:00 %%
* Home ==-
    - [x] Make a dating app profile ✅2023-12-07
* Recurring
    - [x] Gratitude - Morning ✅2023-12-07  
%% TCT_TEMPLATED_END 2023-12-07 23:59 %%
* ? Did these tasks align to your Goals?

# Rollover

# Daily Notes



and we'll insist on having the individual JIRA tickets in place for all the clusters, we should be backwards compatible with that. So what I basically did, I found that the easiest option to have that is to simply have a switch in the original JIRA ticket saying, create individual JIRA tickets true-false. And based on that, I had to implement one major change in JASP, basically having a separate mode that would read or execute some kind of function. And based on that, based on the output, it would process the ticket with either one mode or another based on the value. So that's basically it. So it basically goes that way or that way based on some kind of decision function. So translated to a human language, it's like the user will just say, create JIRA tickets for that deployment, yes or no. And based on that, one or another mode will process the input. And similarly, it works the same for the reconciliation because so far, we did the reconciliation for the JIRA tickets that created all the individual JIRA tickets. While we won't be having that probably for those JIRA-less tasks. But that's yet to decide. And the other thing I did was creating the ingress for Argo events. So it should be working now. Nice. And the CNAME, I see, as well. Yeah, I spent some time with the network services guy because there was some kind of misunderstanding. You know, guys, it's like when you want to have an ingress object for something that's internal, you basically select the Citrix internal ingress class. But it's not true for DNS, basically. If you want to have a DNS entry, and they also work with ingress classes in the sense that they need to tie the DNS entry to a load balancer. So if you want an internal access, you don't tie the DNS entry or CNAME to Citrix internal because that's, according to them, available just in the DCs. So you basically need a public DNS entry when you need an access to your service from Medallion Network or VPN. So it's kind of new to me. So yeah, it took some back and forth with the network services. Hold on. I didn't get that. No. Because, OK, you use the internal. And if you use the internal class, they mean only within the cluster or within the data center. That's what you're saying. Yes. If you use the, OK. They say that if you use the internal, what do you call it? Class. Class. Citrix. Or zone, or whatever they call it. I think classes, they call them. They call it class, yeah. So if you use the internal class, that's available only within the DC, right? So you basically won't resolve the name if you use that class. So you need to have a public class. And it means that now if someone outside Medallia, like anyone, tries to resolve argo events.deal1.medallia.ca, they will get the result. Yeah, they will not get access. They won't get access, but it will resolve perfectly. Yeah. So you use an external DNS record. Yeah, I don't. They do. No, because I think that's the way we did it also in another service. I need to do it. And I think the, anyway, it's that way, I think, for me as well when I was doing something similar. OK.



---


MEC Deployment


to config and then because some, I think Louise was saying that the express operator had some issues in OCI. And I remember that Matti was saying in the previous meeting that probably if you want to orchestrate things that should be done in the operator. And I guess, but I guess we need to agree that we're going to use this instead of like Prod Deployer, right? This is what is happening today in Colors, right? We're using Prod Deployer. Yeah, we're using Prod Deployer to get custom resources to like manipulate them that are managed by the express operator. Well, while I agree with these three points, it seems to me that there will be a disagreement on this among all the members that is going to be there on the meeting, especially Larry. No doubt. Yeah, yeah. Look, we don't… Yeah, yeah, sorry, sorry. Completely finish what you want to say, sorry. Yeah, because what I learned from the previous meetings was that they simply are not ready to get rid of the Prod Deployer altogether. What they would be willing to do is to build some kind of service like our API that would be more user friendly. And what they will do under the hood, it's another question, right? But getting now rid of Prod Deployer is while, again, while I would wish for that, and I would 100% support that idea, I don't think that it's gonna happen now because that's what I felt from the previous meetings. Yeah, if I may. Sorry, yeah, yeah, sure, sure. Yeah, I just want to comment a little bit. The meeting that we are going to have is only with CIF and this project will be driven by Hiram Harami, okay? But by the two teams, I mean, by CIF and the provisioning team. Larry is not invited to the meeting because we found out that Larry has a different opinion of how we should handle the MC deployment. So we are leaving this in, that this is going to be a project between these two teams, okay, or team and team. And what Ese is expecting is a proposal, okay? Something that we could share with him and would make sense for at least the production clusters, okay? I know what Larry thinks about this and what is his idea of how we should be using the protocol, right? But I believe that we have the support from ESE to make a change. So let's focus on that, okay, Tomás? I'm not going to involve Larry at any level. Yeah. Well, that's nice. My question is, does this audience or do these people have the mandate to make some kind of decision? Or is it like we're going to propose something and then Mati will come with Larry and they will simply say, no, this is not feasible because it's, you know, this and that. Yeah, I don't think that. I believe that Mati has is more in our side than Larry's side. And about Larry, that is something that ESE should take care of. Okay. Yeah, we should propose something that could work and make our life easy. Yeah. For the rest, I believe that we have the support from ESE so he should decide what way we should take, okay? I'm not concerned about Larry because actually today I don't feel like he is working on anything new for the bot one, okay? But let's say, okay, I'm mostly sure that if we propose something that is better of what we have and could solve the competition that we are having, I don't see the issue to implement that. Yeah, so, oh, sorry. No, no, go ahead. Okay, yeah, I just wanted to mention because I forgot to mention what is the purpose of this meeting, right? So the purpose of this meeting is as Luis was describing, we want to have a nicely formatted proposal and we should say what we think would make things better. Like that's what we want. If it's gonna happen or not, we don't know yet but we need to have a proposal that will say, okay, we want one, two, three and that's what we believe should happen so we make things easier, that we can deploy MSC clusters easier and so on. Now, if we, yeah, sorry. Well, I think that, yeah, I agree with that but I'm afraid that if we, and I don't wanna be, I don't wanna sound like, you know, I think that, yeah, pessimistic, too pessimistic. So I think that if we come with some kind of proposal that would say that we're getting rid of Prod Deployer altogether, we're replacing it with some kind of new service or reusing Express Operator, then, you know, that Larry will step in and again, sorry mentioning him but he will definitely step in and say, this is not feasible because of this and that. This won't work. I don't know, you know, it's up to Eze to make the call, make the decision and we should be ready to, you know, when we propose something, we should be ready to be able to support it in every sense, meaning that we are perfectly aware what a Mac deployment entails, what are all the corner cases, what, you know, how detail it should be, things like that because otherwise, there are people that will tell us that, hey, this is not feasible and we need to stick. Yeah, yeah. Look, I think our approach though should be that we, first of all, we shouldn't take no for an answer. The other thing is we should push as much as we can because that's what we think it's gonna make things better. But on the other hand, I give it to you and like, I don't say that the way we work it right here, it might not be the right one. But what I wanna say, let's say with bullet point one is that to move away from cluster config and prot deploy is what is happening today in OCI, if I'm not mistaken. From my understanding up to now, this is what's happening. So we have right now in OCI clusters that are deployed without the need of a cluster config and prot deploy. Isn't that correct? Well, it's correct. Okay, but hold on, give me a sec. And starting from this starting point, we want to align everything. We don't want to have variation on the way we're deploying things. So if we have one example, work functional that has problems, like we know, for example, that the operator has some issues and people in our teams are willing to put development cycles to fix something around the deployments, we should go to the right direction and not like trying to put an API around prot deploy, which would be beneficial. I'm not saying no, if we can interact through an API, I'm not saying no, but is this the right thing to do since we have already something that is operating in a different way? Like that. I agree. I agree with that while saying that the solution with the Express Operator has one big drawback and it's the big blast radius, right? And it has happened in the past when, you know, the Express Operator was misbehaving and it basically, you know, destroyed all instances in the whole DC. Well, that's true. And I agree that the operator is a big blast radius, but I feel that like where we're at in terms of like the sandboxes and the QA all using it, if we're gonna have one commonality between all of the Express instances, the one commonality that I would choose, even though I dislike the Express Operator as it currently is, I would choose the Express Operator. Even though it's possible to define the resources as a staple set, I think that will be like ideal as the common, I guess that's my thought on it. But it took me a while to get to that point. Cause I was like, why are we using this? And I don't, I think we need something or we'll just be like using like Production Express will just be its own thing. And the rest of the environments will be using something else. Well, I don't, I don't think it, well, two things. First one is that I'm just, you know, question or challenging if we should like propose the Express Operator thing, like from the beginning, if we know that, you know, it has this big drawback with the blast radius, or if we should, you know, while at this point where we want to propose something new, if you should think of something else without this, you know, major drawback. And the other thing is that I'm really, I don't think it should be a big issue to have multiple ways of deploying things like tied to particular type of clusters, like production, production instances and sandboxes. But we should have a unified, unified API or whatever you want to call it that would be facing the user so that the user shouldn't care what they are deploying, you know, or how they're deploying or how we are deploying the thing, provided that they have a, you know, simple unified way how to request the deployment. But this, so this already kind of exists. It was like, they do use Graphic Deployer to deploy all QA environments. And the rest of the company is encouraged to start using it. And I know that for a fact that there are applications like LivingLens or like CrowdDCD, I think, TechMarket that do use the Graphic Deployer to deploy their infrastructure, right? So that part does exist, but Mac cannot use it right now because it's not a standard Kubernetes service. And since we have customized deployments, custom resources defined, and the problem with them is the operator, I kind of think that that's where we all need to focus since for now, at least from the things I know. And as I was saying before, like if we're willing to spend development cycles to fix stuff, we should all agree to fix a specific thing. And have a common goal. And that's what we're trying to figure out. At least we're saying what our requirements are. Like, I would say we want to, we can rephrase the first bullet and say that we do not want variation on how we deploy things. We want to have one way, right? And maybe you can make this to be the number one right, we want to eliminate variation. And this should be, and we need to all agree, you know, it should be a side inner bullet, you know, let me do it, let me do it. So, I mean like, maybe we can rephrase this and say, hey, that's what we want. We don't want variation on how we deploy things.



---


there are these custom resource definitions, right? And all of the instances will use that custom resource. And if you make a error or a bug in editing the custom resource, the operator will basically apply that bad version to all the running instances. So it's not the express operator itself. It's rather the design of the Kubernetes way of applying changes to resources supported by a custom resource definitions. At the DC level, like basically from our perspective. Hold on. So the problem is, let's say we make a change to the custom resources, sorry, to the express operator, right? And we are afraid that if we apply this change in a cluster, all of the clusters, sorry, to a data set, to a data center, we're afraid that or to a Kubernetes cluster, if we apply that change, that we're afraid that it's gonna affect all the MEC clusters at once, right? That's what we're saying. Yes. Okay. So this though, is not a problem with the solution itself. This is a problem of the way we develop things and we add things to it. This is a problem of the pipeline of the express operator itself, which means that if you have proper testing, proper tests written, integration tests written, you have a proper development environment, you have a proper staging environment and your production is separate and discrete from those, that means you can catch errors in the staging environment and you can affect only your staging clusters, let's say. So that's, it's a strategic problem. It's not a problem with the fact that it's an operator. I agree to a certain extent, yes. If you have a proper testing in place so that you deploy everything that's properly tested and nothing before that, that's true. While deploying something that, or updating something in production that may affect all that's running there, because you're not dealing with a single instance, like now when you spin up a pro-deployer instance and say, hi, I want to do something with that instance. There is no chance that you would affect all other instances at once, right? Well, with the express operator, and I'm not proposing anything. I'm just thinking loud now. It's like when you deploy a new version of custom resource, there is this risk that you're affecting basically everything. Because if there is a chance that someone will be making changes in production in a way that they will change the running instances, like somehow manually, and they will be in an inconsistent, non-standard, status or fashion, like what we do now, because there are lots of ways how to fix or patch or edit the running instances. So you can't be sure that whatever works in the test or stage environment, it will be the same in production environment, right? Because the instances may simply live their own lives there and be in a different shape that you can't simply predict. But I'm not saying that this cannot be, you know, 99% tested or, yeah, I'm just thinking loud that this may happen because you're deploying something that may affect. Sorry Kamil, what I'm not getting is like, in what way they are different? Like those MC clusters that you're referring to that might be harm, in what way they differ from other clusters that will not be harm? Like what can, I don't see, what can go wrong? No, well, I've, I'm not an expert on managing, on managing running Mac instances, but I know that there are usually a lot of cases when something is not working, you know, properly by running pro-deployer instance, by editing XML somewhere in the cluster config repo. And now Murtaza steps in and fixed it somehow. I don't know how. That's a very good point now that you'reYeah, I don't know if this kind of action brings the instance into some kind of inconsistent or non-standard shape, right? That we will be, you know, 100% sure that if we apply a new version of custom resource definition, that this particular instance won't be affected by that, simply because it's not in a standard shape at this moment, right? Yeah. Okay. I understand now what you mean, that somebody goes and does some change, because this is for some reason to fix some problematic situation and so on. This is actually something that I witnessed yesterday happening in front of my eyes. And again, but again, this is a strategy problem, because, and that's what I was telling to the guys in the previous meeting. This should stop, because we cannot control it, you know? Definitely, I couldn't agree more. This is something that's been here for a long time, that, you know, all the production, or not, like the production instances have been managed in so many different ways. But yeah, I agree, this should be there, simply there should be a one way how to, you know, do the, or at least all the basic operations that we may think of when dealing with running instances, like deployments, you know, rebuilding caches, things like that. So if we could support all these, I'm sorry, I'll just finish. If we could support all these tasks, all these like standard tasks for running instances with the operator or with our API or a service, or whatever it's gonna be, then there should be a political, you know, mandate that would say, hey, it's not allowed to, you know, interfere with the instance with any other way. Yeah, and that's what I was trying to tell you with the variation here. This is a big problem, because this is also variation. Variation on how, it's not a problem if somebody goes and does some manual intervention to fix an issue, right? The problem is how this is happening. And maybe if you want, like, we need more standard operating procedures for the people that can do that, like Mortasa or whatever, which means like if you do something on the cluster itself, and it's not reflected, for example, in cluster config, for the clusters that are still using cluster config, you need to go and update it, right? Because you're gonna break things. And I agree with you fully, but this is not only, but as I said, it's a good thing to have in our requirements, right? Because it's not our job to figure out what this SOPs would be and how would they look like and who would manage them. But what Eze is trying to accomplish is like, maybe it's his problem, you know? Maybe it's Eze's problem, how he's gonna talk to the managers of those people that can go and do manual changes, right? But it's a good thing to document it. But again, I think that this has to do with the fact that right now you can do many things in many ways. Like the same thing in many ways, let's say you can accomplish it. And that's problematic, that's what we need to fix. Yeah, that's true. And another thing is that there are configuration of clusters is kept in lots of different repositories, right? And they are not always in sync. And we, if you go through any, or basically almost any of the logs from our GroovyScripts execution tasks, you can see that the Python complains about a cluster being configured in multiple places in a different way. Cluster config and some other repo, I can't recall the name right now. You know, and that's a problem because no one retrofits the, no, no one. There are cases when someone doesn't retrofit their changes that they do to all the repos that they should. So, yeah. Yeah, but that's again, like as you said, like it's a process problem. Like if they want to make, if either the leadership should say that this is not allowed except in very, very rare cases that we need to fix something, but that thing should be documented that it happened. Like, for example, to give you an example, yesterday, Murtasa came, he bounced the servers, right that left. But I was like, okay, why are we doing that? Our problem is not that we cannot, we need Murtasa to bounce it or whatever, because we needed to document, first of all, that probably we need to increase the limit of the, the limit of the, sorry, the time of that specific task, right? We need to figure out what it means that a node is redeployed or not. Plus we found a bug that was there that because our pull requests were doing changes in their own path. So we were, and the changes were never merged. So if we didn't do this exercise yesterday and we said, okay, Murtasa came and like, he fixed the issue, right? And we didn't document anything. We wouldn't know about these three bugs that we have in our code. And that's also like a mentality or a change of approach to things, let's say, and like that needs to happen also. But again, anyway, I, okay. We kind of agree that we hit a, a nail on the head, I would say with this. This is, this is a problem, but it's also connected with the fact that we do have variation. So yeah, wait a minute. I think that to, and maybe Luis, correct me if I'm wrong, but maybe to come up with some kind of solid proposal from the, from the meeting next week, maybe we could also onboard Murtasa there just to be sure that we are covering all the use cases that we should cover. Like, I mean, in the sense of operating or managing a running instance. Yeah, I get that. That should be like a different call, right? BecauseYeah, I agree. Maybe for a different audience, right? But yeah, definitely. And let me add something. I would like to think to add controls to segway the cluster status, right? Because of all the things that are happening outside of knowledge, right? At some point that is really good to have for something like that. I don't know whatYeah, yeah. Maybe limit access, another way to start. LikeYeah, that could be, that could be really, really an issue because they are using all these operational tasks for their own, right? So maybe there's a route that could help us here, right? Yeah, I think something like that, for example, because all of these operations are currently locked behind the prod deployer. And because no one wants to interact with the prod deployer, we ask L1 to do it. And so basically all of that, like what's actually happening is been completely abstract, like removed from our knowledge. And we just rely on a team to get in that tool and do it for us. And that's where I think, because like the result of this for so long is why we need to some extent, I know like we want to too heavily rely on, Larry and Murtaza to be able to help us understand why and how things work the way they do, because it's not always like entirely clear. Like just the other day, for example, when we were like, hey, why is the, in the middle of the Met Deploy, why are we doing a disable the cash donor thing? Why are we doing that? They're like, well, it shouldn't be like that. It should be like this. It should be like that. It should be like this. And I'm like, wait, who knows all of this, right? Like, it's not obvious how this works because it's not like a standard application. And it's like, I feel like as much as I don't want, I want to remove reliance on tribal knowledge. In this initial development process of a new way to deploy Mac, I feel like we need to understand why it was set up the way that it was so that we can, you know, re-in reverse it. Yeah, understand the problem. Like that's the first thing we need to do. Yeah. Yeah. Yeah, and really, do we, or is it from the, you know, because of the nature of MEC that we are dealing with all these corner cases? Or is it like, just because we're relying on maybe Mortaza having the knowledge how to fix that? I think a combination of both, yeah. Yeah. So, I mean, the question is, does the new solution, whatever it's gonna be, does it need to support all these corner cases or are we able to perfectly 100% manage Mac instances just with a set of standard operations? I don't know. I think that that's a separate thing, right? Because now we're mixing two things. Like one is like, how do we deploy a cluster? And the other one is like, how do we maintain it? So I don't think it's so separate because, you know, we were talking about possibly deprecating for deploy here. Or maybe we could, you know, maybe we could deprecate it in phases. Like, yeah. Of course. Maybe we could deprecate it just for deployments. And then if you find that it's working, maybe you could go a step further and deprecate it, you know, for all other operations. Yeah. Look, as I was saying before, what we're doing here, it's an exercise. So we're prepared for that meeting to have some solid proposal, let's say with requirements that we have. As I understood from Luis, and Luis can correct me if I'm wrong, this is a big project, right? And of course it will take phases. But what we're saying is like, we're trying now to figure out what we want to do. Then it might take, I don't know, two quarters, three quarters, maybe more. Who knows? And of course we will do it in phases. And of course, like we'll fix one thing at a time. And like, we can start migrating things to whatever new design we agree on and so on. Yeah. One thing I want to also add is like, which I don't know, like how involved that, like the platform team, for example, would be in this discussion. Not like this upcoming discussion, but if we were to come up with a plan in the coming weeks for this project. Because I think, I feel that they have a lot of gripes about how the current deployment process works, at least definitely in production. And it would be beneficial to listen to those gripes. Like they're the customer, you know? And we're building this, we have all these things that deploy Express right now, and it works great for us, not really, but that's what we think. But in reality, it's not working great for them and they have no, like they want Argo CD to be like, in my opinion, well, based on what I've heard at least, to be the deployment mechanism like every other application, but they don't know how to communicate that to us. How to change it. In the bottom line, what we were saying before, and we mentioned Grafiki Deployer, Grafiki Deployer is actually doing this, like giving you a way that you can make changes that will then be deployed by Argo CD. It's like, because we're using GitOps from that perspective, right? But on what you're saying right now, as I said, now we're trying to set requirements, right? Agree with the shift team on what we're gonna do. Then what the next phase, for me at least, should be, we write a document, then we have a big technical discussion with the involving shift team, platform team, I don't know who Larry's team is, like Mortasa in there, everybody, to show them our proposal. And they can comment on this and they can give us a requirement on this. Like we will gather requirements after in a second phase from them, let's say. But of course we need to include everybody that is like involved. Yeah, yeah. But I agree that maybe we should not be too technical now. I'd rather come up with some kind of proposal or a higher level proposal, while still having in mind that if we're deprecating or if we want to deprecate Prod Deployer, then we'll probably be dealing with much more than deployments. Yeah, because it alsoBecause if we start managing instances with something else, then we must support basically everything what Prod Deployer does. Okay, what else does it do? Don't ask me. Yeah, there are all kinds of operations that it manages. Like for example, some of which we actually probably wouldn't have to worry about as much if the deployment method were like changed. Like for example, like unbinding the one node from the cluster and then rebinding it to the load balancer. Like it's just little things like that that are like essentially a part of the Prod Deployer. Even though that's not a deployment specific action, it's still, it's considered like to be a part of the Prod Deployer. And I think we would have to kind of like unravel, like understand like what are the most used like operations. Okay, do they have, sorry, do they have, where is this Prod Deployer? Does it have like a repo? Yeah, let me get it real quick. It's called, what is it called? Because we need to research like and try to reverse engineer as much as we can and like see what they're doing, what it's capable of. Yeah, I share, I share that. I share, I share over this link to a specific page. But I believe that has all the comments. It's called, yeah, working right now. Okay. Prod Deployer, I can't remember what the, it's not, the repo is not called Prod Deployer, it's called something else. I'll find it. Okay, no worries, no worries. Let's, and we will tidy up those nodes like here. It doesn't matter because things are not fitting together now. Like they're kind of, it's kind of messy, but don't worry, we will fix it later. Okay, so we're saying Prod Deployer, right? Thank you. And this one has little problems. It's a, it's a long list of things, right? So, I mean, I guess that, you know, when, if an X, if an operator, express operator or whatever is gonna be managing an instance and, you know, someone will, will want to, you know, run some of these commands, like bind, shared worker, or something. They should simply have a way how to do that. Okay. Yeah, yeah, I understand. Yeah, and we need to define what does binding means and what is this, like for example, because now we know only about bound, binding and binding node, but there might be, as you guys say, like another 20 things that this thing can do. So, yeah. But I think I would, I would, you know, start reviewing if you still need everything. Maybe it's just a, you know, some of the things are there just because, you know, they may be no longer used or no longer applicable to the clusters. I don't know, really. Okay. Before implementing, you know, some of, everything of these, I would go through them and see if, if we need it. Okay.


---


Yeah, that's, that's one thing I want. I mean, if, if, uh, I tried to like, I'll, uh, put it in the chat yesterday, but like what, basically like the way that I see it and I think like a perfect example is that is what we ran into yesterday with that, um, speech thing that it was, it has to make a change to not only like cluster config, but also the configuration repo, and then the runtime config repo, and then this, and then we deploy. Well, what I've been discovering with DCR and not to say that this is the, like, is that it's so much more, uh, straightforward that when we can have all of those things somehow centralized into one place and in the, what, with a place that I've been using is the helm chart and leveraging like overrides. So that we don't have, are not so like, um, reliant on, on, um, the configuration hash, because I think like this, this, uh, like deciding that the configure, like the configuration hash is the thing that we need to, let's say, add a node to a, uh, express cluster or, you know, change some little thing about it. I think that it's, it's a mistake in some sense, because that's inherently like what's going on. It's not, hold on, hold on, hold on, because this is, this is not a mistake. This is one thing that, that, uh, the class that might need. So this is all externalized configuration. So it's things as we have some values that our services need to operate in space, in different data centers, in different, uh, for different, uh, I don't know, targets and so on, or just to read from different cues and, uh, from different issue, issue projects and so on, this is, this is fine. Like this is external configuration and it should be happening. So, and every application has it and, or should have it. Uh, but this is a problem that we need to discuss with the release management team, because that's something they handle that's their scope. Uh, because imagine every, every, uh, as I say, like every version of MEC might need changes in external configuration and that's fine. But, uh, and the classes that you say, maybe classes are not human friendly or readable, but there are other ways they use the commit class, but in other teams use, uh, branches will say for this and the branches are named after the releases. But this is something we need to coordinate with the, with the release management because it's kind of stepping in their world and so on. Uh, so it's not a mistake that things might have external configuration or a problem necessarily. Maybe the way we handle it could be improved. I think, yeah, like just somehow for us to reduce the amount of places that need to be touched and in sync for something to work, uh, like something that would be, I can, like, I know that like, like adding, adding a single front end node to a cluster requires a lot of changes in different places to the, to the point to where we need to create a whole automation pipeline just to make it, uh, worth the time of doing, of attempting. Okay. I understand. But I think that right now we need to focus that we identify what are these places and that all the clusters do the same change in the same way. And then when we nail this down, then we'll go ahead and say, Hey, can we merge whatever is in runtime configuration, merge it with configuration or take everything that's here and like put it somewhere else to have it in one place. That's an improvement. I think that will come in the future. Well, two places for sure that I know that I'll document here. Yeah. Yeah. And that's why I said probably the most important thing for us right now is the first bullet that I changed from before document and gather information about what is the problem looking like right now, where, where do we need to make changes? Where, what is being done and so on? What does it mean if we want to deploy something, where do we, what do we change and where we kind of already know, but, uh, but that's important to understand what, what the problem is anyway. Okay. I think we have enough though, and we can put it together and like, uh, maybe, uh, Kamil, you said you're off tomorrow, right? From tomorrow. No, from, uh, December 14. Uh, okay. Okay. Okay. So we have time. So I will prepare something and I will, I, we can run it again, like go through it like again sometime next week. Yep. And, uh, Louis for you, like I can send it to you and you can review it like before that meeting or something. Yeah. Yeah. Okay. Okay. Great. I think that's all right. Yeah, I think that's good. Okay, guys. Okay, guys. Thank you so much. Uh, remember, uh,


---



```bash
kubectl logs -n argo-events -l 'owner-name=alertmanager-event-logger' -f --context yul1

```

```bash
kubectl logs -n argo-events -l 'eventsource-name=webhook,owner-name=webhook' -f --context yul1
```


---


I cannot say if `[batch.kubernetes.io/controller-uid](http://batch.kubernetes.io/controller-uid)` will stay forever; as for the current version of k8s it **is** the preferred label. However, I can assure you that if it stays, it will remain present in both places.  
The thing with selectors is that they apply to labels, so if any label is in the `matchLabels` you'll need to have them as part of the `metadata.labels` of whatever it is selecting. In this case, the `spec.selector` from the Job must match the `spec.template.metadata.labels` from the Pod template The label `controller-uid` was left for compatibility, I assume, for use cases where the same pods were selected by any other entity than the Job controller itself. (



Certainly, here's a concise breakdown in bullet points for your GitHub PR response:

* The `batch.kubernetes.io/controller-uid` label is currently the preferred label in Kubernetes, though its permanence is uncertain.
* If this label continues to be used, it will be present in both the `spec.selector` of the Job and the `spec.template.metadata.labels` of the Pod template.
* Selectors in Kubernetes work by matching labels. Therefore, labels specified in `matchLabels` must also be included in the `metadata.labels` of the resource being selected.
* The Job's `spec.selector` needs to match the labels defined in the `spec.template.metadata.labels` of its Pod template.
* The continued use of the `controller-uid` label is likely for backward compatibility, enabling pods to be selected by other entities besides the Job controller.


---


%%
DNS works for inter DC communication in Medallia but I know it's in SC4. So SC4 is a unique case and they implement DNS in a different way than others and I guess you might need to add it in the same record add it into the internal zone something like that but I would pingerness about it because I remember we also had some problems with that and while for the for the rest of the data centers like I don't know we created a public DNS record that was pointing to an internal service and that's fine it's like what you were saying like you can resolve it but you you cannot access it you will get like I don't know unauthorized or something in reality if you are outside the VPN but for SC4 it's slightly different things are slightly different because they they implemented DNS. They would tell you what you need to do. Just to be sure it's from SC4 to UL1 not to SC4. Yeah but SC4 is like I don't know there is the resolution of names is like kind of weirdly set up like I don't I didn't I don't know more but I know that SC4 is kind of unique when it comes to that and you can just tell them I have this this DNS and I cannot resolve it from SC4 and they will tell you what to do. I had a you know conversation with one guy from the network services who created the record for me so I can ping him back but I know that for you know we've had this these issues like always in the Provenge and we always like resolve that by injecting the hosts to the hosts table like locally to the to the to the port. So you know I guess but it's something obviously that we cannot do with the Elad manager so well we could indicate to whoever like OE that we need it to be able to send traffic to like different data centers and they could I don't know how high on their priority list that would be but well I think it's like eventually I don't know the the alert manager runs in just a single instance in SC4 right if I'm not mistaken there is no like alert manager local to a to a data center I guess there isn't so eventually we'll be like in production once we have all of this up and running in production we will be sending the requests from SC4 to SC4 right because then yeah probably it's going to be Argo events in SC4 eventually but you know that's you know it's not going to happen until we have everything ready so yeah we definitely need to push alerts from SC4 to EL1 for now and maybe Denver as well. And this is like when you create the what do they call it in the in the rules like when you say okay that's the receiver or whatever like that's so it fails to send it yes I'm think you're here in the in the thread that we have. Huh yeah so I guess in that thread they can ask OE what they think on it if they have any suggestions yeah but let's try network services first if they tell us hey it's simply something that we cannot do I don't know. That's from ah so that cannot do it also no it's it was just an example just running in SC4 yeah can cannot resolve okay something that's in your way I'm telling you it's like some it's something it has to do with the pin next and the way it's set up there can't remember no what's what we did but I you know before this test that I ran with JASP in SC4 I ran the same thing from Denver yeah with the same result but that's with the same result yes I wouldn't say that SC4 is kind of special in this respect but they will that's what they told me I'm quoting them okay okay but okay but it is in SC4 and where is the because I see resolve not resolve only for SC4 and you tried it in Denver and ah but you just don't post it here yeah yeah this is just I can post the example from Denver as well but it's basically the same thing okay but this article which is pointing to the load balancer in v1 right or not the net scalar right yeah well let me get that PR real quick yeah it's going to let me send this so this is the PR I made and before it was just sending to an internal IP address that is in DIN actually directly to that port and now it's going to what Camille created which is just the DNS record that's created for how does that work again the the ingress created it and that's what we're pointing at with that DNS record no no it's a it's a C name for the internal dot lv.l1.medallia.ca here is the jira ticket that I created for the network services to add the DNS record I don't know what they did in you know that just ignore the description that I put in the request I don't know if they follow the same things that I put here well what it seems to me it seems to me that they you know that there is a curl command in the in the command that's probably the one that they used to create the DNS entry so why is it that why is it that the IP address that we had before that's running in DIN it can go from sc4 to DIN no problem well maybe there is no problem with connection there's just problems with DIN resolution just DNS resolution because because you know and in this case we cannot use or we could use IP address maybe but you know that's something I would avoid doing unless we have no other choice can we do it just to prove the point that um well yeah we can we can connect from you know from any pod uh like what's the next time you don't need to go to them like you don't have permissions for a DNS I don't know create it on your own maybe I do I don't know I think I think that we can with the LBOX we can yeah we can because I I used to do it like all the time okay let me grab the pod IP and try to ping it from oh DOS DOS that's what I meant yeah you can just create the records directly with DOS I didn't I forgot about that you just need the token okay okay look but uh so it is just the the way that this record has been created like uh so they created it when connection refused because if if you try to resolve the the the load balancer like URL you can like from from Denver for example but I'm trying so we could just we could technically in that alert manager PR just point directly at that IP of the load balancer yeah no no no no because like then you need to pretend that the you need somehow to because and I don't know you kind of override there is a way to override with headers like what is the source host because it needs that to route to know where to route it to which service in the load balancer to route this or else it doesn't know if you go with IP it doesn't know where to send this package uh okay looks looks like that we can uh curl to the to the port IP from sc4 or to to your one yeah this works yeah so the whole thing is like but I see this isn't like external zone so it needs records to be created for this into the internal zone as well and isn't the what has already been created what isn't that the internal no no that's that's external because it says zones that that because the path is like that like I can show you how it looks for internal okay internal I need to go to the dots uh dot medallion.com okay so internal records records it should be something like internal band zones and so on it looks like that for example you mean the url in the yeah yeah so the urls look like that when you when you want to create for example or get a record it looks like that in the internal zones but then it's like it and I think I think let me see something I can I can set the screen if it goes through so and what IP do we survive is that your one ta records I'm just pulling this off of the uh page that's what the internal would look like yeah and so if you go and then get the zones and because I think you can get if you remove this part you can get zones yeah see it has a medallia.com in Denver for example but if I go to sc4 the problem is that they will not have uh yeah they do they don't have the yeah but they don't have the ca's all here so because like what you could do uh what was the other one you'll want yeah and so because what you could do is like if they had this zone created in sc4 you could do it this way and like create a record for for this in in the internal zone of sc4 but how would we had this problem also I don't remember now how we solved it I need to start using whatever the heck you're using here for it's thunder it's called thunder thunder client yeah it's like it's like a post one yeah it's like a extension of VS code I want to try that okay yeah but I don't know anyway I think if you just talk to them and you tell them like uh hey I need to resolve because we have external an external record is this one records and like it was there I'll ping him he's online yeah because they will for sure they will tell you either what's possible or what's not and like maybe we need to change or add additional FQTNs in the Citrix definition so you can that that will be possible to resolve yeah but just just be aware that they might start telling you some crazy stuff like yeah create this ENG some weird zones like that because they were telling us also okay so ah and see you have ul1 here so this might not it's it's actually internal record that's what they did okay okay now it makes sense because this is not external record this is internal record so if I go to ul1 I don't say what was it okay it was like ul1 medallion right CA and it should be cargo events not fun is it then the one um I'm sorry because I need to say records oh yeah yeah yeah maybe not internal dc zones zone name records argo events he made oh uh argo events dot ul1 is it yeah anyway I don't remember now how you're supposed to here I'm I'm gonna what he what he created was um this so zones medallion.ca records argo event oh yeah yeah yeah because he has it here in the ticket so zones medallion.ca records argo event oh yeah yeah yeah because he has it here in the ticket I mean what he created is for uh public and we need the internal equivalent is that right I know then it's then he just uh so I was wrong and it's just in this but it's in this one and ca records okay yeah I think we need both that's the thing that we're missing one we're missing the internal one what you were just curling it makes sense that it's not there because we haven't created it yet right yeah yeah yeah yeah yeah I think that's that's what we need but I think it needs to be or maybe maybe if we do have the internal one then will be possible to resolve it yeah and at that point um what is the external um billing the one the one he created initially is that is that still being used I think that it's it's like from it resolves the the dn the c name from like medallionetwork oh okay okay like suppose that um yes yeah suppose that we we had a some kind of ui or something but similar to argo workflows you would have to yeah have both but I think that having both records it's no harm no I'm not sure anyway they need to you need exactly to tell them I want to resolve this record from sc4 uh from sc4 and they they know they will tell you what needs to happen yeah yeah but it's for sure it's a dns issue like because we had same similar issues so just to be clear what I just posted in the slack we just we need that to resolve to a record or uh and we need that with them to create something for that probably but anyway leave it to them because like I'm telling you sc4 is also like a yeah I won't I won't tell

%%

---



The transcript discusses issues related to DNS (Domain Name System) resolution and inter-data center communication within a company, specifically focusing on integrating Alertmanager with Argo Events. Key points and actionable items for updating a Jira ticket include:

1. **DNS Configuration in Different Data Centers (SC4 and UL1):** The conversation highlights the unique DNS setup in SC4, different from other data centers. This requires special handling when setting up services that need to interact with SC4.
2. **Creating Public DNS Records:** For most data centers, public DNS records pointing to internal services are created. However, SC4 requires a different approach.
3. **Need for Internal DNS Records:** The discussion suggests that internal DNS records are necessary for proper communication between SC4 and other data centers like UL1 and potentially Denver.
4. **Testing and Verifying DNS Resolution:** It's mentioned that DNS resolution works differently in SC4, and testing from different locations (SC4, Denver) is needed to understand the issue.
5. **Consultation with Network Services:** The transcript suggests consulting with network services for creating the appropriate DNS records, especially for SC4.
6. **Jira Ticket for DNS Record Addition:** A Jira ticket has been created for network services to add a DNS record. The specifics of the record, whether external or internal, need clarification.
7. **Use of Tools for DNS Management:** Tools like Thunder Client and DOS are mentioned for DNS management and record creation.
8. **Possible Use of IP Addresses as a Temporary Solution:** There's a suggestion to use direct IP addresses to bypass DNS resolution issues, though this is seen as a less desirable approach.
9. **Alertmanager Configuration:** The discussion includes configuring Alertmanager in SC4, possibly pointing it to the IP of a load balancer as a workaround.
10. **Need for Both Internal and External DNS Records:** The transcript suggests that both internal and external DNS records might be needed for proper resolution and operation of services.
11. **Verification of Current DNS Setup:** There's a need to verify the current DNS setup, especially the records created for public and internal use, and how they are resolving across different data centers.

**Actionable Items for Jira:**
* Create and verify internal DNS records for `sc4`, especially for integration with `yul1`. Might need an `A` Record.
* Consult with network services to understand and implement the DNS requirements of SC4.
* Verify and potentially update both internal and external DNS records to ensure proper communication between data centers `sc4` (Alert manager) → `yul1` (argo events)
* Potentially update the Alertmanager configuration as necessary, considering the DNS resolution issues in `sc4`.
 

The DNS name cannot be resolved (from SC4 k8s pod - alertmanager). I don’t know what are the rules for DCtoDC connectivity so I guess we have to ask NS. I don’t know if this, for example, is supposed to work (this is from the gesp pod running in SC4):

The DNS name cannot be resolved from the `sc4` k8s pod - alertmanager, so we need to ask NS about the rules for DC to DC connectivity. It is unclear if this is supposed to work, as shown in the gesp pod running in SC4.


---


%%
in suspended state when someone forgets about that, right? And that's one thing. Another thing is that we may want to run the preflights, like, you know, because when we have a suspended step, we wouldn't be able to run the preflights, like, separately. It will be just a single workflow. And if you want to run the preflight, just to check something, you know, we can do that. And then run the preflight again if something changes. Or, you know, having a single workflow is just a single workflow. It's just, you know, we wouldn't have too much control over that. So I think that this is kind of more robust. While the other drawback of the suspended step is that we would have to implement the support for that in the API, in JASP, and everywhere else. Yeah. But I would say, why do we need, like, the suspend thing in the first place? Like, because I was thinking, OK, let's say that you want somebody to take the whatever the commissioning is about to do, right? And then approve it or whatever. And I'm like, OK, what if, but you could say, how could you? Because I'm thinking, like, what if there was, like, the as they need step that picks up, collects all the data that we need, and so on. And then it just is waiting for something, like, to be flipped, like a flag on a ticket or something. And it's checking periodically. If nothing changes, like, it will time out as another, as any task. And then, OK, timed out, so it's stopped. And then if somebody wants to approve it and allow it again, he goes and flips that flag in the ticket, and, like, it hits retry, and so on. So we don't really need to create a suspension. But we will play with what we haven't accomplished, kind of. And the way that I'm saying that is, like, it's OK if we split it to two parts. But then I'm, again, thinking, like, how can then we use it for doing integration tests and so on. Because we kind of have higher hopes that if we get this one, like, it will create an opportunity for us to do testing by, you know, what we said, like, creating a cluster, running different tasks, and then decommissioning. Well, I think that we should be able to do that. Because we don't have to run the pre-flight step or pre-flight task in the integration test. I don't think that the output, or maybe, yeah, maybe the output from the pre-flight task shouldn't be the input to the decommission task. It might be, like, independent on that. I don't know what the input parameters for the task should be. But I think that at least, or we should be good just with data center and instance name, right? SPEAKER 2 So it will be kind of a dry run, right? That we'll just run init and finalize, or whatever. Yeah, something like, just to be safe that, you know, hey, this is what's going to happen if you approve this ticket, right? This is going to be decommissioned. SPEAKER 3 OK. But should we then introduce, like, a dry run flag or a dry run parameter in the step UI script, which will do exactly what it would? But every time we're about to do something intrusive, we're saying, like, if the flag is, like, dry run, just print out this message and exit, or whatever. Which will be, we would have deleted the database. We would have removed that front end, or whatever. Now, what is a dry run? Is it just for removing records, or is it also for adding records, or editing them? SPEAKER 2 I thought anything, though, would say, like. SPEAKER 3 Yeah, well, in that case, you would have to implement this dry run for everything that you have in prop ng, right? Because every step might be potentially intrusive. Or not just a step, but all the, every operation, so to say. SPEAKER 2 Thank you. And another plus for this splitting into two tasks is that what happens if these properties are generated, and someone decides, hey, this is not what I want to do. I don't want to run the decommissioning, because it would delete my database, or whatever. That's something I don't want to do. If we had a suspend step, it would just wait. And what now? Now what? You would have to implement some kind of, you know. SPEAKER 3 Abort mechanism, a little bit. SPEAKER 2 Abort mechanism, yeah, to the API, maybe to the UI, to Jira. I don't know. And what if there is no Jira? It's like splitting this into two individual tasks. It's clean. It's simple. We don't need to implement any new logic to the framework. And it's, to me, it's more safer, even. SPEAKER 3 OK. OK. OK. But yeah, as you said, like, so this one will be just like a dry run. Like, it will just run and say, yeah, this would be deleted. This would have been deleted, and so on. But then the other tasks will really do it. Yes. The first task will not do anything, like any write operation. It will just gather the properties from a lot of systems everywhere, from tenant registry, from, you know, database, from I don't know. And it will say, hey, this is what's going to be deleted if you approve the ticket. Well, I asked Ignacio to try to see the API spec. Yeah. He could implement the spec for these two tasks. Or maybe the first one first, because that could be really easy. And yeah. But this is like, so this ticket, the 440 that you have, it's for Ignacio, right? Probably, yeah. I can assign it to him. And not now. We will do it like, we'll just set the priority. Mm-hmm. Kind of major, maybe. OK. But I was thinking about this, because this is a new task. And one thing we don't do right now is that we put small pieces of this, merging it directly to master. And I was thinking, maybe what we need for bigger things like that, like a new task or whatever, maybe we should have an epic and have a branch for that epic, we will say, and push all the changes there. And when we're done, then we merge to master from that epic branch. Definitely, I agree that we need an epic for Mac decommissioning. I don't think it's there now. And these branches will be needed in multiple repos, not just in Provence G, but also in JASP, APIs pack, maybe API server as well. OK. There we are. OK, anyway, I need to run, guys. Because I need to take my daughter to some activity. OK. Have a nice weekend, guys. Thanks. You too. Bye. You guys too. Later. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye.

%%

---


%%
was with Sasha's team doing a lot of load testing ultimately that I was really excited for because we've been pushing for that for a long time and like while we would like to execute it we're just not big enough to execute that kind of testing. Yeah I brought that up with the SA too. It was like you know let's bring up the well I don't know if it was with SA or was it with RDP but either of them to bring up the infrastructure that we had earlier for like perf testing and stuff so like get the budget to kind of you know spawn that up again and yeah dedicated servers and resources just for that so hopefully it picks up some steam and we can get that done in the next year but we definitely need that now because you have so many variables yeah in like with the XLUG and non-XLUG clusters being the biggest difference and then again how some customers use the API's mostly on our end right like most of the query API's is where the problem is and then action validations right yeah what's happening is I feel at times the teams are not aware that this part of the core actually will invalidate every cache like that's what I think came out of the one CX core incident channel where they said oh we weren't aware or we didn't expect this cache to be invalidation or something right now doing something or off so those kind of things is something which probably we should have a more thorough code review or some kind of testing and analysis done on that code to know if it is going to do a full invalidation or not and not catch us by surprise right like I mean yeah I'm just imagine if this had to hit all customers at the same time and everyone just came in and just open incident you would have had like 20-30 incidents or something like that yeah yeah it does feel like it's a little chaotic though in general right just from all the performance issues that we've seen no escalations I know that the PE team has seen it's been unreasonable like wall like we we should be taking direct escalations from PS teams etc we we have been and like that's it's not great but you know but even even from the development side the number of things that we've seen oh yeah like the one yesterday right like just for JFRs I mean it was a question who if I mean the JFRs up for them right it wasn't like yeah I approached that the wrong way though I shouldn't probably I just rich is getting pinged like in 40 different places right now and I'm like yeah like he's one human right like if you can load balance on the team let's do that so we can help because I is able to jump on a meeting with with Tom and talk through with the different things please I have to say this the Tom situation was a little concerning to me not for Tom but when he when he got on a call with me he told me yeah I'm investigating this problem that we're seeing the app frame right I'm like yeah but you're a front-end developer investigating a back-end issue where's the back-end team like are they looking at it he's like no it's like no they're not I'm like why like they should be digging in doing JFRs you shouldn't have to build a whole there's no way he's gonna know the front-end and the back-end I'm sorry that's just way too much information for any one human right so I'm like we should pull in the back-end team to start helping assist in this investigation so you can look at the front inside and they can look at the back anyway long story short I had a really good conversation on zoom with him yesterday and that's good I mean yeah I mean I it's not this part as well I mean he's pretty on that side so it's understandable and that team has a lot of changes happening every now and then right like I mean we ourselves don't know what the reporting services team is looking like today and what it's called like the difference and who's the on calls for them and stuff like that so they keep having so many name changes or yeah it's always it's always a pain like how do you get if this is front-end back-end or is this max or is this something else going on yeah I actually originally thought when Thomas good I thought that was he's from the PS team

%%

---


Scale issue is not always a performance issue

%% should probably still escalate that, right? The problem isOkay, don't escalate to you guys now, but they escalate that to us. In the senseAre you guys getting hit more? I have been hit a couple of times by name, especially like, just, oh, we're gonna do this launch. Do we need to send any comms out or do we need to have any? Yeah. We have all the alerts in place. The system looks stable enough. We have done this previously, so it should be okay. That's about it, right? Like, if something goes south, we'll find out or we'll start an incident then. But there's no real need to kind of, you knowYeah, I agree. Eyes or keep a slack channel just for that one day and things like that, unless it's a very new launch and it's a very massive launch, right? Like, I mean, you've done things a quarter ago and you're just repeatedly doing it every quarter. That should be fine. We don't downscale or we don't downsize without proper analysis. And keep in mind that you have these kind of waves where you launch stuff, right? Like, we already are aware that Apple Feedback and the Apples do that on a quarterly basis for when the Apple products are gonna launch, right? In September. So we know about those windows. We know about like when Walmart is doing things. So, I mean, obviously he's also kind of now slowed down on that because I didn't stop even informing L1 about it because I knew that we're gonna be fighting for those, right? Like, there's no need to really panic on those situations and kind of, you know, just get extra alert about it. I mean, we're not alert about it in any which ways, right? Like, L1 is pretty good at this. SREs kind of jump in ASAP as possible and we do reach out to the right engineering teams and things like that. So I feel we already have a good process for that. Why kind of now get extra diligent? Why change it? Yeah, yeah. Unless it's a very new customer, like they're growing from like zero to like 100,000 right away. Yes, for that, maybe we need to kind of have eyes on it because it might be a new launch. We don't know how things are gonna go south, how things are gonna break. We might need to restart back in a couple of times and things like that. But until now, for whatever you think, nothing actually happened, right? Like they all went through pretty fine. So I think he's also understood that maybe they need to be aware of it, make others aware of it. Exactly, exactly. You don't have to like, you know, get extra scrutinize the situation and… Yeah, and that's my question. The other side of this too is like, we've already done scale assessments on a lot of the stuff that's launching. I'm not all of it, because that does miss us. But if that misses us, then we ask the dev teams to look to make sure that that will work correctly. However, I just, I'd like to be clear, I don't think there really needs to be monitoring on it, because we have already done, we've already scrutinized the situation. If there's something we're concerning, we're gonna bring it up in the scale assessments, right? And so the intent with us pushing that on, I was concerned about this. I'm really glad you brought this up, because I'll bring this up in the scale exec meeting again. The intent was, they're saying, hey, we want monitoring during the launch. I'm like, if you need monitoring, ask tech support to monitor, not SRE. They can go and they can look at the data on it. They need to know what to look at. Like we can show them heap. We can show them CPU. You know what I'm saying? We can show them the integrations dashboard, right? And explain what things mean. I don't wanna get too deep, you know, we're not asking them to do a deep dive into what that means, but just some basic dashboards is all they need to do. Right, I think they can catch the anomaly on that day and reach out to SRE then, right? Like reach out to me that if you need to. And I can guide, is that a problem? Is that okay? Are we gonna get through it? Like we can do that, right? Yeah. But I don't think we need to get into a state where, oh, we wanna panic and have incident. We used to do that previously, right? I get it. We used to do that before. Because obviously we were not maybe as good at the monitoring or maybe we weren't having that great monitoring and we didn't have that great of a scale assessment happening and things like that. All that has changed over time. All that has improved. We are at a better state with most of these customers. We're giving them extra resources now, if need be right beforehand and things like that. So there's no need to get really panic but yeah, just be aware, let your support people know, look at these dashboards. If things are spiking, absolutely let us know and we can deep dive and figure out does that warrant an incident or does that warrant more turning eyes and things like that. So I thinkYeah, no, that's a good call. And I can make sure to reiterate this to them. Cause I, again, the intent was not to have them direct it to you. It's absolutely not what I told them. I haven't got like 15 weeks or something. I just got a couple of things on Apple and a couple of other launches, but it's not too bad. It's just like, we do get those things. It's fine. I don't, they don't annoy me anymore. Like those things are fine. Like I get it. They want to launch it. They want us to have eyes, but I then take a decision and don't involve L1. Cause I know we are in a good state with those customers and we do have monitoring and things in place for at least those launches, right? Like if it was something which even I wasn't aware of or we haven't done it before, obviously I would have involved more people and kind of made everyone aware that, okay, this is going to happen. Let's just keep X for eyes, right? So. Yeah, absolutely. That's great. Again, cause we've already done scale assessment. There's something that's concerning to the solutions architects. So in most cases, I think it's just, you know, they want more eyes, but I think you're exactly right. Make a discretionary decision if you're going to look at it or not. And I think most cases you probably don't need to. Cause the reality of it is we want alerts for those things anyways. If there is a problem, we want to be alerted if there is a problem. Right. We don't want to have humans monitoring ultimately. We want, you know, systems monitoring and letting us know. So yeah, I totally agree with that. If there's something you want us to know, like if you're getting hit by something and they just are relentless, let us know. Let me know. I mean, I think everyone's become more better at this, but obviously I can understand why Rich gets pinged so often is because everything falls under a perf analysis or a scale analysis from like the inside, right? So. Yeah, I have to tell you, there's a little bit of a misnomer when people talk about scale assessments. Whenever there's like a, any performance issues, people are like, hey guys, we need a scale assessment. And it's like, do you know what a scale assessment actually is? I want to know that. Yeah. Because a lot of people don't understand like what the difference between scale is and performance issues are. Can they be related? Absolutely. Are they usually related? Usually not actually. Usually there's a problem in the code that's just not optimal. Like there's just a suboptimal code path, right? But scale related issues are like, hey, we're like dealing with 900,000 units, right? And now they're seeing performance impact. And it's like, yeah, that one is a scale issue because we, that is way above and beyond what we support. You know what I'm saying? So we've been doing this scale. I don't know if you guys are aware of this too, the MEC stabilization initiative. Did you guys, are you guys aware that we're doing like scale assessment analysis on the top 10 clusters? I should show you this because this is actually really important. Give me a second here. I'll pull this dashboard. Like even the T-Mobile issue, right? Like that was not a scale problem. That was more like the R fields being used in those reports. So that's a perf-perf problem. That's not a scale assessment problem. Exactly. Yeah, and because of the perf problem, they probably reach out to you guys again first because also the perf ends team, but they don't understand that the perf ends team is not for perf analysis every time. It's like, you need to reach out to the development teams as well because now that's a solution architect problem where you're going to sit with them and figure out how your R fields or K fields or A fields or any goddamn field has to be properly defined, right? Like, I mean %%
