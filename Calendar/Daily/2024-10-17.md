---
up: "[[2024-W42]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20241017213352
modified: 20241018155912
aliases:
  - Thursday - October 17th 2024
linter-yaml-title-alias: Thursday - October 17th 2024
title: Thursday - October 17th 2024
id: 10
week: "[[2024-W42]]"
yearly: "[[2024]]"
quarterly: "[[2024-Q4]]"
monthly: "[[2024-10]]"
daily: "[[2024-10-17]]"
month: "October"
weekday: Thursday
---

# Thursday - October 17th 2024

## Memos Personal

## Memos Work

## Working On

```
{
      "label": "Run step.py directly",
      "type": "shell",
      "command": "python",
      "args": [
      "${workspaceFolder}/step.py",
      "--debug",
      // "--customer-identities", "${input:customerIdentities}",
      "--customer-identities", "demodcrtest4",
      "--context", "${input:context}",
      "--platform", "${input:platform}",
      "--task-type", "${input:taskType}",
      "--working-dir", "${workspaceFolder}",
      "--config-directory", "${workspaceFolder}/config",
      "--secrets-file", "${workspaceFolder}/secrets/secrets.yaml",
      "--params-file", "${workspaceFolder}/params.yaml",
      "--output-file", "${workspaceFolder}/output/${input:outputFile}",
      "--run-dc", "${input:runDc}",
      "${input:stepName}"
      ],
      "problemMatcher": [],
      "presentation": {
      "reveal": "always",
      "panel": "new"
      }
    }
    ],
```

```
Explore Now >
PLAUD NOTE can record audio, transcribe it, and summarize it by AI.
10-15 Meeting: Mec Bounce Node Process Improvements
2024-10-15 13:45:53
00:41:11
00:00
41:11
Date & Time: 2024-10-15 13:45:53
Location: [Insert Location]
Attendees: [Speaker 1] [Speaker 2]

Overview
This document outlines the process improvements for the Mec Bounce Node, focusing on workflow optimization, challenges, and potential enhancements. It covers various aspects such as integrating with Argo CD, handling shutdown protection, managing pod readiness, and addressing different node states and phases. The content provides insights into the current workflow, identifies areas for improvement, and lists action items for future implementation.

Current Workflow and Challenges
Integrating with Argo CD for CR state management
Incorporating startup probe considerations
Handling shutdown protection scenarios
Shutdown Protection and Pod Deletion
Attempt to disable shutdown protection
If unsuccessful, log a warning but continue
Failure to disable shouldn’t immediately fail the task
Check pod status
Ensure pod is not in terminating state before deletion
Delete pod if safe to do so
Wait for pod to come down
Use custom resource to bring pod back up instead of manual patching
Waiting for Pod Readiness
Leverage startup probe for waiting period
Set maximum wait time (e.g., one day)
Task should fail if pod doesn’t come up after one attempt
Monitor pod status during startup
Check for “booting” and “booted” states
Identify if node is in efficient catch-up mode
Retrieve and display ETA for efficient catch-up if applicable
Grace Period and Failure Handling
No specific grace period defined
Pod should go down within 5-15 minutes typically
If pod not down within 15 minutes:
Notify end user
Allow user to decide whether to continue waiting or intervene
Wait for Argo CD to bring CR back up (within 5 minutes)
If CR doesn’t come up:
Attempt manual intervention
Notify of stuck state and raise exception if unsuccessful
Potential Improvements
Refine handling of terminating pods
Develop strategy for force deletion when necessary
Improve integration with Argo CD sync process
Enhance startup probe information retrieval and decision-making
Node States and Phases
Synchronization Process
The startup probe is primarily for show and data collection
Actual decision-making based on collected data occurs in the bounce node
Different node states and phases need to be considered in the context of node bounce
Rebuild phase
Efficient catch-up phase
Node Status Handling
Regular catch-up is no longer used; all classes now use efficient catch-up
Need to identify scenarios where synchronization will never finish to raise exceptions
Error handling for various node states:
Booted: Should not be a long-term state
Alert needed if node is stalled in booted state
Broken: Requires immediate attention
Indicates a bad state needing manual intervention
Booting: Timing configured at Helm chart level
Synchronized: Not necessarily indicative of node being fully operational
Error and Waiting States
Error Handling
If an error state is encountered after boot but before full readiness, notification is necessary
Any non-standard state requires notification for prompt action
Waiting State Management
If node is synchronized but in waiting state:
Implement a waiting period of 5-10 minutes
Maximum waiting time should not exceed 15 minutes
After waiting period, final status check to be performed
Ongoing Improvements
The process is acknowledged as complex
Plans to enhance and streamline the monitoring system
Action Items
 Test patching CR down when pod is stuck in terminating state
 Implement logic to detect and display efficient catch-up ETA
 Develop user notification system for extended wait times or failures
 Create process for manual intervention when automatic methods fail
 Implement 5-10 minute waiting period for synchronized but waiting nodes
 Create alert system for nodes stalled in booted state
 Develop notification process for error states encountered post-boot
 Review and optimize error handling for various node states
AI Suggestion
Review and optimize error handling for various node states
Develop notification process for error states encountered post-boot
Create alert system for nodes stalled in booted state
Implement 5-10 minute waiting period for synchronized but waiting nodes
Create process for manual intervention when automatic methods fail
Develop user notification system for extended wait times or failures
Implement logic to detect and display efficient catch-up ETA
Test patching CR down when pod is stuck in terminating state
2. Plans to enhance and streamline the monitoring system
1. The process is acknowledged as complex
2. Waiting State Management
1. Error Handling
2. Node Status Handling
1. Synchronization Process
4. Enhance startup probe information retrieval and decision-making
3. Improve integration with Argo CD sync process
2. Develop strategy for force deletion when necessary
1. Refine handling of terminating pods
4. If CR doesn't come up:
3. Wait for Argo CD to bring CR back up (within 5 minutes)
2. If pod not down within 15 minutes:
1. No specific grace period defined
2. Monitor pod status during startup
1. Leverage startup probe for waiting period
3. Delete pod if safe to do so
2. Check pod status
1. Attempt to disable shutdown protection
3. Handling shutdown protection scenarios
2. Incorporating startup probe considerations
1. Integrating with Argo CD for CR state management
This document outlines the process improvements for the Mec Bounce Node, focusing on workflow optimization, challenges, and potential enhancements. It covers various aspects such as integrating with Argo CD, handling shutdown protection, managing pod readiness, and addressing different node states and phases. The content provides insights into the current workflow, identifies areas for improvement, and lists action items for future implementation.
Attendees: [Speaker 1] [Speaker 2]
Location: [Insert Location]
Date & Time: 2024-10-15 13:45:53
Action Items
Ongoing Improvements
Error and Waiting States
Node States and Phases
Potential Improvements
Grace Period and Failure Handling
Waiting for Pod Readiness
Shutdown Protection and Pod Deletion
Current Workflow and Challenges
Overview
10-15 Meeting: Mec Bounce …
Transcription
Speaker 1 00:00:01 --> 00:00:35 
to work in tandem with the way the startup probe will work. So, so for example, like right now we don't, we need to kind of like ballpark, uh, like how long we want the ETA to be based on the instance. And if we have the startup probe and we're like reflecting like inside their, tenant values.yaml, like how long it would be, what will be even normal and have like a default in case it hasn't been specified yet. But how long would it take for this FE node or whatever to come up about this.
Speaker 1 00:00:36 --> 00:00:58 
long? Then it, we can base how we can base how long we want the mech bounce node, like to like, we only want it to fail after that readiness probe or startup probe is like in a failure state after a certain amount of time and that amount of time by the.
Speaker 1 00:01:30 --> 00:02:00 
Um, like additional minutes. Like let's say, you know, whatever it's failure is, let's say 10 minutes past that. And then also, let's say, let's say we want to give it a hard stop. If it's not done by 30, if it's not up by 30 minutes, then what would we do? We should like essentially like do something. Like fail the task and tell me.
Speaker 1 00:02:00 --> 00:02:09 
So I can intervene manually, in some way. Isn't that what you guys usually do, like you force sync it in a case like that, or no? No, it depends, right. 
Speaker 2 00:02:09 --> 00:02:13 
Like, what's the cause for it to not come up. 
Speaker 1 00:02:13 --> 00:02:43 
I don't know. That's what I'm saying. I don't know if it's necessary, but it seems like, given the situation, it might be like, regardless of what this client startup probe is configured to be, I need to know in 30 minutes. If this node's not coming up in 30 minutes, then I need to do something. Like, would you say that that's something that would be still necessary, or what. 
Speaker 2 00:02:43 --> 00:03:08 
No, I don't think so that's necessary. I mean, unless there is a failure state, and then it needs to alert us on the failure state, that is required. But not like, yeah, why are you taking 30 minutes, or why are you taking 45 minutes, right? Yeah. I mean, as long as this job… So, this task will tell us what that node is doing in the log or the UI saying, I am doing efficient catch-up.
Speaker 1 00:03:08 --> 00:03:09 
Yeah.
Speaker 2 00:03:09 --> 00:03:29 
Now, there's no point of an alert for that, but if someone is interested to see what's going on, they could open the job or task UI and see what state is it currently at and probably that output in the log line should tell us, okay, I'm still doing EC and that's an expected state.
Speaker 1 00:03:29 --> 00:03:30 
Yeah, that makes sense.
Speaker 2 00:03:32 --> 00:03:36 
Right. So, let's assume you go down and then what happens after the disabled shutdown. 
Speaker 1 00:03:36 --> 00:04:16 
So, first of all, disable shutdown if you can. If you can't, then don't fail, but log a warning because let's say the node's in a state that you can't shut it down. And I guess I was thinking about that. What if you disable the shutdown? You're doing disable shutdown before the node is stuck in terminating.
Speaker 1 00:04:16 --> 00:04:50 
So if you don't do that… Right, so that would just be failed.
Speaker 2 00:04:50 --> 00:05:03 
But we don't know what the error is going to be. I don't know what is the exception it throws when it fails to disable shutdown. We'll have to figure that out. To be able to catch in this failure test.
Speaker 1 00:05:03 --> 00:05:05 
Yeah. Right. 
Speaker 2 00:05:05 --> 00:05:20 
Yeah. So that is a pending item. We will need to see one, so I'll have to check how we can figure that out. But for this failure test, above the log warning, you will need to know what that will look like. Yeah. So that you all can log on that, right. 
Speaker 1 00:05:20 --> 00:05:50 
Yeah, because we'll have the pod phases and conditions to consider. So let's say it's stuck in terminating, then we know somebody already fucked the node up. Then we know what to do, what not to do in that situation. But let's say it's not in terminating, it's in running, but we can't hit admin. That's why we can't disable shutdown protection. So then that means something completely different. I guess there's a lot to do right here.
Speaker 1 00:05:50 --> 00:06:22 
But I'm simplifying it by just saying, okay, it failed, let's continue. There's more to it that would happen here, I think. So let's just do happy path though. first. So success. Alright, let's get the pod status. Okay, so let's assume that it's not terminating. Can we delete the pod safely? Yes. Wait for the pod to come down.
Speaker 1 00:06:22 --> 00:06:54 
And by this, instead of doing a patch, we would do just deleting the pod. And then the custom resource would bring that pod back up. So we wouldn't try to do anything. We would just wait for that to come back up. And then we would have like a grace period configured, like probably a default of something. Or hold on a second. Wait for pod down. Ah, well, I haven't thought about the grace period yet.
Speaker 1 00:06:54 --> 00:07:30 
But I think, like, how I meant it when I haven't reflected it. here yet is wait for pod down should base it off of the readiness or sorry the startup probe so like whenever that the pod phase is like being considered like failed that means the startup probe is done like it failed and then I think like I was thinking of a grace period might be necessary here but maybe it's not I don't know but it seems like we.
Speaker 1 00:07:30 --> 00:07:58 
should we should leverage this the startup probe for the waiting like so like let's say the task is going to take like at most a day will set it at a day and then however long it's going to take we're willing to wait and we're not going to try to define it in a way but we will fail if it doesn't like if the pod doesn't come up and.
Speaker 1 00:08:00 --> 00:08:25 
It should fail if it fails once and then obviously we can retry the job or see why it didn't.
Speaker 2 00:08:25 --> 00:08:56 
go down right because there should never really be a state why they should fail. If it's taking the happy path where disable or shut down protection work and you're deleting the pod, it has to go down. There is no reason why the pod will not go down or shouldn't go down, right? Like unless the Kubernetes API fails to make a call to the pod or the CR customer resource and say, I want to bring it down, right? That's the only probability that something in the network or the API fails that this task could fail.
Speaker 2 00:08:56 --> 00:09:04 
Because nothing should really stop the pod from going down if the, DisableShutdown was successful for that specific IP.
Speaker 1 00:09:30 --> 00:10:00 
I need to make some adjustments here. CR was up within five minutes. No. Ah, okay. I think I know what I meant here. So, like, so there's two paths here, kind of, because, like, technically, like, let's say we were not able to get the pod status, or sorry, we were not able to disable shutdown protection, and let's say the pod status.
Speaker 1 00:10:30 --> 00:11:00 
1 does when they do the create and remove and it always fixes it, that's what I'm trying to identify as a path towards doing that here. Because it's like, okay, if we can't delete the pod safely, then, well, we're going to have to do the create, remove, like effectively here. But instead of doing a create, or sorry, remove, create, we just have to do a remove and then Argo CD would create it, create it again.
Speaker 1 00:11:00 --> 00:11:37 
Well it would set the state back to up, like for us, but by that time the custom resource I think would have deleted this pod. I don't, we'd have to test it, but you know what I'm saying? So because I think, like, and actually I want to know your opinion on this because I don't know, I'm assuming, that when you try to delete it and it fails, like, it gets kind of stuck, stuck in this state that it can't, you can't force delete it because you shouldn't, but.
Speaker 1 00:11:37 --> 00:11:38 
you're basically stuck.
Speaker 2 00:11:38 --> 00:11:43 
Yeah, so it's just stuck in terminating and it's not doing anything beyond that point.
Speaker 1 00:11:43 --> 00:12:14 
Yeah, and you're even more fucked based on, look at this, and you've probably already seen it, but based on the changes that SIF will be rolling out soon, you'll be even more fucked because you won't even be able to, like, not saying that you should, but you wouldn't even be able to force. This is what I want to know. When you do a patch CR down, I think that even if it's stuck in terminating, no matter.
Speaker 1 00:12:14 --> 00:12:36 
what state that pod is in, when you do a patch CR down and it takes the pod down, I think that it kind of cleans everything up and whatever it was stuck as, the pod goes away. And I think cleanly. Do you know. 
Speaker 2 00:12:36 --> 00:12:55 
I don't think that is going to be true because once it's stuck in that forced state where it's lost its definition, because once you give the or trigger the delete command, the state of the CR has changed. And then marking it down.
Speaker 1 00:12:55 --> 00:13:23 
The state of the CR or the pod, because when you delete the pod directly, I don't even think that… Yeah, we'll have to probably test that. Let me try and test it today. No, no, it doesn't always. No, it doesn't, right. 
Speaker 2 00:13:23 --> 00:13:54 
Because the remove and create is only a deployer command that is working on clusters that are in a stable state. If you were in a messed up state of a cluster where 1FE wasn't terminating, the remove would not always work. It's just the deployer is doing the same thing. Instead of running the delete, it's modifying the custom resource. But if someone had run a delete outside of the deployer and then tried a remove, it may not always work.
Speaker 2 00:13:54 --> 00:14:25 
It depends on like, say, let's say… Because sometimes what happens is the board is so busy or it is in GCP. hell that the api call is not making a connection like it's not even acknowledged by the fe yet and that takes some time for the socket connection to get established before the job can go away so so what what had happened is like when l1 tries that like they're supposing they wait for 15 minutes and they see something in terminating and we say okay probably you did the wrong thing just go and.
Speaker 2 00:14:25 --> 00:14:54 
run this from the deployer and see what happens and they went and probably ran the remove what in reality happens or has must have happened is this connection got successful and that's when the pod actually went down neither did we force kill it nor did the remove do anything it just it just completed because there was nothing actionable for the remove command to do and it doesn't, send a status saying i didn't have anything to do and i just finished myself it just says yeah i finished successfully.
Speaker 2 00:14:55 --> 00:15:18 
ah okay so what i'll do is i'll go in and i'll figure this i have a couple of pods, I'll try and see if I can get them to this failure state and see what happens if I just patch the CR. And then we can design this part of it. So let's keep this as undecided for now and we can finish the success part.
Speaker 1 00:15:18 --> 00:15:40 
So right here. Right here. Undecided. Yep. That's it. Okay. Correct. Okay. Along with this. Yes. So in the meantime, don't fail the task if disabled shutdown protection, still attempt to proceed if you can.
Speaker 2 00:15:40 --> 00:15:53 
I mean, Evan, why would you proceed because if, I mean, we can only proceed if this fails, if we have another way to take the part down, because otherwise your part is never going to go down. It's just going to be stuck in terminating.
Speaker 1 00:15:53 --> 00:16:14 
Yeah. Even though we're not doing anything like in cryo or whatever, it's not the. I mean, we just don't have the approvals to do the harsh deletes any further, right?
Speaker 1 00:16:31 --> 00:16:32 
As a failure, that's fine.
Speaker 2 00:16:32 --> 00:16:36 
Let's just mark it as a failure. And this is undecided.
Speaker 1 00:17:00 --> 00:17:02 
I mean, we just don't have the approvals to do the harsh deletes any further, right. 
Speaker 2 00:17:00 --> 00:17:32 
I don't think there should be any default, to be very honest. I mean, if the pod is going down, it is going to go down. It's not going to go down if it's in terminating state, right? And that will fail much before that. So this grace period, really… And no one will know what the grace period should be. I mean, there is no default we can tell like what a Walmart is going to take or what a small cluster is going to take, right?
Speaker 2 00:17:32 --> 00:18:05 
This is completely unknown to anyone. And this is just dependent on if the Kubernetes API call is successful and if that socket connection is made and can the pod get that, OK, I have to kill everything or delete all my net policies XYZ and mark myself as down, right? And that should just happen. It can happen in five. It cannot be more than an hour or so. So if you just want to keep a default… Maybe 60 minutes, but everything should be down within 5 to 10 minutes at a max, right?
Speaker 2 00:18:05 --> 00:18:08 
Or 15 minutes to the max, nothing more than that.
Speaker 1 00:18:08 --> 00:18:32 
So no matter what, if it's not down within 15 minutes, we'll assume something is probably wrong. Yeah? Yes. Or is there cases like in some of these big nodes, like you said, they're trying to finish up doing something before it'll allow itself to go down, and we should anticipate this amount of time in some way? Yep.
Speaker 2 00:18:32 --> 00:18:35 
That sounds correct, yeah.
Speaker 1 00:18:35 --> 00:18:56 
So I guess that's what I'm wondering. Like let's say we know that it's doing some shit, and we don't want to fuck with it, but we do want to bring it down still. So like the user says, oh, you know what? I'm going to say that I want this to take, like if it's not down in two hours, then fail it.
Speaker 2 00:18:56 --> 00:19:00 
But don't fail. So if you were in an incident, you want that to happen.
Speaker 1 00:19:00 --> 00:19:16 
I don't think we really need a grace period here, because the pod, if he wants something.
Speaker 2 00:19:16 --> 00:19:37 
to go down, it has to go down right away. I mean, the grace period is not going to do anything other than delay the inevitable, which is, if you're not going down, I need to look at it. I will kill it myself, like using force means, right? Or it's just going down, it's just probably taking 5-10 minutes more here and there and that's fine.
Speaker 1 00:19:37 --> 00:20:10 
Well, that's the next thing. So wait for pod, if it's not down within a certain amount of time, let's just say 15 minutes. If it's not, then notify the end user, or whoever, and say like, hey, whether or not you failed the task, if you want to keep it going, or if you want to fail it. Like, but effectively say, like, we have, we, we know that it failed here and this, and this is where you need to intervene.
Speaker 1 00:20:10 --> 00:20:40 
Okay. That's, so that's how it is right now. Okay. And if we become more comfortable with this path, then perhaps we can adjust it and make it smarter. But for right now, it seems like, okay, that's, this is when we want somebody to do something. Because nothing, you know, like you said, like it's delaying the inevitables, like something, nothing's going to happen and we're just going to be, we're basically going to be waiting. Okay.
Speaker 1 00:20:40 --> 00:21:17 
Yep. Okay. So let's say that doesn't happen. Then we wait for Argo CD to say, well, I guess technically that would already happened. Delete pod. Yeah, that would have happened up here. So I need to fix that. Okay. So, CR within five minutes. Okay, so I think the reason I put that here is because I was anticipating the path is.
Speaker 1 00:21:17 --> 00:21:58 
this, but just recognizing that if we did it like this, then the CR would already be up. So, if you were to assume a patch did happen, then we would wait for ArgoCD to bring it back up, like properly, before considering, like, okay, it's not, let's try to do it ourselves. And if that doesn't come up by itself, and within, you know, the amount of time that its startup probe has set, then we would fail it, or it would succeed, and then the bounce.
Speaker 1 00:21:58 --> 00:21:59 
would be done.
Speaker 2 00:22:00 --> 00:22:09 
I think, yeah, I think that makes sense, yeah. Yeah.
Speaker 1 00:22:09 --> 00:22:41 
Need to probably adjust a few things, but yeah, it's like, I thought it was going to be kind of open and shut as soon as, like, you know, with the startup probe, but the shutdown protection throws a wrench in it and also this whole, like, how to, because, you know, like, we've been in these situations before where L1 has, like, said, hey, tried to use it. Didn't work. I'm like, yeah, I'm sorry. And then they do something manual, like, if we don't have the manual thing, then what.
Speaker 1 00:22:41 --> 00:22:48 
do we do? Like, um, that's what I'm trying to figure out here.
Speaker 2 00:22:48 --> 00:23:04 
The manual steps are always going to be the same, right? Like, it's going to be more or less ask someone to kill. Or force delete a part and that's about it, right. 
Speaker 1 00:23:04 --> 00:23:40 
Yeah. We can't force delete, let's say it's not this, but it's technically this. Edit the CR and set the state to down. It's confusing, but that's technically as far as we have in terms of a force. And if that doesn't work, then that's when we have to consider the path of deleting the resources themselves, like cryo or whatever, but I'm not going to include that right now.
Speaker 1 00:23:40 --> 00:23:55 
And maybe we should similarly anticipate this situation, verify that it's happened, and then notify stuck, raise exception. For now.
Speaker 2 00:23:55 --> 00:23:59 
Yeah, I think that should be more better.
Speaker 1 00:24:00 --> 00:24:33 
I do want to eventually make it better though, really, I do, because I think it can be, but I don't want to try to do it all at once, because I think, just for, like all I'm concerned about in the scope of this PR is adjusting Mec Bounce Node to be like, like familiar with the fact that Argo CD is like now managing the state of the CR, and so we don't need to, you know, manipulate ourselves, and then also the Startup Probe, taking that into account.
Speaker 2 00:24:33 --> 00:24:42 
Yeah, I mean obviously we'll have to improve on those as we move along, but for now I think that should be pretty good.
Speaker 1 00:24:42 --> 00:25:00 
I'm trying to think if there's anything else, like, delete pods safely, success, if we weren't able to delete the pods safely, then that's when we would…
Speaker 1 00:25:30 --> 00:26:00 
Yeah, five minutes, that's probably enough, because this isn't considering anything about like forcing ArgoCD app to sync. We're just waiting ArgoCD to whatever timer it has going, sending that signal to bring it back up. Assuming it takes like no more than five minutes to do. So I think that's probably a good time frame. But if not, then I think it is.
Speaker 1 00:26:00 --> 00:26:30 
Because let's say, for whatever reason, if the auto-sync is not on, then this would catch that path here. Yep. Wait for pod to be ready. Okay, so, yeah, okay. Wait for pod to be, this is… Yeah, this actually is where all the magic is supposed to happen right here, in terms of the startup probe. Mm-hmm.
Speaker 1 00:26:37 --> 00:27:00 
And so I guess, I'm thinking about it here, because, like, let's say you are in an incident. Mm-hmm, okay. And you do wanna bounce this pod, and you, like, and I guess if it's gotten to…
Speaker 1 00:27:30 --> 00:27:58 
So I think the only state that would be in is if it's in booting and not yet into booted.
Speaker 2 00:27:58 --> 00:28:33 
or sync, that's it. It's not going to be wait for ready. The wait for ready will be first in booting if there is any efficient catch up, right? So we can probably put some, there's some logic saying the node is an efficient catch up and the ETA is going to be so and so, right? That can give you an idea of if I need to wait or should I do something to quicken the process up. So when CR is up within five minutes and we get into the wait for pod ready, the first.
Speaker 2 00:28:33 --> 00:29:05 
output that it should give is say that I'm in booting state now and my efficient catch up ETA is five hours or something like that, right? The first run will always be the longest. The second and third runs will be much shorter. So if I was in an incident at that time, I would probably just say, tell me my ETA, sorry, tell me if the node… What is doing an efficient catch up? If it is, what's the first ETA, right?
Speaker 2 00:29:05 --> 00:29:36 
It might take 20 minutes or 30 minutes for this to show up in the process status. But once we get that number or that value, we should display that on the UI or as an alert, saying, OK, for this task, this is my ETA. And if we think it's five hours or seven hours, then we can probably, forcing the FE to avoid the incident, we probably would, right? And honestly, I mean, we would have to wait for that much time to know that state.
Speaker 2 00:29:36 --> 00:29:43 
Like, even if we were in the incident, we would need to get the process status before deciding what needs to be done.
Speaker 1 00:29:43 --> 00:30:16 
Yeah. Because like, for example, and this would be looking at something similar, but I've sent you a snap of what output we would be also getting this from. Startup probe is failing. The node state is broken. It doesn't currently have any of this other information, but just assume that it will. In this situation here, we can make decisions based on some of this information if it's.
Speaker 1 00:30:16 --> 00:30:47 
being given to us. So I guess that might be a whole other discussion, I'd say, because there's a variety of things that can happen here, like a combination of things that is happening here that we're like, okay, this is never going to be finished. It's not actually ever going to be finished at all. Right? Yeah.
Speaker 2 00:30:47 --> 00:31:01 
I think, yeah, that's how it would be, probably, because, again, the more time we get, the more we would be able to figure out what's going on, but yes, that's how it would be. That's how it should be for now.
Speaker 1 00:31:01 --> 00:31:37 
But the startup probe, like, you know, for context, and I think you already know this, but basically, like, it's, it doesn't, all of this is just for show. It's not making any decisions based off of this. All it cares about is, am I synchronized yet? And then in the meantime, it's getting this shit for us. So we're making decisions based on this, like inside of probably the bounce node. Yeah. So I think maybe we should, we should think about it further, like how to, how to handle.
Speaker 1 00:31:37 --> 00:31:54 
in the context of a node bounce, like this variety of like, you know, or, you know, variability of like different node states and phases. That's what it's called, right? Rebuild phase. 
Speaker 2 00:31:54 --> 00:31:56 
Rebuild phase. Yeah.
Speaker 1 00:31:56 --> 00:32:24 
Okay. Rebuild phase. Efficient catch-up can also have that, yes.
Speaker 1 00:32:30 --> 00:33:00 
Okay, so we would have to figure out how to get that, because all I'm doing right now is getting rebuild status from cash rebuild, so there's probably a different way to get that for efficient catch-up, maybe. But let's say it's in, what's it called, not efficient catch-up, but just regular catch-up, which I don't even think I have here. Anyway, which one of these would be regular catch-up. 
Speaker 2 00:33:00 --> 00:33:04 
There's no regular catch up now on any class. They will all be doing efficient catch up.
Speaker 1 00:33:30 --> 00:34:00 
Things like, we know, maybe we don't have to anticipate everything, but if we know there's certain things that for sure, like it's not synchronized yet, but based on this node state and what's going on here, it's never going to finish. Those are the things I would prefer to know, like, because then we immediately raise an exception and say somebody needs to, it's stuck. So I think that would be like.
Speaker 1 00:34:00 --> 00:34:07 
Error? What am I doing here with error. 
Speaker 2 00:34:07 --> 00:34:10 
Error.
Speaker 1 00:34:10 --> 00:34:42 
I think… Actually, no, I don't think I'm doing anything with those… I'm only looking… Is it… Oh, yeah. Whoops. That was it. So, yeah. Exit when it's synchronized. But I'm not doing anything with, like, let's say it's in booted. Or booting. Or booting…
Speaker 1 00:34:42 --> 00:35:19 
If it's not in synchronized, then I check those other things. But maybe that should be adjusted, too. So, I have another question. So, booted… So, what that used to be… If it was in… Nothing should be stuck in booted now. I mean, the booter would be a very short phase now.
Speaker 2 00:35:19 --> 00:35:29 
Once it's done efficient catch-up, the node should never be stalled in booted. If it is, then we need to be alerted on why it's still in booted and what it's doing.
Speaker 2 00:35:55 --> 00:35:59 
Thanks for watching.
Speaker 1 00:36:01 --> 00:36:33 
God damn it, no, where is it? The fuck? Okay. Booting… Stuck and booted.
Speaker 1 00:36:34 --> 00:36:45 
Ah, also, what about broken? How should we, like… Should this be also an immediate to like, hey, like, this something's fucked, like…
Speaker 2 00:36:48 --> 00:36:49 
So, broken…
Speaker 1 00:36:49 --> 00:36:52 
Let me see. So these two…
Speaker 2 00:36:52 --> 00:36:56 
Broken just needs to alert us based on, it's a broken, right? There's nothing really…
Speaker 1 00:36:56 --> 00:37:08 
Error during boot, which I don't even know… Yeah, that should be correct.
Speaker 2 00:37:20 --> 00:37:25 
It's in a bad state and it needs manual intervention.
Speaker 1 00:37:30 --> 00:37:59 
It's in booting for however much time, we don't really care. Well, we do care, but we're assuming that that's configured correctly at the level of the Helm chart. And… Hmm. Okay. This is the main… I think what will get kind of confusing right here is this.
Speaker 1 00:38:00 --> 00:38:31 
comes to like setting the startup probe to be really long. Like, it's because what if it gets, well, that's why we'll be checking for it. So we shouldn't need to worry about that. Okay, I think. Think. Ah, I guess something to consider as well.
Speaker 1 00:38:32 --> 00:39:03 
is let's say the boot got done and it's synchronized, but okay, that doesn't mean the node is up. It could be that it needs to get through this too. So what do we do here? Waiting error. So if it says it's an error, then do we similarly? Notify.
Speaker 1 00:39:03 --> 00:39:28 
this passage one, so, Let's say it got past the boot But it's technically not up yet And we're still waiting for it to get to a point of like ready not not only synchronized, but also ready. Initially so if it gets to this point where it's just says error like should we, notify.
Speaker 2 00:39:28 --> 00:39:34 
Yep, any any other state we need to get notified so that we can try and action it, before.
Speaker 1 00:39:34 --> 00:39:40 
Wrong time as fast how long should if it's in waiting should we wait I. 
Speaker 2 00:39:42 --> 00:39:46 
Mean why would it be in waiting it would be in waiting if it's a booting or booted, right. 
Speaker 1 00:39:47 --> 00:39:49 
Yeah, but let's say it's in synchronized.
Speaker 2 00:39:51 --> 00:39:59 
If you don't synchronize and waiting is what you're saying yeah, oh Yeah, so that means is waiting in number checks that should probably just be like five minutes or something.
Speaker 1 00:40:01 --> 00:40:03 
Okay, five minutes.
Speaker 2 00:40:03 --> 00:40:06 
Five or ten minutes, yeah. Nothing more than 15 minutes, I would say.
Speaker 1 00:40:06 --> 00:40:36 
And then, yeah, but at that point, then this is it. We're at the homeland. So… Okay. That helps. That's complicated. It is. I don't know how you've done this just without it for so long. But… We'll make it better.
Speaker 2 00:40:38 --> 00:40:42 
Let's see how it goes. But yeah, I mean, this is a problem I solve.
Speaker 1 00:40:45 --> 00:41:11 
I'll make a note for right here and right here. Put a pen in it, come back. But, cool. I think that's all I had on that right now. Anything else? Uh, no, not at the moment.
Copyright©2024 PLAUD.Al.All Rights Reserved
```



```
2024-10-18 15:56:06,829 INFO step_bounce_node Starting bounce process
2024-10-18 15:56:07,173 INFO tunnel starting kubectl tunnel to pod/demodcrtest4-fe2 in tenant-124634-prod
Forwarding from 127.0.0.1:65052 -> 9100
Handling connection for 65052
Handling connection for 65052
2024-10-18 15:56:08,379 INFO tunnel stopping kubectl tunnel to pod/demodcrtest4-fe2 in tenant-124634-prod
2024-10-18 15:56:08,380 INFO step_bounce_node Successfully disabled shutdown protection
2024-10-18 15:56:08,672 INFO step_bounce_node Pod was recently restarted
2024-10-18 15:57:24,490 INFO step_bounce_node Pod was recently restarted
2024-10-18 15:57:24,490 INFO step_bounce_node Waiting for Argo CD to reconcile and bring the custom resource up
2024-10-18 15:57:24,564 INFO step_bounce_node Waiting for node demodcrtest4-fe2 to be up
2024-10-18 15:57:54,947 INFO step_bounce_node Pod is running
2024-10-18 15:57:54,947 INFO step_bounce_node Waiting for node demodcrtest4-fe2 to be ready.
2024-10-18 15:57:54,947 INFO tunnel starting kubectl tunnel to pod/demodcrtest4-fe2 in tenant-124634-prod
Forwarding from 127.0.0.1:65149 -> 9100
Handling connection for 65149
Handling connection for 65149
E1018 15:57:56.066425   36105 portforward.go:409] an error occurred forwarding 65149 -> 9100: error forwarding port 9100 to pod e791d730067eceb1c8db9ec5eaad830df63ba33f895ae12e84ae425266cd64b0, uid : port forward into network namespace "/var/run/netns/210265a0-30d6-4d4b-bfe0-a96ee362d7f2": failed to connect to localhost:9100 inside namespace e791d730067eceb1c8db9ec5eaad830df63ba33f895ae12e84ae425266cd64b0: dial tcp [::1]:9100: connect: connection refused
error: lost connection to pod
2024-10-18 15:57:56,067 INFO tunnel stopping kubectl tunnel to pod/demodcrtest4-fe2 in tenant-124634-prod
2024-10-18 15:58:11,068 INFO tunnel starting kubectl tunnel to pod/demodcrtest4-fe2 in tenant-124634-prod
Forwarding from 127.0.0.1:65176 -> 9100
Handling connection for 65176
Handling connection for 65176
```
