---
id: 9
up: "[[2023-W46]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20231113181704
modified: 20231115000100
aliases:
  - Tuesday - November 14th 2023
linter-yaml-title-alias: Tuesday - November 14th 2023
title: Tuesday - November 14th 2023
week: "[[2023-W46]]"
yearly: "[[2023]]"
quarterly: "[[2023-Q4]]"
monthly: "[[2023-11]]"
daily: "[[2023-11-14]]"
month: "November"
weekday: Tuesday
---

# Tuesday - November 14th 2023

## Tasks

%% TCT_TEMPLATED_START 2023-11-14 00:00 %%
* Recurring
    - [x] Meditate ✅2023-11-14
    - [x] Check Calendar for what events are occuring ✅2023-11-14
    - [x] Go Exercise - Morning ✅2023-11-14
    - [x] Take Vitamins ✅2023-11-14
    - [x] Start Laundry - Morning ✅2023-11-14
    - [x] Fill Up Water Bottles ✅2023-11-14
    - [x] Start Rice Cooker ✅2023-11-14
    - [x] Brush Teeth - Morning ✅2023-11-14
    - [x] Start Dishes - Morning ✅2023-11-14
    - [x] Start Cook/Eat Breakfast ✅2023-11-14
    - [x] Go to 36 degrees ✅2023-11-14
    - [x] Take a shower ✅2023-11-14
* SRE Router ---
    - [x] Complete Design Document ✅2023-11-14
        - [x] Process feedback > Complete Design Document ✅2023-11-14  
%% TCT_TEMPLATED_END 2023-11-14 23:59 %%
* ? Did these tasks align to your Goals?

# Rollover

# Daily Notes


Commented on the helm document jira


---


I got a question. So yeah, if would it be possible for us to add to the GitHub template? Just were not reminder for us since we know but any future like contributors that like if this is a hotfix and we want this in master right now, then it should be like Merge to delivery and if you are emerging to delivery Then you have to make sure that this label is added and I have like a checkbox like yeah for that Actually actually I want to ask you because you started having or I saw those two This was it Yeah, you started putting some documentation here putting together some documentation, yeah this one and this I started commenting, but I I Don't know you still want to make those I hadn't seen your comments here. I didn't publish them. That's why You didn't see them. Yeah, but So for example, it's either that we do this what you say here because it's exactly what you were saying how to open a pull request how to So this can go here. What if it's a hotfix? What I Actually, it's one of my comments here And in this case we can say here that you need to have this label But my question is like should this go here or should it go to this? I think Space that way. Oh, oh, yeah. Yeah. I feel like anything that's specific to the repo we should keep there and link to there from the specific page because then we can keep the documentation as close to the code that we can and then like, you know Link to it as needed Okay, okay. Okay. Yeah, cool. So then then I will continue with reviewing that and maybe we cannot this this there and in the future we

---


357



386

398

401




---


to have it self-contained yeah okay well like in the same way that there's like a source gira i think like a source slack thread should be similarly possible yeah but the thing is the problem is that you have ideas in in slack you cannot i don't believe you can easily search by i don't know topic or whatever the name of the of the post like you you can if you know the idea of the of the thread you can use it but if you don't know it it's really hard to to find it you need to iterate through that and like figure out which one is this like and if you don't keep it somewhere when you first create it well yeah this is yeah the only option how to you know store it in the workflow object is to you know is to create the the initial notification grab the channel grab the thread id and update the running workflow with that that's something that i was not in favor of doing uh because updating a running workflow is you know may result in uh you know unexpected uh results but i don't think we have another option unless we want to store it you know in some you know uh sidekick database or okay i think let's keep it as a nice to have because like in the bottom line like what i what i what i want when i go to the to the ui is like i want to find out what failed and what didn't at at least that's that's me right like i just want to find out what failed and what didn't and yeah i can i if i can open up the uh the workflow itself and the jira i'm good with that like for now for now that's the case for sre router whenever there is no jira ticket like i guess you'll still know that the task failed which is obvious like most important but the context in which like it you know it'd be talked about would you would know that you were you could but you have to go search for it and hey evan in another you know note is that you know if we are going to be creating these jira less uh you know tasks for uh for the mac deployment you know mass mac deployments there probably won't be a separate slight threat for each of these tasks anyway well that's okay for that for that case i but i think for the case that's like responding to an alert that it'd be it'd be helpful yeah yeah yeah i agree as you know since we as you know as soon as we start having these jira less tasks then we probably want to have a link or link it or tie it to a slack thread where we can discuss the thing so yeah okay yeah


---


Even if we change this approach and we have a single, a proper Helm chart version, let's say, we will still need to somehow to say which one is the version that we want to use. And the whole point of this one is like removing this logic from here and just adding it in the variables of the of the task in configuration repo where it belongs. Yeah, that's what that's what that's what we were saying that as we have we have this mech and here it should have somewhere what is the Helm version like as we said, Colo, Helm version, like… I agree. I agree. We should do that. I'm just saying I think that if we did decouple the Helm chart properly from like then we we wouldn't need to designate the the version in the ProvenG code or a configuration for ProvenG code because they like a like a then we would have like almost a separate process to update those those applications. So we would only ever point at the directory that is like DCR has its own, Helm has, or sorry, Express has its own. Yeah, but that's, hold on, because that's if you want to use the latest, let's say, right? So it will go there. It will say, okay, what's the status right now? Okay, that's what I'm using. But what if they come up with the idea that I don't know half of them need to use version 2.2 because like of some reason. Yeah, I think I think what we would need is like I put it in a gist but like a script to like update the application metadata files when when the like as as like a hook to when that file is update when when a specific Helm directory sorry, whenever Helm version is updated like it would update it would propose a pull request to up to update the Yeah, okay, but hold on because like how many pull requests? Well, are you gonna have one per cluster? It will be one, ideally one pull request and that one pull request would contain the changes for all but I think in the case of like OCI that would be not what was desired even though I think we should make our deployment process robust enough to like handle that it's currently not now and only like what is like like, you know what people are comfortable with is doing in like batches, so But I think for like say Express or DCR you would want to you know You have no reason not to just change them all at once but the fact of the matter is we have like a granular like Hold on because you don't realize something that if I have one point to change that and To change that and that will trigger things even if it's like just cutting pull requests How are you gonna cut this pull request if you do it by cluster? You're gonna you're gonna create thousands of pull requests or just one if you do it by if you do it if you do it by Maybe though they don't you can do it by by stage meaning lightning thunder or whatever and and DC but still then like How do you know that they want to do it all at once and they don't want to do some waves or whatever I feel like What would be? Ideal is somehow which I don't know if this is possible Designate it to when the next time these pods come up. They use this next manifest instead of what it currently like so that it happens as a part of the what's already but Let's talk about it later. Not so sure because like because if you do that You are kind of if you change the version of the helmsman you kind of Telling it it might reinstall the whole application. Yeah, I know we got it I don't know. That's something I think we need to like like when we bring this up to sif we should like propose a different process um so I That one that one's more yeah for now it's minor because we're not sure and Okay

---


Pg-scripts hardcoded

---


If we were filtering out what was completed and we're only seeing what like has not completed then what I think would be most useful to see at the top is what last failed. So you wouldn't have to go searching for it. What's most useful when you when you look at it is what last failed. So we're here, right? Oh no, no, I missed. I was here. Okay, because I put the groomed. Okay, give me a sec. This one. Okay. So hold on the latest started at the latest finished. I would I would say just use this one. Number three. Well, it's it's it's not, you know, this this is not three options. It's like it's like one, you know, implementation of the last updated field because workflow object doesn't come with last updated information. We wouldn't have to calculate it from all the individual steps or nodes in the, you know, Argo terminology. So we will have to take started at or finished at and, you know, take the take the, you know, greater value of each values and completed. We have we have a we have a started at and a finished at. We have that right now, right? Yeah. So finished at from my perspective is really last updated. Like that's what it because it's it's it's what last maybe not last updated. I don't know what you call it, but finished is what we need. So what I what I want to know is at the top, what last finished, whether it finished in this in the in the sense that it succeeded or it failed. If it well, yeah, you can you can now filter out like you can say that I'm interested in tasks that are not completed, you know, meaning that are failed or errored. And you can then sort it by by completed at which is not here yet. But yeah, we can have that call. You know, but the last updated is something else because you may have workflows that are still running. Right. And they're being updated right now. OK. But you're not interested in those. Now. Yeah, I'm not really only if it has failed and it's completely failed. OK, so we can have the completed at column here in this table and make it sortable. And that's it, I believe. OK, OK.


---

the situation is, if even one cluster is doing DCR, I'm basically, I can't make the change. So I want to, I guess, I want to use DCR as the guinea pig for how we eventually do this change for the rest of Express and OCI and whatnot, because if we can figure it out and like using this, which is kind of still in its like testing states, then it's like low risk. So that's why I made that one. Yeah. I think though, the solution to this, let me see, but no, you can not, because I was thinking


---

bet on someone in configuration Yeah, well like I guess what's you what's what's interesting about this Is that CIF has control over all the express resources that makes us big pain in the ass? We have to talk to them about everything but with DCR I have complete control over it so we can we can like test how we want to do it as a part of the like proposal Because because they don't have any say in what's happening with DCR since I'm controlling it via Staple set Okay, but this is like kind of again now like if we try to estimate it It's kind of weird because we don't have a clear path here. We just say yeah what we want to do But we still don't know how right? Yeah Yeah, yeah I I have an idea of how but I don't know. Okay, this is like Okay, but this is like just doing What we're discussing like just changing the way we handle it in yeah It's the same thing except we don't have any dependency on CIF on any team It's all on our we can we have complete control versus express Okay, let's then do

---


Well, this is somewhat deceiving because I never went because DCR is like the pods aren't up all the time I I don't I can just make the change directly to the helm chart and it but but Why I created that other JIRA is that that is possible most of the time just because we don't have many clusters running DCR, but it say I want to make a change to the to the to the to DCR I can't Unless I know that no DCRs are running because if what DCR is running and I try to change that it'll make all the pods change and then I'll abrupt the DCR for that even one cluster if it's running so So I guess But sorry, I'm connecting things that don't need to be This is what we do today Mm-hmm. Yeah, we don't need like I'm saying. All right, so for this JIRA, so you can Yeah, like I never have to change helm for DCR I never have to change helm to 0 0 dot 2 because I just don't I don't make changes to that I just don't I don't need to so I'm saying in a similar way We could if we set the process up the like in a better way Then we would never have to we would just have a for mech we would have a helm directory It would have no version in it and neither would neither would cloud because that process would be controlled via something else That's not proven. Gee, I think it's the wrong place to control it is in proven. Gee, I Think we always should need to designate is where the directory is and not give a shit about what the version is Yeah Yeah, but somehow you need to tell which one to use if because if you have version helms out you need to say which version Well, it's no Metadata the metadata files the metadata files designate which version via the target revision Which would court like which would like have a value of whatever the the version is and so it's okay But then you mean that but then you mean we shouldn't be able to change this specific value in the metadata file well, we shouldn't change it and because this is To me, this is part of the deployment. This is something that can change as part of one deployment Maybe you're adding a new service. So you will create a new a new version of the Helm tab, right? Maybe you're adding a new Resource either make or DCR so you will change the Helm tab version and the way you're gonna deploy it. It's either Because you really need to say say somehow you need to deploy a new version of the Helm tab, right? Though in my from my understanding you will always have to change that thing from the provisioning engine If you are to use it for deployment, I Mean I think the only thing that we're changing from for the only thing we should be changing from provisioning in G in my opinion Is the values dot yaml file that is the Helm? Like that contains like express version and other Yeah, yeah, yeah like that for that. I think we should continue to change from provisioning in G But I think we should divorce ourselves from having to manage Metadata files and in what like in versioning of the Helm chart from I Get it. And like if this is the case, like why do we even bother telling them how to do it? That's what I'm saying I'll figure it out myself and then I'll say hey, this is working. I don't know. This is working this way better like the DCR And what will?


---


Is that we have it our repo set up in a way that that would have to be the case but yes like because that's the case Then yeah a script should do that. Not a human so that we could ensure that it's being done like, you know A specific way, but yeah like that script And blah blah blah This was like not this one not this camouflage But this is the version I don't remember. Okay, so it was like source target versus revision Yeah, that's a problem Because it's in the source and since it's in the source If you do it, ah, no That means you need to go One by one and change this value to the metadata file Yeah, so that means that if you change it if the if the person changes that doesn't mean that the You're going to start all of them thousands of classes are going to start doing something like you could set it up in such a way that it did do it like In pieces at a time or you could do it all at once and I think depending on the the application like Like let's just say it's it's not really an application set but it might as well be because we have a metadata file for each application so the application Uh, we can like the the benefit of it being like, you know having separate applications even though it kind of doesn't make any like It makes sense for what we need it for because then we can use the separate application metadata files to control when we apply the change like Via like some kind of script like the one I showed in the gist. Yeah. Okay, but you need to modify those metadata files. Yeah Yeah, that's what you don't want to do it from provisioning yeah, all I want to do from provisioning ng is is is modify the values.yaml files and then wait for the the thing to sync and then the helm chart itself, I consider that to be a different kind of change that's happening than What we're doing when we change the values.yaml because we're not changing like we're potentially changing the structure of all the kubernetes resources When we're changing the metadata file, but when we're changing the values.yaml, we're just interchanging some values Okay, okay So we can use dcr as a guinea pig is what i'm saying yeah, okay I made because I see that now they're using this Uh target revision to specify the branch And actually here go go back to that. Uh real quick go back to the


---


`labels = sre-provng-sprint1-06.11-18.11.23 ORDER BY assignee ASC`

---


What is happening in reality, first of all, there is this, how to say it, at some point we serialize some objects, right, and we save it in this, not params, but vars, but in reality this later on, it's called vars, but it's loaded as params, and this is what's confusing me, because like, I see that this is the file, right, shared vars, in here there is this vars. Yaml, right, this is what we get, and I was looking into this, right, and if you look at this, and you go to the, if you go to the cluster config, right, this is the history of this file, right, this has the right version, the one that you were expecting to send, and from what I understood, there is a local copy, so somebody did git clone on the kubernetes nodes, right, and that's where you read it from, initially, I'm right up to now. Yeah, yeah, I believe you're true, yeah. Okay, so then if you see at the history of this, at this point, this value was not changed, okay, at this point, though, this value did change, so this is the old value, and this is not even the value that you're expecting to see, it's some other one, right, because this was done about 11 days ago, but in reality, this last change changed it to what you are expecting to see, which is this one. Yeah, yeah, I think even the two values… Okay, so hear me out now, hear me out now, hear me out, because I want to get somewhere. Okay, so this is the value that right now is stored in this serialized file, in the vars file, this file I copied over from the pod itself, not the pod, the serve drive, I copied it over, so this makes me, I understand that this value was set to that serialized file when the first time that this task ran, and especially if I understand it correctly, this is filled in when the init step is run. Yeah, that's true. Okay, and then no other step is updating that file. Yeah, it's not, I don't believe it is. Okay, but this, this, this is used actually to pass it to the patch, to the patch, not resource, but method, which is patching the cluster configuration, because what happens is init is running, it creates this file, then configure is coming, and it runs this part, it tries to modify the cluster config, but because the local copy of the cluster configuration is not deviating from the remote, there is no new thing, and it's not modifying the cluster config repo, that's what was happening in the logs that I saw, but I see, I need to see the first run probably now that I think it, think about it, but anyway, at this state, this was not executed, this modify was not executed because the value was set to this, no, sorry, it was set to this, 231 saying, right, and there was no change, but this is not important, then what happens though is that the instance deploy is sending this vars topology, which is actually this file, the old file, this file, yeah, not this file, yeah, and it has the old value in it, and it doesn't change it, because it's still the same value in the actual pod, that makes sense, yeah, so what needs to happen is that I think that in the end of configure step, regardless of what is happening in here, we need somehow to modify that vars yaml, yeah, so the next container that will be executed, it will start, it will have a different value in here, because if you see an instance deploy, when you go to redeploy, when you go to this one, because it is a simple deployment, and it is going in here, right, but this is doing podcr, you just pass it the cluster, and if you want it to be start or stop, this means that it's gonna stop it, it's gonna stop the cluster, and this is gonna start the cluster, because it's true, it's here, this podcr though, it's just using the cluster, it's a part of the topology object, which is passing to this thing, anyway, yeah, that makes sense, that makes sense, and if you see here, this is where it changes, and it gets it from node, and node is coming, if you iterate through the, if you iterate, when you're iterating through nodes in this cluster, cluster, so you go to nodes, you get nodes are coming from it, anyway, it's kind of like, that's what it's doing, it's grabbing the node, actually, yeah, it's grabbing the node from the cluster, and it passes to this podcr, the node, and then in this podcr, the node data is used to fill in this thing, but this all comes from this vars. Yml file, in the init, yeah, that makes sense, which is only created when the init step is executed, and that's what I understood, and it never changes, yeah, because like, if you don't, if you don't do a resubmit, or whatever, or even if you do a resubmit, this will not change, because it will see there is a check in init, I think, that says that if this is there, don't use that, yeah, don't do it again, and that makes sense, why it makes sense, why only the config, that's not changing, but the express version can, I think, because the express version is passed in as a value, it's something else, it's something else, it's used from, yeah, if we go again to this redeploy instance, this redeploy Kubernetes, and we'll go to this pods, the express instance, you said, it's like some express object, which is coming from something else, from the namespace, yeah, yeah, that's why this is changing, or that's why the version, it's not gonna, it's not gonna change, if it would change, but I think pods namespace, express object, I don't know what this is doing, but anyway, now, right now, ah, no, this is doing the actual pods, but this is, this is what can be changed, yeah, and the change set is not changing for this, this reason, but the express object, yeah, that's weird, now, I'm wondering if it all, even the express version is, would change, because node. Version would probably have the old one in there, too, can you go to vars. Yaml, I guess, yeah, yeah, it had the old one, so what, this had to have worked at some point, I don't know what the, ah, this is frustrating, but anyway, that's, that's what I found up to now, yeah, like, um, well, yeah, yeah, I didn't, maybe, maybe I'm wrong, you know, like, I don't know, no, I, I think you're on the right track, this is the, this is what's going on, that makes sense, yeah, because I see that this is old, and I, I told you, like,



---


kubernetes and or kubernetes and not simple deployment just run this thing yeah so i don't know maybe if you can change the name yeah yeah i think i'm going for me i think i'm i'm going to uh do you already have a branch open for this or no no i do have but this is like only for the i'm just taking it now in delivery so i understand what's happening okay but you can you could do it like um it's okay it's okay yeah you could do it like i don't know if you need like maybe create an issue so we know about it okay and just just this just understand this and even this let's say even this redeploy right yeah that what you're saying right now it says like okay simple why yeah yeah here i i can no no i can explain this and i know it doesn't make sense the way that it's like written but here's the point so what we were getting to the point of when we were arguing with sif about this because sif basically you know they don't want to they want to just they want to just manage the kubernetes resources regardless of what is deploying it and we're trying to say to them hey uh prod deployer sucks it sucks ass and they're like well we have nothing to do with that so we're like okay so prod deployer is doing this what it's this is all it's doing to the to the custom resources let's just do the same thing from here uh but like ideally we could use a helm chart but that seems to be too big of a change for right now so instead of trying to build a helm chart for right now what we're what we're going to try and do is for mech deployment task do do the exact same thing that the prod deployer is doing but from our orchestration and unless for whatever reason um they have designated which we we don't have as a configurable like it's not configurable yet but it could be unless for whatever reason we want the to use the prod deployer even though we don't um need to like so the prod deployer should be used if it's mesos because i'm getting confused again this is it controls if we're going to use prod deployer or not yes okay so that's simple simple yeah simple the reason it's simple is that even though it involves more code it's simple because the prod deployer sucks and it's not simple and we can't okay we're not gonna use we're not gonna use prod deployer if it's simple yes no if it's not sorry if simple is if it's simple yeah we redeploy this and that's it yeah it's not simple we did we do this yeah but in this case it can never be not simple because it's it's configured as true uh always so i think what would make probably sense is if on the jira or whatever if that if we had an ability to determine whether or not it's simple and so what that would do is okay in the instance that it's not mesos it is kubernetes but we want it to use the prod deployer and then we because we don't feel comfortable doing it this way then we could use the prod deployer but what we're trying to get away here just one one last bit of context the reason that we can't use the prod deployer for like this type if we were to take the deployment away from the prod deployer the reason we can't use the prod deployer like the like because we spin up a a session for each cluster individually like we don't because if you the way that the prod deployer works is when you spin up a cli session and you don't provide any parameters to it it will load every cluster and that's how they that's how they load that that's how they talk to a whole all the clusters and from a single session in a d in their dc but we only talk to one cluster and and if we were to do that if we were to do that from prob and g we would probably hose the node because we would be doing what it wasn't designed to do we would be spinning up you know hundreds of sessions uh of java you know okay so so yeah instead of doing that we're basically just doing what the prod deployer is doing by patching the resources that's that's that was the goal i don't have time energy or whatever i i seriously didn't understand what you were talking about but it's because i'm kind of like for me at the end and like i'm overwhelmed from yeah the whole day and i can't understand it now but uh i really i'm really interested in like the understanding this part maybe we should continue tomorrow but now it's almost two hours that i'm within the meeting and i need to go and take care of my daughter okay yeah i'll try i'll try to write it down sorry man but i can't now i'm up to you know i don't have capacity i'm sorry i'm sorry to add anything else it's okay it's okay but
