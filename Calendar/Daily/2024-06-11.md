---
up: "[[2024-W24]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20240610223700
modified: 20240611172203
aliases:
  - Tuesday - June 11th 2024
linter-yaml-title-alias: Tuesday - June 11th 2024
title: Tuesday - June 11th 2024
id: 10
week: "[[2024-W24]]"
yearly: "[[2024]]"
quarterly: "[[2024-Q2]]"
monthly: "[[2024-06]]"
daily: "[[2024-06-11]]"
month: "June"
weekday: Tuesday
---

# Tuesday - June 11th 2024

[Update sre_express_demodcrtest1_prod.yaml by erauner · Pull Request #66291 · Atlas/deployment](https://github.medallia.com/Atlas/deployment/pull/66291)

- Express

[Update sre_dcr_demodcrtest1_prod.yaml by erauner · Pull Request #66293 · Atlas/deployment](https://github.medallia.com/Atlas/deployment/pull/66293)

- Dcr

[Configure Memory and CPU Quotas for a Namespace | Kubernetes](https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/)

```
helm template foo apps/dcr/helm_quick_cashing --values apps/dcr/overlays/den/demodcrtest1/values.yaml | kubectl apply -f - --context den
```

```
kubectl get pod demodcrtest1-dcr-0 -n tenant-124630-prod --context den -o json | jq '.spec.containers[].resources'
kubectl describe ResourceQuota dcr-resource-quota -n tenant-124630-prod --context den

{
  "requests": {
    "cpu": "100m",
    "memory": "512Mi"
  }
}
{
  "limits": {
    "memory": "114Gi"
  },
  "requests": {
    "cpu": "2",
    "ephemeral-storage": "10Gi",
    "memory": "114Gi"
  }
}
Name:            dcr-resource-quota
Namespace:       tenant-124630-prod
Resource         Used  Hard
--------         ----  ----
limits.memory    0     660Gi
requests.memory  0     660Gi
```

```
kubectl get events --sort-by='.lastTimestamp' -n tenant-124630-prod --context den
```

```
kubectl delete pod demodcrtest1-dcr-0 -n tenant-124630-prod --context den
```

https://argocd.den.medallia.com/applications/dcr-demodcrtest1

[Kubernetes Pod Policies — preemptionPolicy - Decisive DevOps](https://decisivedevops.com/kubernetes-pod-policies-preemptionpolicy-420826b3de40/#preemptionpolicy)

## Working

To modify the StatefulSet and leverage a PriorityClass and ResourceQuota to prevent more than 100Gi of memory usage by the pods in the namespace, you can follow these steps:

1. Create a new PriorityClass:

```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: dcr-priority
value: 80000000
globalDefault: false
description: "Priority class for DCR pods"
```

This PriorityClass named `dcr-priority` has a value of 80000000, which is higher than most of the existing PriorityClasses except for the most critical ones.

1. Update the StatefulSet to use the new PriorityClass:

In the StatefulSet YAML, add the following line under `spec.template.spec`:

```yaml
priorityClassName: dcr-priority
```

1. Create a ResourceQuota:

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dcr-memory-quota
  namespace: tenant-124630-prod
spec:
  hard:
    requests.memory: 100Gi
    limits.memory: 100Gi
```

This ResourceQuota limits the total memory requests and limits in the namespace to 100Gi.

1. Test the setup:

Apply the PriorityClass, updated StatefulSet, and ResourceQuota to your cluster:

```bash
kubectl apply -f priorityclass.yaml
kubectl apply -f statefulset.yaml
kubectl apply -f resourcequota.yaml
```

Verify that the ResourceQuota is enforced by checking its status:

```bash
kubectl describe resourcequota dcr-memory-quota -n tenant-124630-prod
```

Observe the used and available memory resources.

1. Suggestions for the StatefulSet:

- Consider removing the `privileged: true` security context if it's not strictly necessary, as it grants elevated privileges to the container.
- Review the resource requests and limits for the containers to ensure they are appropriately sized and align with the ResourceQuota.
- Consider using a more specific `topologySpreadConstraints` if you have specific requirements for pod distribution across nodes or zones.

Remember to test these changes thoroughly in a non-production environment before applying them to your production cluster.

Let me know if you have any further questions!

## Network Policies

1. Namespace for DCR resources:

```yaml
apiVersion: v1
kind: Namespace
metadata:
 name: dcr-tenants
```

1. Network Policy in the DCR namespace:

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
 name: allow-be-fe-to-dcr
 namespace: dcr-tenants
spec:
 podSelector:
   matchLabels:
     app.kubernetes.io/component: dcr
 ingress:
 - from:
   - namespaceSelector:
       matchLabels:
         tenant: tenant-124630-prod
     podSelector:
       matchExpressions:
       - key: app.kubernetes.io/component
         operator: In
         values: ["be", "fe"]
 policyTypes:
 - Ingress
```

1. Network Policy in the tenant namespace (e.g., `tenant-124630-prod`):

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
 name: allow-be-fe-to-dcr
 namespace: tenant-124630-prod
spec:
 podSelector:
   matchExpressions:
   - key: app.kubernetes.io/component
     operator: In
     values: ["be", "fe"]
 egress:
 - to:
   - namespaceSelector:
       matchLabels:
         name: dcr-tenants
     podSelector:
       matchLabels:
         app.kubernetes.io/component: dcr
 policyTypes:
 - Egress
```

## Left Off

```
kubectl logs demodcrtest1-dcr-0 -n dcr-tenants -c express --context den
2024/06/10 22:02:34 Configuration will be fetched from Config Service
2024/06/10 22:02:34 Trying to retrieve the config for the 1 out of 3 time
Error when requesting config from http://config-service.config-service/getConfig?application=express&configVersion=master&environment=den.medallia.com&ignoreSecrets=false&overrides=nodeId.var%3Ddemodcrtest1-dcr-0%2CnodeTypeContents.var%3Droles%3DDISTRIBUTED_REBUILDER%250Abase-endpoint%2520%253D%2520http%253A%252F%252F10.205.202.23%253A9100&service=dcr.express&tenant=demodcrtest1: Get "http://config-service.config-service/getConfig?application=express&configVersion=master&environment=den.medallia.com&ignoreSecrets=false&overrides=nodeId.var%3Ddemodcrtest1-dcr-0%2CnodeTypeContents.var%3Droles%3DDISTRIBUTED_REBUILDER%250Abase-endpoint%2520%253D%2520http%253A%252F%252F10.205.202.23%253A9100&service=dcr.express&tenant=demodcrtest1": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
2024/06/10 22:02:49 Trying to retrieve the config for the 2 out of 3 time
Error when requesting config from http://config-service.config-service/getConfig?application=express&configVersion=master&environment=den.medallia.com&ignoreSecrets=false&overrides=nodeId.var%3Ddemodcrtest1-dcr-0%2CnodeTypeContents.var%3Droles%3DDISTRIBUTED_REBUILDER%250Abase-endpoint%2520%253D%2520http%253A%252F%252F10.205.202.23%253A9100&service=dcr.express&tenant=demodcrtest1: Get "http://config-service.config-service/getConfig?application=express&configVersion=master&environment=den.medallia.com&ignoreSecrets=false&overrides=nodeId.var%3Ddemodcrtest1-dcr-0%2CnodeTypeContents.var%3Droles%3DDISTRIBUTED_REBUILDER%250Abase-endpoint%2520%253D%2520http%253A%252F%252F10.205.202.23%253A9100&service=dcr.express&tenant=demodcrtest1": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
2024/06/10 22:03:04 Trying to retrieve the config for the 3 out of 3 time
```

- Need network

## WORKING

Here are the all the resources created by the helm chart

```
kubectl get all -n dcr-tenants --context den
NAME                               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/demodcrtest1-dcr           ClusterIP   None             <none>        9100/TCP   20m
service/demodcrtest1-metrics-dcr   ClusterIP   10.205.138.107   <none>        9273/TCP   20m
service/demodcrtest2-dcr           ClusterIP   None             <none>        9100/TCP   5m50s
service/demodcrtest2-metrics-dcr   ClusterIP   10.205.138.119   <none>        9273/TCP   5m50s
service/demodcrtest3-dcr           ClusterIP   None             <none>        9100/TCP   5m35s
service/demodcrtest3-metrics-dcr   ClusterIP   10.205.148.191   <none>        9273/TCP   5m35s

NAME                                READY   AGE
statefulset.apps/demodcrtest1-dcr   0/0     20m
statefulset.apps/demodcrtest2-dcr   0/0     5m50s
statefulset.apps/demodcrtest3-dcr   0/0     5m34s
```

- 3 sts in `dcr-tenants` namespace

Here is the resource quota:

```
kubectl get resourcequotas -n dcr-tenants --context den
NAME               AGE   REQUEST                     LIMIT
dcr-memory-quota   32s   requests.memory: 0/1310Gi   limits.memory: 0/1310Gi
```

- All replicas are zero currently: `limits.memory: 0/1310Gi`

Turn on a single replica:

```
kubectl get sts -n dcr-tenants --context den
NAME               READY   AGE
demodcrtest1-dcr   0/1     21m
demodcrtest2-dcr   0/0     7m27s
demodcrtest3-dcr   0/0     7m11s
```

- `demodcrtest1-dcr 0/1`

Look at resource quota:

```
kubectl get resourcequotas -n dcr-tenants --context den
NAME               AGE   REQUEST                            LIMIT
dcr-memory-quota   61s   requests.memory: 117248Mi/1310Gi   limits.memory: 117248Mi/1310Gi
```

- Increased: `limits.memory: 117248Mi/1310Gi`

Turn on another replica on a different cluster

```
kubectl get sts -n dcr-tenants --context den
NAME               READY   AGE
demodcrtest1-dcr   0/1     22m
demodcrtest2-dcr   0/0     8m3s
demodcrtest3-dcr   0/1     7m47s
```

- `demodcrtest3-dcr 0/1`

Look at resource Quota

```
kubectl get resourcequotas -n dcr-tenants --context den
NAME               AGE     REQUEST                         LIMIT
dcr-memory-quota   2m37s   requests.memory: 229Gi/1310Gi   limits.memory: 229Gi/1310Gi
```

- Combined resource quota: `limits.memory: 229Gi/1310Gi`

[switch DCR sts to created in single namespace by erauner · Pull Request #66298 · Atlas/deployment](https://github.medallia.com/Atlas/deployment/pull/66298)

https://claude.ai/chat/7e2c9b11-dd24-4f88-b740-b938c3730dd1

```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-tenant-namespaces
  namespace: dcr-tenants
spec:
  podSelector: {}
  ingress:
    - from:
      - namespaceSelector:
          matchExpressions:
          - key: kubernetes.io/metadata.name
            operator: Exists
          - key: kubernetes.io/metadata.name
            operator: MatchRegularExpression
            values: ["^tenant-\\d+-prod$"]
  egress:
    - to:
      - namespaceSelector:
          matchExpressions:
          - key: kubernetes.io/metadata.name
            operator: Exists
          - key: kubernetes.io/metadata.name
            operator: MatchRegularExpression
            values: ["^tenant-\\d+-prod$"]
  policyTypes:
  - Ingress
  - Egress
```

```
{{- $namespace := (include "instance.namespace" .) -}}

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dcr-tenants-namespace
  namespace: {{ $namespace }}
spec:
  podSelector: {}
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            name: dcr-tenants
  egress:
    - to:
      - namespaceSelector:
          matchLabels:
            name: dcr-tenants
  policyTypes:
  - Ingress
  - Egress
```

---

For example, I something I wanted to say as well is if there's anything I've learned here, like recently with the alerts and creating one for creating them for express, is that we should look at the labels that exist on the metrics that are already relevant to us, the alerts and the task metrics and make sure that it is, you know, like, like, all the labels are unified so that if we did want to, we could combine them easily without having to do label replace or recording, you know. And the metrics, the metrics, yeah, like the naming convention is consistent with the other metrics that are relevant to it so that we can easily combine them if we wanted to. Okay, but this is

```
kubectl delete pod busybox-test -n dcr-tenants --context den --grace-period=0 --force
```

```
kubectl --context den run busybox-test --image=release-docker.artifactory.eng.medallia.com/busybox:1.35.0 --image-pull-policy=IfNotPresent --overrides='{"spec": {"containers": [{"name": "busybox-test", "image": "release-docker.artifactory.eng.medallia.com/busybox:1.35.0", "command": ["sh", "-c", "nslookup den-prod-db-demodcrtest1.tenant-124630-prod.svc.cluster.local; sleep 100"], "resources": {"limits": {"memory": "100Mi"}, "requests": {"memory": "100Mi"}}}]}}' -n dcr-tenants -it
```

```
kubectl --context den run busybox-test \
  --image=release-docker.artifactory.eng.medallia.com/busybox:1.35.0 \
  --image-pull-policy=IfNotPresent \
  --overrides='{
    "spec": {
      "dnsPolicy": "ClusterFirst",
      "containers": [
        {
          "name": "busybox-test",
          "image": "release-docker.artifactory.eng.medallia.com/busybox:1.35.0",
          "command": ["sh", "-c", "nslookup den-prod-db-demodcrtest1.tenant-124630-prod.svc.cluster.local; sleep 100"],
          "resources": {
            "limits": {
              "memory": "100Mi"
            },
            "requests": {
              "memory": "100Mi"
            }
          }
        }
      ]
    }
  }' \
  -n dcr-tenants \
  -it
```

```
SREPROVNG-666-5
```

[disable syncPolicy demodcrtest2 by erauner · Pull Request #66456 · Atlas/deployment](https://github.medallia.com/Atlas/deployment/pull/66456)
