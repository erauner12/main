---
id: 9
up: "[[2023-W48]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20231127000100
modified: 20231128000100
aliases:
  - Monday - November 27th 2023
linter-yaml-title-alias: Monday - November 27th 2023
title: Monday - November 27th 2023
week: "[[2023-W48]]"
yearly: "[[2023]]"
quarterly: "[[2023-Q4]]"
monthly: "[[2023-11]]"
daily: "[[2023-11-27]]"
month: "November"
weekday: Monday
---

# Monday - November 27th 2023

## Tasks

%% TCT_TEMPLATED_START 2023-11-27 00:00 %%
* Home ==-
    - [x] Cook chicken that is marinating ✅2023-11-27
* Recurring
    - [x] Check Calendar for what events are occuring ✅2023-11-27
    - [x] Meditate ✅2023-11-27
    - [x] Take Vitamins ✅2023-11-27
    - [x] Fill Up/Drink Water Bottles ✅2023-11-27
    - [x] Brush Teeth - Morning ✅2023-11-27
    - [x] Catch up on Email Inbox - Morning ✅2023-11-27
* Recurring Weekly -=-
    - [x] Order Groceries ✅2023-11-27  
%% TCT_TEMPLATED_END 2023-11-27 23:59 %%
* ? Did these tasks align to your Goals?

# Rollover

# Daily Notes

But then in the next step, that we want to merge these changes, we recalculate that branch name, and we try to find a pull request with a new branch name. And it doesn't find it and it errors out. Okay, okay. See what's happening. So let's see what exactly you want to accomplish. And either you need to move this to DCR, if for DCR that makes sense, or we could do it but taking under consideration that some things cannot have step-specific branch name. I see, yeah. Because they are interconnected between many steps. So just like a real quick summarization is that like DCR, I figured this out with DCR because I had this issue. I needed to make, within the scope of a single workflow, changes to the same repository more than one time. So the second time that it looks, it can't look at what was, so I noticed that's exactly what's happening in the provisioning workflow. Okay. But yeah. So it doesn't have, so it doesn't, you don't have the changes anymore. So let's say in one step you make some changes, you use one branch name, and then the next step you cannot see those changes in the remote. There's this method from what I remember called is GitHub PR merged, and then it looks through all the branches, and it sees that that one that was from the previous step, it was already merged. So even though it's a completely different step, but a completely different, which is the one where it creates the lightning step, like because it thinks that that thing is already done, even though it's a related step. Okay, okay, okay. But then in this case, it seems that this is like a DCR specific, and I understand because the method is like get PR or create, or something like that. Yeah. But it is DCR specific in my case, but it also applies to the provisioning workflow from what I can tell, unless I'm misunderstanding it. No, no. I think you are because like in this, it creates problems in the case of provisioning because, and I can show you why. Because if I go to provisioning, right, and there is like instance configure, and instance configure merge, right? So these are two different steps, and this is happening first, and then like it's configure merge. Yeah. But this one, this is doing modified deployment repo, and it uses, where is the branch? The branch name. Yeah. Which is, which before it was calculated per step, so it had the step name. So in this case, this would be like, I don't know, it's like the Jira ticket, Jira ID, underscore, the ID of the, of the hash, I think, of the commit, or not the hash, but the workflow ID, right? And then you were using step name. So it was like going to be, I don't know, step instance configure. And that's doing changes. But then, you go to instance deploy, sorry, instance configure merge, you will go merge configuration PR to master, and in here, it's trying to get the pending pull request. But here, it's going to search with instance configure merge. Yeah. And it finds nothing. And it was throwing me this error. Yeah. I see. I see. You see what was going on. So in the case of provisioning, we cannot do that. But if it makes sense for you, because like you do in one step, you, you make one change and then in the next step, you want to make a change, but again, to the same workspace, to the same repository. But because you're using the same branch name, it seems that it's already merged this part. Yeah. In this case of like, in your case, like we need to do it uniquely, but it's okay if you still want it. And it's relevant for this year. Yeah. It makes sense. And it's working. And it will not create any problems. Then just apply what you wanted in the, in this abstract step, create a, no, you should do it. Instead of doing it in abstract step, you should do that in the platform specific abstract step. So here. I already was. And that, that was the point of my PR is that I wanted to make that also available for Mac in that same way. Cause I think that was, was going to fix my issue, but it sounds like I just introduced more issues in doing that. So I mean, What was your issue with, with MEC related to MEC? That's what I didn't get. It's it's this two 76. Which I can show you later, but, but basically it's, it looks like the, the step that is case namespace, which happens like fairly like early is, Is merging something. And then later, whenever the step lightning, which is at the very end, when that occurs, it, it, it thinks it already made the change. Hence what it's just not adding the entry to the XML file to, to put the express instance into the deployment, which is lightning. So basically it thinks that already did it, even though it didn't. Okay. So, and you're using this modified deployment report to do that, like, or look, whatever method you are, you want to, this is happening in a, you just need to, to change the branch name that you're using there. And like, you can, you can just overwrite the specific method in your abstract step or create like in the, in the, so I would say, I would say, so in DCR, in start farm, those things are happening. Hey guys, I'll probably drop something for me. Sorry. In the, yeah. And like the history updates and then the, okay. Thank you. Bye. Bye. Startup. Sorry, what the history update and then the startup farm, for example, like that's, that's where it's like, I'm making a change to the deployment repo once, and then I'm making a change to it the second time on the startup farm. Let me, let me tell you, like, can we, let's sync up in a bit. Yeah. Cause I need to grab lunch now. Late lunch. And yeah, let's sync up in a bit and I will, we will figure it out. Okay. Cool. Okay. Sorry, man. No, no, you don't have to be sorry. You got it. So we'll talk about it. Okay. We will figure it out. Like how you can do it because in the bottom line, you can, you can just override whatever method you need to make steps specific, like our platform specific. We can just override it first called and called super with whatever we want, whatever different parameters we want. Well, I think we need to figure out how to override it at the step level, not just the whole platform, because this seems, you can. Okay, cool. That's, that's what I'm saying. Like, which, which is, which step you're doing that. So, well, no, like the, where the problem lies is in Mac, not VCR. So go, go back to Mac and then open up step. Kates namespace. Yeah. Provision. Step. Under. Okay. Okay. So it does one here where it creates. If it, if the namespace doesn't exist, I'm sorry, you are calling us. So you are calling make modules, make steps. Yeah. So go to DCR. No, no, no, no, sorry. So, so DCR, I have solved that problem with DCR long ago. And I noticed that the same exact problem exists in, in, in, in Mac. And that's what, that's what I'm trying to fix. Okay. Okay. Okay. Listen, you will explain in a bit later. And like, now I get where the problem is and like, we will see how we do it. Okay. Okay. Yeah. Thanks. Yes. See you. Yeah.



---


Here's a cleaner version of the Jira description, with the same information organized for clarity:

---

**Problem Description:**
1. **Requirement for Two Separate PRs**:
   * There's a need to create two separate pull requests (PRs) in the `{{clusterconfig-[dc]}}` repository via `{{MecProvisionInstance}}`.

2. **Non-Unique Branch Names**:
   * The branch names generated are currently not unique enough, especially when checked by `{{github.was_branch_merged}}`.
   * The uniqueness should extend to the step level, not just the context ID. More details are provided below.

**Example Log Entry:**
``` 
2023-06-16 18:46:43,898 INFO github_mixin branch SREPROV-3072_URMO7YZ2 of git@github.medallia.com: DeployerConfig/clusterconfig-sea1.git is already merged to master, ignoring
```

**Cause of the Issue:**
* The problem arises due to the way branch names are generated and checked in various scripts and modules:
  * `provisioning-ng/step_k8s_namespace.py` ([view code](https://github.medallia.com/medallia/provisioning-ng/blob/master/prov_platform/mec/MecProvisionInstance/step_k8s_namespace.py#L72))
  * `provisioning-ng/github_mixin.py` ([view code](https://github.medallia.com/medallia/provisioning-ng/blob/master/lib/mixins/github_mixin.py#L56-L58))
* This issue is particularly evident during the execution of the lightning step:
  * `provisioning-ng/step_lightning.py` ([view code](https://github.medallia.com/medallia/provisioning-ng/blob/master/prov_platform/mec/MecProvisionInstance/step_lightning.py#L22))
  * `provisioning-ng/deployment_group.py` ([view code](https://github.medallia.com/medallia/provisioning-ng/blob/master/prov_platform/mec/deployment_group.py#L97-L100))

**Example of Issue:**
* An existing merged branch `{{SREPROV-3072_URMO7YZ2}}` is checked again, leading to issues. Refer to the PR [#2041 in DeployerConfig/clusterconfig-sea1](https://github.medallia.com/DeployerConfig/clusterconfig-sea1/pull/2041) for an example.

**Proposed Solution:**
* Modify the branch naming convention to include the step name for uniqueness, such as:
  * `{{SREPROV-3072_URMO7YZ2_StepK8sNamespace}}`
  * `{{SREPROV-3072_URMO7YZ2_StepLightning}}`
* This change will ensure that when branches are checked [here](https://github.medallia.com/medallia/provisioning-ng/blob/master/lib/mixins/github_mixin.py#L56-L58), they will be unique and not previously merged:
``` 
if self.github.was_branch_merged(github_repo, work_branch, master_branch):
    log.info(f"branch {work_branch} of {repo_url} is already merged to {master_branch}, ignoring")
    return None
```



---


start much sooner till the 30th farmer is in hidden state, right? So that means we will kind of be confused as to when the DCR start and is it actually making progress and we might, I mean, by we, I mean, if I hand this off to L1 and others, it just becomes a bit complicated to write SOPs. But if we have like a status, which we can tell them, okay, once all farmers are in hidden state, that's when your DCR is going to start and you can then start observing the process status. I mean, do you want to like get, what do you think? Do you think that matters or do we then just look at metrics and we just look at the process status to confirm if a DCR is successful or failing and things like that? What we could do is we could add a new step. You know how we were seeing DCR in process status in yellow and nothing below it, right? We could have a waiting for farmers stage and that could show a progress of X out of minimum Y already there. So in the process status page, you would be able to see that information. Oh, so good, good, good. That's a good idea. I think I like that. That makes it very easy to implement. Right. So if you can do that, that would be the best, much easier for us to write SOPs. But I have a question on that now is, you said if minimum farmers are ready, so the minimum number is going to be three for us. That means let's assume for even a Walmart or a T-Mobile in the future, if three farmers are up, it's going to start the DCR, right? That's okay. That's what we want or that's not how we want this to work? We could override that and we could make it to where, if we wanted, no? No matter what that number is, the principle is the same. If we have, can you give me, I'm going to get out and I'm going to go to a booth because I'm screaming in the middle of everybody. Give me five minutes, I'll be back. Murtaza, what I'm thinking though is technically right, because we have to know every, like when I say we, I mean, oh. Hey, so what I was saying is technically right now, like when we start DCR for a client, we have to know how many farmers to give it, based on how large we know the client is. But technically what we could set, configure at the tenant level and have a default for in the Helm chart is, if this client's massive, we're not going to let this start until it lets, until it becomes this, until it has this many farmers and then just have us. The thing is that, that's what Sergio was telling us, right? That with dynamic farmers, we don't, we can start the rebuild with three, but they will keep scaling up to the number that we have them at, like say 40 or 20, right? And once they get into this, the topology, they will, they will distribute the work amongst themselves. So it's not going to break anything. It's always really, there's no need to wait for 30 farmers to be up before the DCR actually starts. That's what I understood. So he was trying to tell us. What's interesting to me specifically is being able to set a default amount of farmers and only use an overridable amount of farmers if it's configured at the tenant level, because then we could take out of the JIRA or whatever it is that we use to initiate this. We could take out the option for the user to need to designate the amount of farmers. We could just have that default to a certain amount and in the values.yaml, like if we know it's a Walmart or something, we make it like 20, 30, 40, whatever. And then whenever the JIRA goes into approved, less information is required from the user. It just starts with what we have configured. That's what, that's what I'm, that's what I'm thinking. I mean, we can have that configuration in config repo if we need to, with whatever number we decide for large customers or medium customers and leave it at that. We don't have to keep that as a JIRA entry anymore. So yeah, that, that, that, that, that makes sense. We can do that. Well, I don't want to make it in the configure rate. I want to make like maybe the default for all DCR in the configuration repo, but at the, at the instance level, I'd like to configure that in the deployment repo because that's what's most, that's what requires no config hash. If we need a config hash, then it's, then, then they're going to need to know how to, which config hash to use. Like I want, I want less of that. Okay. We can, we can keep it in the deployment repo as well. Like, yeah, that's fine. I mean, that shouldn't be too much of a problem. Noticing, noticing how much farmers we need, but, but, but Sergio, the question then is, sorry, the answer you were giving was, it doesn't matter if you have three farmers up for a cluster that needs 30 farmers because the DCR is going to begin. But once the remaining 27 come up, they're going to distribute the work between all 30 and it's not going to restart anything or it's not going to slow anything down. It's just going to keep moving onwards. So it really doesn't make any difference if you keep the default minimum to be three or 10, it's fine. They come up, DCR begins, and then the other 20, 15 nodes can join in, distribute the work amongst themselves and just continue doing what they were. So that answers my question, if that is correct, what you were trying to say. Yes. We are in a good state. We are in a happy state with that. The thing with that is that it's something that I think is a good decision to make at this point is that we have been testing static DCR, which is what we run so far up to 685. But from 686 on, the default is going to be DCR balance. So I don't think we need to do a lot more of testing looking at DCR static. We need to switch to DCR balance like right now. Okay. Because everything that we are going to be using this on is going to be 686 and up. Okay. Right. Okay. So in DCR balance, yes, that is true. You can actually start with one farmer if you want and move on from there. Yeah. Okay. Now that you say that, though, what you said about Murtaza, okay, so say it hits a minimum of like two, three farmers, but we have like 30 set to hit. If it's going to just go ahead and start doing that until the rest come up, then I guess that's what we want. Right. So that means if using IP is okay then, because this also addresses the problem that I was bringing up with the IP or the FTDN required for the farmers. Now, even if it's the IP and changing the readiness check is okay, because that means even though they're not in hidden state, right? Even if one farmer comes up in the hidden state out of those 30 is going to join the cluster, start the DCR. No, sorry. You don't want to start the DCR then. We wait for the three minimum required farmers to be in that state. They will begin the DCR. The others will start coming in, getting into the hidden state and joining the topology, distributing the work to themselves and so on and so forth, right? Until the last one is up, which is the 30th. And then you would know the exact ETA for your cash table to complete. Maybe it starts with three farmers at seven days, because obviously there were only three farmers, but then as 30 farmers joined the topology and they all have distributed the work that ETA should then definitely come down to like maybe two or three days, right? That's how I assume this should look like. Okay. So yeah, let's try with the E686 release if you have that and we can play around with that. We can, I mean, we can use this version. This version is master. So this version that we have right now should be a very simple configuration change to make it DCR balanced. Sure. If you give us the property, we can make that. And then let me, Evan, yes, what's the question? The question that we, so what, so for example, if we were to take these three farmers down right now and then restart the farmers, would it restart DCR or is there something that we need to do to trigger it? It will actually fail DCR and start the 30 minutes wait time. And then it's going to try starting again. Once you are below the minimum numbers, it will abort the DCR. The only time you have to wait for the minimum time is when initializing the DCR. That makes sense. That is good actually. But then now that brings me to my other question, which I forgot, was if you go with the IP instead of the FPDN. So let's assume during the rebuild, we have a DCR going on or we have some hardware issues. We restart two or three farmers. Those farmers IP is going to change. Okay. Now that is not a problem. Hell yeah. Yes. Good to confirm because that is what I was trying to confirm. Because actually when they come up, even if they have the same name, nobody's going to care about the name. They're going to start from scratch. Okay. I mean, even if any, even if it had like a dynamic IP address assigned to them, when they restart, they are going to start from zero. I mean, nothing that was done by that node before is going to be considered valid. Okay. That's good to know because, because I was assuming like how IPs and backends work today, they have all these RPC calls going on and those RPC calls are using static IPs, right? So if the IP changes for those services, it's not going to pick up that IP without a restart. So I think now with the dynamic DCR that you have built, we are not dependent on the IP anymore, which, which actually that allows us to use this change much more seamlessly and we don't have to think too much into how to make it more robust. This isn't that robust. Like your code itself is robust enough. We don't need to worry about the state of one individual node. Like we just, we're just, we're able to spin them up like as instances, like, you know, I don't know, just as we want. Like, I think that's the way that it should be. I mean, it makes sense. I think this is perfect. Let's, let's get the dynamic.


---

without changing the overrides, we need to think about the structure and put it the right way. All right. So let's finish the testing, right? And then, like I said, I'll rethink the exact. That's why I didn't even approve or merge the change on the weekend, because I just didn't want to do that over a long weekend and have any issues over the weekend. I purchased first thing today when I got in. It's just because, like, now, OK, we are online. We can figure out if things break. But nothing was going to break. It was a change which was not going to impact anything. But we can now fine-tune it further to help us only make these changes on the DCR for nodes and the synchronizer only, and not the back end of the FP. Because right now, this property file is going to get applied to every Express node that comes up. And probably, we may not want to do that, just to keep things cleaner. Makes sense. Makes sense, so that if we want to, like, reverse-engineer and see why we are configuring the way we have. It just makes it easier to understand that, OK, this is only a DCR change, or this is, like, an Express. It's just keeping it stupidly simple, right? Like, for someone who's even stupid like me can just read and understand that, OK, this is for DCR, and this is for Express, right? And I mean, we don't have to keep asking Sergey or keep asking RDP why this property file is in this specific location.

---


like, using this as a way to, like, to me, like, actually, let me know what you guys think, like, but what it means to me is that we can actually start using Argo CD as the deployer, like, without needing, like, this dedicated, like, proprietary deployment mechanism that's understanding the state of one node, or another node, or whatever, like, we can stop, like, treating them, like, like, so special, you know, but I mean, obviously, not right now. But like, just what it means is that we can do it for DCR means it can eventually be done for like front ends. I think that needs a bit more work. But yes, if Sergio is ready to take that project up sometime in the future, we might want to have the best to be done. I would love that to happen. But I mean, that and that can be like, that was the one we dreamt of when we started this, say, okay, we don't need to have like, I don't know, in Walmart, we don't need to have 20, 20 front ends up all the time. Maybe during, I don't know, eight hours of the day, we can have like four. And then when the real traffic hits, we have 30. Yeah, like, and I feel like what, again, like, I don't want to get too ahead of myself here. But you know, we like doing this with just front ends, like using this exact chart would be trivial to just bring up another staple set. It won't be a replica, but it will, if we're going to use the Express operator, but it can be just another staple set, just bringing up another one. And if it is able to dynamically like attach itself to the cluster, like I'm imagining what that would look like for like cluster let FEs, you know. But, but yeah, there are some things that we still need to solve. Like, for instance, we have a lot of a lot of the storage that is, I mean, all of S3 is attached to a node name, right? So storage, the property files, there's a lot of other things, which is outside even the RDP scope. So it's not just that. So make that I mean, obviously, if RDP can make that like so you can work and finish and give us that he says, Okay, now the FEs can be dynamic. We still have a lot of other dependent teams that have to make changes on their kind of code. And we have to then make changes in the infrastructure layer to allow for this to also work seamlessly, right? Like the feed servers has like its own mount, the NFS mounts, the shared work directory mounts as well, like all that has to go away, which we want to go away. We don't want NFS anymore inside Express. But it's just like, there are a lot of handholding required. And RDP is not just I mean, RDP is the important one, because they have the biggest part of the code base that has to be fixed. But then there is integrations, there will be Cocoa, there'll be like, TA, there'll be ourselves because of the infrastructure. So it's going to be a lot of work, but it's definitely becoming more fascinating that we're getting there. Like at least now we know it can be done. And now it means that we just have to focus on how it can be done for Express FEs and the backend itself, right? Tomorrow, maybe this might allow even to have two backends, because now you can dynamically. That is not going to happen. In theory, we can have two backends, it means that we don't need the backend anymore. Right? That would be the best, right? There's no backend. The FE works as a backend. Like, I guess what is so timely about what progress we're making here for me is that like on the prod services side, or SRE side, there's been a lot of discussion about, okay, we got to figure out what to do with MEK deployments, because MEK deployments are not great. Like we need to make them better. Okay, so what's going to be the way that we choose to do this? Are we going to take the prod deployer and make it a service? And I guess, like, without completely dismissing the idea, I'm trying to argue that that's not the way to go. Like, we need to figure out how to make this work with like, you know, something like Argo CD, but the application has to be adapted to that. So like, before we go down this, like, rabbit hole, that is like, you know, completely like restructuring, revamping the product player to do something like I think we need to make it we need to, I guess we need we need proof to the rest of SRE that that's possible. And I think DCR is that proof. Yep, yep. Makes sense. Yep. Yeah, I really love what we are achieving here. And the speed we are moving with. I mean, if we had to trust on one of the regular environments, what we did in the last two weeks, it would have taken probably more than a quarter. Yeah. It's it's Argo CD, man. It's not me. It really is just Argo CD. I don't know. I'm very happy with it. And I think you guys are I really appreciate that you are that you can recognize the value in this and I'm doing a big effort. I really appreciate that. Oh, yeah, no problem. Something else that I wanted to mention real quick that I was thinking about, that I think that might be more relevant to Murtaza, but I'm just going to bring it up anyway. So the way that we're currently doing like waiting in the in the pipeline, we're making a get change to Argo CD, or sorry, to the to the deployment repo and then Argo CD is we're just waiting for that to sync it. So I'm just waiting for the staple set to reach a certain amount of pods. But what I think I'm going to like consider changing it to do instead is I'm going to I'm going to turn like I want to turn DCR into instead of it being like an application, I want it to be an application set so that we can change. And so that instead of waiting for the staple set, right, I'm going to make the change in Git. And then we're going to either through the GUI or via the API trigger the application, the Argo CD application to sync, because if because then that is the because right now, if we were to like, say all of the instances that we have, if we were to change one thing about the chart, and it's going to the way that the application is set up is that it's going to start applying that immediately. And if we have a bunch of clusters that are in the middle of a DCR, that's not what we want to do. We don't want that to change. We don't want that to sync until we want it to. So we want to, we want to be able to make the change in Git for every cluster, but then defer the like new the incoming changes until a point that we're choosing which would either be triggered by automation or manually through the GUI or through this Argo CD CLI, whatever, like, we just we don't want to have to be in this state where we're like, controlling that, like at the express application, or express instance to instance layer, like we want to, we want to, we want to use Argo CD to do that, I think. And I need to think out how that works. Yeah, that's, that's, that's, I don't see any issues with that, that's actually going to make things a lot more simpler. So if we can make them if it's going to be an easier port to move them to application sets, instead of an application, I have, I have no concerns. I mean, it's actually just going to make things a lot more simple for us. Mm hmm. We won't have a metadata file for each express cluster, we'll have one metadata file, and then we have a clusters.yaml file, which will have all of the clusters in it. And then we'll make changes to that as we need it, but it won't affect any existing clusters as they're doing their thing. It'll just, it will like the next time that they do sync, make the like update those changes, but it won't. And really, the reason I started thinking about this is because this is going to benefit us and express most, like, it'll benefit us in DCR, just not stepping on the toes of other, like DCRs that are running, but this will benefit us and express and that we have this problem where, you know, we don't want to make changes to all clusters at once via get change, we want to make the get change and then control when we want the rest, the express cluster in a in a context of a group to get that change. So we want, we want to use the application syncing, like trigger to enact those changes. So we can control when it happens. So I'm going to, I'm going to test that with DCR as a part of a separate effort. Sure, sure. That sounds good. I think I understood about half of what you said. Yes. Thanks. Okay.


---




And solving the problems for the old version, it did not allow us to move forward with a new version, which is where we really want to go. So now that that is happening, I like how we are looking. We really like, I mean, I guess we need to harden this first but we need to figure out eventually who to bring this information to so that it can be like a part of the discussion as to where we go next and what we do next. Because if this is like not considered, then it's gonna be a hard sell. I think we need to get this in front of the right people. Bye. Don't worry, if we get this to a point where we can run it in 686, this is going to make a big splash. Cool. Because everybody loves DCR, everybody hates that it's so buggy. And if I say, okay, we don't need to worry about configuring an offline cluster, we don't need to worry about snapshot, we don't need to worry about running it six times because some of the nodes went down. I think it's going to, it's going toDon't sell for a second. Momentum on its own. So it's just gonna be noticed once it's started being in use. Exactly. On clusters and then what we need to kind of get more traction on later is gonna be on how do we start implementing or adding these changes that we've begun to make with DCR into more of the Express build, right? I mean, so this will allow us to then start focusing on how we want to make deployer changes, how we want to make infra changes around this, how we want to get RDP maybe to get more focus to look at making dynamic FEs, a possibility in the future, which then would mean you're getting a project of that stature, but then obviously this all has to come in because obviously RDP has a lot of projects going on. So you cannot just keep focusing on this, like quarter after quarter. But again, we have to then start selling it up to Robert and Mike and Robert, right? Is that, okay, then this is something which is now a game changer, which is helping us scale, speed up deployments and have less handholding required for these, which means DCIP does not have to wait for a cache rebuild to complete on 10 clusters. DCIPs can just move seamlessly along and restart the DCR farmers, but that's not gonna restart the rebuild. So like SIF is unblocked by the DCIP activities and our deployments are not gonna break and we both are gonna be happy. So that means we get more time back because that means that's less, a lot less of decision-making, which I have to make with SA, most of the times that a DCIP is going on, oh, this cluster is doing DC, oh, sorry, this cluster is doing DCR, this cluster is doing a cache rebuild, don't touch this, don't touch that. And something happens by mistake, you restart a node, oh, God damn it, our RMD reaches out and why the hell did this rebuild restart, it was a critical customer, they wanted a change to go out by the end of this week. So all these are automatic, what do you call those, praises for the tool, which will be noticed and they themselves are gonna bring this up to Robert at some point. Like RMD, Sabrina, when she sees this, she will definitely start bringing this up and talking about this, being so helpful and so good that even without us having to do any work, it's gonna sell itself, right? Like that's what I'm trying to say, Evan. So we need to make sure that this is buyer-proof. I agree with you and where I'd like to harden this the most, like, yes, in the pipeline that's executing it, but I want, I mean, eventually the goal, right, is to set up Argo CD in such a way in which it's declarative and we don't need some tool like ProvenG to execute it. I mean, I'm glad that we have this right now to help us do this so that we can do it faster and more easily, but we need to set up a proper Helm chart. We need to have the Helm chart be an artifact itself. We need to set up the deployment repo in a right way. We need, like there, we need this to be an effort. Because right now in OCI with Express, for example, we're just winging it. We have no idea, it's not even set up ideally at all. Like that's why every time we try to deploy to it, it just fails miserably because we haven't set it up a system to be able to iterate on it like a chart. Like it's, like we really just need to like pull it apart and be like, okay, this is the process we're gonna use to update no matter what Kubernetes types of resources, whether it be like we're adding a network policy or we're adding a replica to a DCR node. Like we need that to be a process defined by a specific project or Helm chart or a combination of the two. Like, and that's just, that doesn't exist right now. And I don't, what I'm, my goal is not to make ProvenG like the thing that is doing it all. It's only like supplementing what is an, like, it's only supplementing the infrastructure beneath it, which is Argo CD, deployment repo, Helm chart. Like that's the, that's where the real logic actually is, should lie, the express operator, stuff like that. So, I mean, that's just my opinion. Like that, that's where the most work needs to be. That's all gonna fall into place, right? I mean, once we get this done, we can then start focusing on improving on that pipeline because then you're not testing or we don't need to be testing with RDP. We can work on that within ourselves and then figure out how best to improve the pipeline. But at least right now we finish fireproofing, like Sergio said, with him, so that once all that is signed off on, then Sergio's inputs are not needed. And we then just focus on the pipeline to allow for this to be more seamlessly applied across multiple clusters with less handholding or less time taken from ourselves. But at least we have like agreed upon and tested thoroughly all the other changes that are being made, right? Like the code that Sergio's made, looking at how it's gonna work, any of the readiness checks that we need. So we've already checked off almost all the boxes. There's just now one change we need to do with the DCR properties to allow dynamic DCR. And then when we can apply that, we test that, and then we can continue working on the pipeline on our side, the background. Cool, cool. So I'm gonna drop from this call. I have to go on another call. But Sergio, you can pay based on Zoom, sorry, Slack, the SQLs that you need, and I'll unblock, I true for test. And Evan, you can maybe, once you finish, I don't know if you have had lunch, but maybe you can come back and then start with the synchronizer using the DCR property changes that we need, which Sergio mentioned, and then we can take it from there and see how that cluster comes up and behaves. And then we can do the same again with I true for test once it's unblocked. Okay, sounds good. Cool. Perfect. Nice talking to you guys and amazing work, Sergio. Thank you again. Yeah, thank you. Excellent, lots of fun. I'll see you guys. Bye-bye. Okay.


---


## Sync Node Configuration

1. **Startup Probe:**
    
    * Executes a `curl` command to check the node's status, looking for "waiting" in the response.
    * Waits 300 seconds (5 minutes) before the first check, then checks every 300 seconds (5 minutes).
    * Allows up to 10 failed checks before considering the startup a failure.
    * This configuration gives the sync node a significant amount of time to start up properly before the probe starts assessing its state.
2. **Readiness Probe:**
    
    * Determines if the node is ready to handle traffic by checking for "ready" in the response.
    * Starts checking 30 seconds after the container begins, then every 3600 seconds (1 hour).
    * The node is allowed up to 15 failed checks before it's considered not ready to handle traffic.
    * The readiness probe has a very long interval between checks, which means once the node is ready, it will be a while before its readiness state is reassessed.
3. **Liveness Probe:**
    
    * Regularly checks the health of the node, considering it healthy if the response is either "waiting" or "ready".
    * Begins checking 30 seconds after the container starts, then every 10 seconds.
    * If the response is neither "waiting" nor "ready" for 3 consecutive checks, the container will be restarted.
    * The liveness probe has a standard configuration, ensuring the node is checked frequently for its ongoing health status.
