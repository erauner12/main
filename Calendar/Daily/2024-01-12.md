---
id: 9
up: "[[2024-W2]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20240112000100
modified: 20240115144147
aliases:
  - Friday - January 12th 2024
linter-yaml-title-alias: Friday - January 12th 2024
title: Friday - January 12th 2024
week: "[[2024-W2]]"
yearly: "[[2024]]"
quarterly: "[[2024-Q1]]"
monthly: "[[2024-01]]"
daily: "[[2024-01-12]]"
month: "January"
weekday: Friday
---

# Friday - January 12th 2024




## Procedure

**Step 1:** Acknowledge the Alert
- Ensure that you have received and acknowledged the `express_jetty_threads_utilization_too_high` alert in Slack.

**Step 2:** Update Stakeholders
- Immediately inform team members by tagging SRE in the Slack channel created by the alert. Provide a brief summary and mention that you are starting the troubleshooting process.

**Step 3:** Check Database Status
- This alert may trigger if a database is being relocated. Check if a DB relocation is occurring. If so, tag MDBS for further assistance.

**Step 4:** Write Out the Caches
- Execute the necessary commands or scripts to clear the caches.

**Step 5:** Record JVM Behavior
- If possible, take a Java Flight Recorder (JFR) snapshot of the JVM. Add the JFR data to the EEQ ticket and/or link it in the Slack channel for reference.

**Step 6:** Restart the Impacted JVM(s)
- Identify the node causing the alert.
- If the Jetty threads have not started to decrease after a prolonged period, gracefully restart the JVM to see if it resolves the high Jetty thread utilization.
- Monitor the utilization post-restart to confirm if the issue is resolved.
- If restarting the specific JVM that triggered the alert doesn't resolve the issue:
    - Consider restarting other nodes as a preventative measure. This can help reset the thread count and potentially prevent the issue from escalating.
    - If the thread count doesn't stabilize post-restart, a deeper investigation into the root cause will be necessary.

**Step 7:** Create an EEQ Ticket (if needed)
- Before escalating the issue or taking further drastic measures, create an EEQ Jira to investigate the root cause of the problem.
- Mention in the ticket the details of the alert, steps taken so far, and any relevant information. This will serve as a record for future reference and ensure that the issue is thoroughly investigated.
- To avoid creating an immediate incident, SRE will determine what might be causing the Jetty threads to spike.

**Step 8:** Monitor and Escalate (if needed)
- If you observe related Pingdom or Downtime alerts triggering post the steps above, escalate the issue to `#eng-ops-all` immediately.
- Provide them with a summary of the alert, actions taken so far, and any other relevant details to help them take over the troubleshooting process.

---
