---
id: 9
up: "[[2023-W44]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20231031000100
modified: 20231104090402
aliases:
  - Tuesday - October 31st 2023
linter-yaml-title-alias: Tuesday - October 31st 2023
title: Tuesday - October 31st 2023
week: "[[2023-W44]]"
yearly: "[[2023]]"
quarterly: "[[2023-Q4]]"
monthly: "[[2023-10]]"
daily: "[[2023-10-31]]"
month: "October"
weekday: Tuesday
---

# Tuesday - October 31st 2023

## Tasks

%% TCT_TEMPLATED_START 2023-10-31 00:00 %%

%% TCT_TEMPLATED_END 2023-10-31 23:59 %%
- ? Did these tasks align to your Goals?

# Rollover

# Daily Notes


- Spent some time looking into what Evan proposed last week about the help chat of Express
- One of the topics discussed was researching about the Argo event
- Kamil was also present

---



This is a common concept for ingesting things somewhere. If you work with Hadoop, for example, there are really similar concepts that you basically take some information, transform it somehow, and then you push it to some kind of sink to be stored somewhere or do something with it. So I wouldn't say it's an Argo event or my concept. It's like a generic concept of processing some kind of event. Anyway, but it really seems to be a good fit, because yes, you can define multiple sources, including webhooks, which is like you can even write your own, since this is a webhook. We're working with posts. And then in the triggers, you can have whatever. You can trigger Argo workflows, or you can trigger Kubernetes jobs. And see here, like create any Kubernetes object. And this is kind of really fitting the use case. So now I'm kind of torn. Then I'm thinking like maybe we should really look into utilizing this instead of just like, because we're going to write something similar, very similar to this. Yeah. I, well, go ahead. Go ahead. Go ahead. Come here. Well, you know, we really need to find out if it covers really everything that we're expecting, because we probably will have to have some kind of database. In this sketch, it says like either NoSQL or Jira or some kind of database, because we probably may have to have some kind of external access for both users or some kind of applications that would be interested in getting the existing events, or from inside, like updating an existing event. Because I can imagine that when the event workflow starts, like when we probably create a Slack channel for an incoming event, we probably want to update the database with the channel ID or the thread ID that this relates to. So yeah, and that's because someone else might be interested in getting that thread ID or channel where the topic is discussed or where the updates to that event are being pushed. So database, be it NoSQL or Jira or something, is definitely something that we may want to include in this. Plus, another thing.


---


maybe all of them, will be defined by lots of, or not lots, but defined by several steps, like creating a Slack channel, creating a Jira entry, spawning the API to ask maybe something else. And that's something that sounds to me very arbitrary. So when using Argo events, we probably will be matching just the, you know, when you outline that there is something like triggers, we probably will be utilizing Argo workflow to, you know, perform all these steps, or maybe something else, I don't know. But a workflow-like, you know, flow sounds good to me in this kind of thing. But I was thinking that since it's flexible and you can create any Kubernetes object as a trigger, we could just spin up a job and do what we want, and like, get done with it. But you are right that I also wonder, I was also, I also was wondering, like, what kind of like data it keeps and where, and like, for how long? Like, because that could replace the database need or whatever, but for sure, if we really need that for reporting, I don't know, visually to somebody to see what kind of events and so on, like, we really will need the database. On the other hand, we could even use exporters, you know, to push metrics and so on, like, export metrics from the thing, so we know how many and why and so on, but anyway. Yeah, I think that Evan mentioned, you know, right, about some use case for the database. So, I mean, again, I don't know, like, everything about Argo events and like, what, how, but at least as far as like, the design as it currently like, stands, I don't think we would have anything that would cover the ad hoc requests with Argo events, because the event itself is a Slack or whatever we decided to be, but in this case, I think what is desired is Slack. So I don't, or maybe like an event, it could similarly listen to a Slack event, I don't know. But it seems like, for like alerts, for alerts, it seems like this does make sense, but for the ad hoc request, I don't know how, like, if it would translate the same. And it seems like as well, like, yes, we can map an event that happens in Argo events directly to, like, let's say an HTTP request to the Province GAPI server to do whatever the thing is, but what about that interaction that is supposed to happen with the user, like, be it like Slack or Jira, like, because that's their interaction point. Like, if it's just immediately like, sending to and doing the thing, like, that's great, but I feel like there's some, there's some, like, tasks that we don't, like, we want to know when it's about to happen, but we don't want to immediately execute on it. We want to, like, provide the user a confirmation whether or not they want to dismiss it or if they want to proceed with that thing before we're completely comfortable with, like, executing it, like, automatically in causing more issues, if not, like, vetted in this way. So that's what I don't know how, like, Argo events will be able to do. I think that this later case is, or should be covered by the event workflow. Like, there might be some kind of, you know, pre, there might be some initial steps like creating a Slack channel or Jira issue, but the execution itself, I mean, the remediation of the alert, you know, as you say, might be, or might be postponed.


---


for someone to concern, right? Which I think is what you'reâ€¦ But this could be a separate event, right? So, let's say the first event is some kind of alarm system or whatever, like some slide creating the initial, or creating the initial event, and then like, if you need to wait, you just react to that initial event, and then let's say that whoever will respond wants to proceed, like it's a separate event. I guess. It could be. I would really start, maybe not start, but I would really like to see a list of events that we're talking about or alerts, because some of them might be obvious, like a new alert from alert manager is raised, and then we need to restart a Mac note or something. But some of them might not be that obvious, because there definitely will be events like that something has been resolved, right? That's an event. But we probably don't want to generate a workflow for that, or we just want to update something that's already existing. So, I just sent a thing in there. That's what like an alert, one individual alert would look like, which usually would come in, like as probably in a list of alerts that are coming from like multiple sources, I guess, like multiple express instances. Let's just call it that, even though it can be more than that. But like, that's what like one alert would look like and how we could model it with like a Pydantic if we went the Python route. But I guess what I'm wondering is if what we're talking about and regarding Argo events right now is like it being a queue that we're acting off of, instead of trying to create our own queue. Is that like what we're saying here or no? Would Argo events effectively be the thing that is receiving the events themselves, but act as a queue for us in our program or no? No, but you can have an action to put something in the queue. For example, but it's like, it's not, it's like, it's gonna do immediately something like it gets an event and it does something with it. It's not for queuing and different events, if that's what you're saying. But imagine that it could be in front of everything and just like the actions that it will do, it will do one single trigger, it will have one single trigger, add things in our queue. So maybe, so if you're, if that's what, so if you were saying we could have this part be just simply this, just this part to be Argo events. For example, and then it puts the request into a queue and so on, but yeah, it's just, it's not a queuing system if that's what you were asking. Yeah, okay, okay. So in this case, you're saying the event manager would be like the SRE router and what we would expect as input to it would be this like event that is coming in initially into Argo events and then we're picking it up from there? Yeah, I think that that's what Varsamis meant. Yeah.

---


So how exactly would the flow look in terms of event manager picking up the event from the event API in this diagram? Like, would that be immediate somehow in the same way or no? I think it would be immediate. I think that the Argo events in this case would have a single trigger in the terms of Argo events. So the trigger will be the event manager here. And it would either process the alert immediately or do something with it, just store it in the database, or based on some kind of logic, it would handle it somehow. Yes, yes. Because then we wouldn't have to model that. We would expect. We are saving ourselves, as Kamil said, to create our own API. Yes, but I have one comment to that. We would have to really make sure that the Argo events sources, or whatever the correct term is, that really the sources that are available in Argo events really cover everything what we need, starting from the alert manager, going through Pingdom, and maybe something more. Webhooks, webhooks. This is what it says they have already. But then they have webhooks, which is like, you can do really anything with that. Like anything can send a POST request to this hook. Like, I mean, yeah. Yeah. So Slack could send a POST request to this hook, for example. OK, yeah. Slack. That's also something else to consider, because this doesn't exist right now. No, we don't have an Argo events instance. So it means that we will need to get one going, plus we need to maintain it. Right? Yeah, definitely. So that's another thing to consider. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Yeah. Well, considering we're also technically managing Argo workflows solely, we would be additionally taking Argo events, which I'm not opposed to. I'm just like, that would probably be what happens. Because in the bottom line here, OK, now we say one gets. So just to get the event, so you just wanted to have the ability to get a list of all events or something like that with this one, right? I think that in this concept, if you're really utilizing Argo events as an API to create new incidents or create new alerts.


---

Me too, because I don't know anything about that. We may quickly find out that it doesn't work out. But it seems likeYeah, it didn't spin up also an instance like you're in to try it, like I was just reading about it. Well, it seems like, yeah, webhooks in this case is what we're concerned about, but look at all those other sources, or is that what it was called, event source? All those other ones, we wouldn't have to additionally implement those if we ever were to use them. It would just be something we could adapt to the event manager piece that we would write instead of having to, I mean, not saying that we have like a use case for any of those yet, but the fact is we wouldn't have to if we built it on top of Argo events. That's true. On the other hand, always when you're using some kind of existing API to receive something instead of writing your own, you very often find out that you need to somehow modify the things that you expect to work because you need to adapt to the existing API instead of having the freedom of creating your own, but I'm not proposing anything. I'm not mandating that we need to write the new API from scratch, but that's just something just to be aware of. And I'm not sure that we will need like this type of, because this is a different end point, right? Different end points. In this proposal, I, you know, I count with the, that all of the remote systems like the alert manager, Pingdom, or the manual request that would suit for Slack as well, have their own kind of structure of the payload that they send out, right? And we would have to adapt to that. So giraffe or the alert manager sense when they, you know, trigger the alert, they, I don't know if you can freely modify how the request is going to look like. Look for, I cannot say for alert manager,


---

There might be an alert that, you know, maybe Ivan, you know, a lot more than me in this. So putting together all the alert types that we're expecting that this system will receive. Yeah. Yeah. That's what we need. I mean, again, like I've only considered alert manager realistically up until this point. Nothing else has been considered other than that. I've mentioned Pingdom, but I haven't even like modeled it. It's just alert manager has been my, the only one I've cared about so far, but like an example of what, like one of those alerts look like that I've sent, I sent that in the zoom. I think I could get a couple more just like, but it's going to look very similar. One's going to be like resolved and the other is going to be firing, but they're all going to have very similar data. Okay. So if this is the case, so we could try to map this to the second point that I have here and it's the, some kind of POC of the Argo events or at least the minimal use of Argo events in terms of having the event source replacing the API or our custom API and everything will go to a single trigger, which is going to, which will be the event or events manager that we will be putting together. So that is what, like what is that? Let's summarize it somehow. So try to POC the Argo events, what we can achieve with that, if it really matches our use case or if not, what I can do or what we can do is to deploy Argo events to dev. I don't think that's, you know, something that we wouldn't or shouldn't be able to do and see if it matches the left side of the diagram. So did I write it correctly? Try to POC Argo events used instead of defining our own API for receiving events. Yeah. Okay. Maybe we could, we will find out that we can really expand Argo events to do basically everything, but at this point, I don't think it's the case because we, you would have to introduce, you know, some kind of logic into the system because there might be use cases when the alerting system fires, you know, several alerts and we will be, we will want to process just a single one because they are, you know, some kind of glitch or, you know, for some reason we will be processing just a single one in a row. There might be some, you know, some logic that we would, it might be very difficult to achieve in Argo events. Maybe not, I don't know. But at this point, I think that having some kind of more control over the alert or managing the alerts that we receive is something good to have. So one last thing I wanted to mention. So at this point, yeah, the Python application just becomes like a handler of various event types. One being a webhook, a webhook that is a particular type of webhook, which we would make that distinction within our application. What, you know, is it a webhook that is Pingdom? Is it a webhook that is alert manager and then, or say it wasn't a webhook at all. It was a completely different event type, but like, which we wouldn't immediately implement. But is that kind of what, sorry if this is obvious, I'm just like reiterating it, like that's what it would effectively be. Yeah, I think so. The Python application will, or Varasames, could you please switch the diagram? Yeah, so the Python application will start at the middle of this, or in the middle of this picture, which is the event manager. Yes, the right side, everything that's from the right side from here, it's going to be the Python application and it's basically it's going to receive the events coming from Argo events, which will basically, all of them will go to a single trigger, which is going to be the event manager here. And it will try to map it to, because I suppose that in the event, we will be having the information from which system this alert is, you know, originating from, and we will be able to map it to some kind of workflow, yes, maybe that's best. That's what I wanted to say also, and could you point it out, because I was watching this, and we could simply have one more thing here, which will say what is the source of this, so we could send it as a payload, let's say. Event source, yeah, yeah, yeah. Definitely, something like the task types that we have on the SRE API, because we definitely need to know the original system that generated the alert, because that may be the factor when deciding, you know, to which SRE API task we want to map this alert to. Yep, okay.

---


repo, and also push different versions to artifactory as artifact. Because in the bottom line, you always need the the TGZ file that to apply the help child, right? That's, that's what we were saying. Yeah, that's, I mean, keeping the values in the deployment repository. Yeah, I feel like decoupling this would be beneficial to us. And, okay, yeah, there are there are those some problems with that. Because in order to do that, or let's, let's take a step back and see what's what's happening. So what happens in the in Argo CD and in the application definition, you can define a source. And it's noted down here that the values when this source is a help child repository, this can be the repo URL can be either a git repo or like a help repo. In our case, it's always a git repo. So if you go to an existing one, let me see. So if you go to an existing one, the repo URL is always a git URL. Right? And it's either or it's going to be either a git or a URL for help repository which looking it's like a server like an HTTP server. Okay, but the problem with this is that the the repository well this help child are defined and the values should be always in the same repository because the values and the reason behind that is that the values are referred to by a relative path. So you can not they that's why they always need to be in the same repository. So the way things are now. So then Evan found out about this feature, which is like you can have you can actually define separate sources and one would be the chart repository and the other one would be the where the values are existing, right? But this feature is still in beta. But plus it's available from version 2.6 and on of Argo CDN we are still at the 2.5 as you can see here. So the short answer to this is that it's kind of not possible right now. Well, like, at the moment, let me see. So could you go back to that? Sorry, I was here. Yeah. Yeah. Sorry, where like, so on the documentation, it's saying like, it's in beta in that, you know, the UI and CLI, like, are going to expect it to be a certain way. And I guess what argument I have is do we use the the Argos UI and CLI in this way? Like, would it need to have parity with what this look, the problem is that right now, as you did try it, you show that it's like, what was your test, like, it's failing, and it's failing exactly because of this, because in the existing No, it's not. It doesn't exist like this, you cannot define that resource that has the spec like this looking with using these black sources. Can you not like, I don't know that for certain yet, because the reason it's failing now is actually a some validation related issue. No, no, no, it's no, no, they Yeah, but this given the validation is say exactly this that you saw, what is your I don't think that's true, though. I mean, I could be wrong.

---


So, one question I have here, so in terms of like what options we have considering the version that we're currently on, it's not supporting what I was proposing and I didn't realize that until now. So what option we would have to be able to accomplish this would be to relocate not only the chart but also the values files to a single place and ideally both of those in the same place. I think this is also not, I don't think that people would go for it because it's like you're saying that I will have a separate deployment repo just for Express. Well I mean, point to me what other application the deployment repo has the same requirements of Express. I think there's not anything close to it, which is why we haven't, you know, customized No, there are people that are using Helm charts and that's what I was trying to find out. So for example, this application is having charts and they deploy Elasticsearch with it. Is this multi-tenant? Sorry? Is this like a multi-tenant application though? I don't know this, I don't know what it is like but it's Athena, like it's one application that they're using Helm charts and if I didn't delete that, yeah this, but it's a few, few of them because then, yeah, yeah some Stella is doing it like for Redis. I think Stella and Athena are the only ones that I found. I think that the distinction still, unless I can be, unless we can find something, like I think the distinction still is that Express not only needs Helm for its like own reasons but also needs a multi-tenancy support, like which I guess would be what warrants having a separate repo if we were to decide that. I'm not saying we should, I'm just saying that would be the justification that the others could make. But look, the, what you're saying, like multi-tenancy, like okay, here in digital for example,


---
