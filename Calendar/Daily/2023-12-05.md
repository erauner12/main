---
id: 9
up: "[[2023-W49]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20231205000100
modified: 20231206000100
aliases:
  - Tuesday - December 5th 2023
linter-yaml-title-alias: Tuesday - December 5th 2023
title: Tuesday - December 5th 2023
week: "[[2023-W49]]"
yearly: "[[2023]]"
quarterly: "[[2023-Q4]]"
monthly: "[[2023-12]]"
daily: "[[2023-12-05]]"
month: "December"
weekday: Tuesday
---

# Tuesday - December 5th 2023

## Tasks

%% TCT_TEMPLATED_START 2023-12-05 00:00 %%
* Recurring
    - [x] Gratitude - Morning ✅2023-12-05
    - [x] Fill Up/Drink Water Bottles ✅2023-12-05
    - [x] Go Exercise - Morning ✅2023-12-05
    - [x] Check Calendar for what events are occuring ✅2023-12-05
    - [x] Take Vitamins ✅2023-12-05  
%% TCT_TEMPLATED_END 2023-12-05 23:59 %%
* ? Did these tasks align to your Goals?

# Rollover

# Daily Notes

It could give you, like, it could, I think it could be made actionable. I'm not exactly sure how yet, but I imagine, like, once we start sending these to them to be, like, addressing manually, we'll know what, how, and what is actionable, because I'm sure it'll become, like, you know, apparent, like, that it's the same thing that's being done each time that they get it. But I, it's, I'm not sure yet. I just know, I just know that, like, with the prod deployer, for example, they know whenever the deployer messed up, and then they handle that accordingly. Well, we don't have the equivalent of that in OCI. I think that, you know, the reason why the application is not syncing, or is degraded, or whatever, might be, there might be a ton of, you know, different reasons, right? And we probably, you know, I think that, at least from the beginning, we probably won't be able to spawn some automatic action to, you know, sync it, or fix the reason. But I don't know. Maybe we will be in some specified cases. I don't know. Yeah, I'm not sure yet. Just step one for now, or step zero. Yep, definitely. Um, okay. Sorry. I think the threshold is too low. Yeah. Because if it's like, for one minute, it's not synced, that can happen, or unhealthy, that can happen. And like, anyway, but it's still in place. Yeah, I think I probably need to adjust that to upper, yeah. Anyway, okay. Should we start the…


---


Let's say you uh, that's what I want to to suggest that you keep you can keep a clean clean history and like As I said, like if you want to change something else, of course, you can do a second commit and so but it's a it's good like if we kind of Start doing this so we keep our history cleaner in github And we don't have like just creating one and try to fix I don't know something and like we do multiple commits like something like this yeah, because I ask I ask for comments, for example, and then we need to Redo a create another commit if you if you did this git commit amend locally To your in your repository and use force push Then you will still have one commit your pull request will be open and so on. I think it's better and cleaner Uh Wait, yeah, I agree I I tried to do something similar, but it's not the same and I wanted to actually ask a question about it. I do. Um, Like i'll push that commit and whenever I want to like, um consolidate the commits i'll go and uh, like take the commit and then What's the word like? Um I can't remember But basically take it and squash them. Yeah squash it into its parent I'll squash multiple commits into its parent and then force push it but it seems like this is effectively the same thing But what I what I noticed as well is that sometimes like let's say I merge master The latest changes from master into mine so that I can or I carry so is it still possible to do this? Uh when like Using you're talking about the basing right? Yeah, like yeah, are you gonna have multiple commits or no? Yes. Yes. So for example, let's say that i'm working on a feature and you do Changes, uh in master and you we made some changes in in master and like I want these changes So yes, I can do so in that case. I will do git pull That's that's the base from master right and then and then I can It will replace my history it will replace my commit By pulling also applying first your changes and then applying my changes on top of them and then I can also do git git Force push and it will rewrite my history the history in my feature branch without closing the pull request or Or anything. Yeah, see that's perfect. That's exactly what I need. Yeah That that can that it works also and that's another thing it's good that you pointed out because like I would suggest that uh, you you should pull for changes like Every day, if you know that things could merge or if even if you don't know just do a git pull So you have always the latest because right now there might be cases that You are testing something and because of some changes I did unrelated to your change some other file Things will not work as you expected For example, and then you might test everything everything would be nice, but then we put to production. It's not working like for example Okay. Anyway, that was it. That was the first topic like then Kamil wanted to demonstrate like some do a small demo put some new features in Uh In the api server, so you want it I can share yeah It's related to the um for the Probably make a deploy cluster tasks that's going to be uh initiated by a by a single jira ticket So that multiple deploy cluster, uh tasks will be will be submitted without Uh individual jira tickets created, right? That's what we what we saw last time The drawback of this as we know is that we won't be able to You know operate or manage the tasks because we simply don't have jira tickets for them And we need to substitute it in the in the ui. So I have several tasks here Some of them are running one is failed Uh, what's new here is this manage task? Uh section here. So when it's failed, you can go ahead and retry the the task One thing to note here is that we allow this or these buttons uh will appear here only if the If this specific task does not have a jira reference Right, uh, it means that this jira It might not be completely empty as in this case, but it doesn't have the jira issue jira issue key here So if you look into the spec, uh Basically We have the jira object here and it has several properties and one of them is the issue a string so if if this is empty or Simply not there It means that we consider it the task, uh being uh without jira and in that case These buttons will appear here So, uh, if I click in on this this one, this is a normal exec groovy It doesn't have these buttons and that's because it has the issue reference here Right And Kamil, can I ask like is is it we're not limiting to who can see those buttons like Remember we were discussing this a bit In the past Yeah, not yet. It's it's open to everyone who's logged in via google, but that's yeah, if we wanted to limit who can Execute these actions. We can further modify this Do this but for now if I tap retry here it's gonna say tasks retried and You know, if I could get back to the uh, it's now submitted, right? And waits waits to be unsubmitted. Similarly uh, I cannot retry this one because it's it's uh It's still running But I can terminate it Nice, but there is no there is no but there is no resubmit, right? There is no resubmit, uh because this uh, I don't know if you want that, uh, to be honest it's it's like um It it may cause some um You know unexpected things to happen, but let's consider that maybe we could have We could have a resubmit because it's it's it's basically generates a new task, right? It's it doesn't modify this one. It generates a new task. So uh Yeah, let's think about it. But for now, uh, it's just retry and and Yeah, terminate. Yeah Yeah, I was just thinking thinking because like As far as I as I know up to now that when we are trying to do a bug fix Uh, we need to resubmit the task so it picks up the new image that we would Yeah, yeah, yeah, but that's I would say Yeah, but that's another use case right this is specifically targeted to the tasks that you know someone or were submitted with some kind of Quote and quote operator or or some kind of controller be it You know a single jira ticket created for the mass mac deploy cluster deployments, right? Uh, so yeah, and it should work with just terminate and retry since these are the two operations that we think or you know that Uh, we agreed on that. This is going to be something usual that the users might want to execute or perform Either terminate running tasks or retry the failed ones if they wanted to do something more, right? Like resubmit or use different image Uh, I think that they should create another jira ticket to do that. Yeah. Yeah. Okay


---


Always like be setting the time out at the Argo workflow, like later, we should set it to, like, let's say probably two, three days. And I kind of documented like what my plan is in that separate JIRA, because there's going to be two things that we want to do. One being, we want to monitor the cache rebuild status, because right now that kind of gets done in like with SRE bot. And it's kind of just not efficient, because we have like two separate programs. One, you know, is watching, it's just, if we do this right, then it should be the job that's the source of truth, the SRE API job. If it's running, that means the cache rebuild is running. If it's not running, that means it's either failed or it's completed. That should be the goal, so that we don't have to have any of these external programs like, like, you know, being the thing that's, yeah. And I'm thinking, yeah, accommodate. So there's there's two things I kind of want to do in this JIRA, is like create that additional step, like you said, to wait until it's done. But I also want to monitor it. And you know how we have that ability, at least in the mech deploy, it'll say like, it'll report like, as a, as a part of the like step status that, that it's still going. Well, I want to report to the step status what the rebuild percentage is using that, using that cache rebuild status command. Okay. Okay. Yeah. Okay. Anyway, okay. We'll see. Yeah, man. It's a, it's, it's kind of a work in progress for sure. This whole thing. But I'm glad we're getting a chance to restart it with like your kind of supervision, because I think we'll be able to harden it. It will make it a lot better. I would, I would say you need to also consider having it as a job. Like as we did here, because that, that's why I was asking, like, what is it stopping it? You know, because like, if you can somehow, because in this case, you would not need, you would not need to have it in, in cloud configuration at all. You know, in, in, in, sorry, in deployment repo at all, because if you had a job and you run a job, you could just simply read directly from cluster config. What is the current version or from the instances themselves, right? Get the current version and start, start it as a job using that express version. Yeah, we, we could, but I think that would be the wrong approach because I feel that the approach that we need to start in, like, I feel like we need to like have in, in I I'm trying to figure out how to say this right. Like, I think that we need to be able to have a source of truth that is something like the get repository inside of the deployment repo. Do I think it needs to be in the deployment repo? No, I think from my perspective, I'm just saying, because from my perspective, if this was a job, you could get all the data you need, and you could speed it is spinning up dynamically. So I mean, you can read what is the express version? You got, you got this piece of information, you can find the size of the size of the class, the cluster, is it a big, medium or large one is the smaller, medium or large one. And having those information, you can spin up different type of job dynamically without having defined somewhere. But what we're losing in that process is the ability to so with a new rebalance, like logic, we'd be able we wouldn't be able to have as easily the ability to like add replicas to an existing cache rebuild, say we spin it up with 10 nodes by default. And we're like, yep, this is gonna take like a week, and we need it to take a day. So we're gonna spin it up to 100. And yeah, but imagine that if you but imagine that if you if you grab cache size, a size of the cluster, and the version or whatever else you need, you can make this decision beforehand. Because if you if you know that there is a cluster with a very huge cache, you will know that I need like, I don't know, 50 plus things, or the number of replicas could be proportional to the size of the cache. So you if I have a cache of 10 gigabytes, I get two replicas, if I have 15, I get three replicas, if I get and so on, you know, go exponentially or whatever, like, and grab because this will also give you the ability to be modest when it needs and overdo it when you when you need. So, you know, you would if you make this decision, based on some numbers, you would always spin up like, I would say the according like, depending on what what you're dealing with the creating the this amount of replicas, I don't know, I agree, I agree. And I've actually been like trying to ask multiple people, Murtaza, Sergio, like, can you guys give us like, an exact like, way to determine how many because so that we don't have to determine this each time that we start a farm, and I haven't gotten any clear answers. So which is why, like, I think, what what's useful about Argo CD and helm is that I can handle these conditionals outside of our code completely. Okay, like, we're able to, like use provengy to orchestrate this to some extent, but really, my goal is not to make provengy the only thing I want, I want, I want as least amount of logic as we can. Yeah, there is another another thing you should take under consideration. Another thing you, you don't want also to overload the clusters with a demanding resources. So your logic shouldn't be greedy. Like you shouldn't say, Oh, it's gonna take like a week. Let's give it another 50 nodes. How do you know that this is possible? How do you know that other services might not be disrupted if you if you start doing those things? You know, yeah, I mean, this is also something to consider. I'm not saying that I have the answer. I'm just saying consider it. Yeah, you're right. You're right. And I think that the the constraint that we have now is that we have we're using these topology constraints. So we want to make sure that, like, let's say each pod is only coming up on, we're not going to have more than a certain amount of pods on a Kubernetes node. And if there is not enough, like capacity in the cluster, then Argo CD is going to try to do it. But it's gonna be like, Hey, I mean, I know what you want me to do, but I can't do it. And, and, and that, and I don't need to, the only thing I need to be aware of from proven G is whether or not it's, it's, it's actually happening. Is it sinking or not? And ideally, like right now, what I'm doing is just watching the staple set directly. But I think what would be ideal if we could get the logic right, is that we're just watching whether or not the Argo application is healthy and in sync. And because then I actually don't even need to know if it's a staple set, or if it's a Kubernetes custom resource, it's doing it. All I need to know is what, what is deployed, if what's deploying it is saying that it's healthy. And if the readiness probes and the startup probes are set up correctly, then it shouldn't be healthy until, until it is. Yeah. Okay. But anyway, okay. Let's say, I hope, I hope what I've shown you today is, is no, it was helpful because I understand better. I would like to have the answer though, to what exactly is that cache? Like, is the database cache? Is it like a server responses cache? Like, what the hell is that cache? Yeah, I need to find, I need to find that answer. Yeah, it's just, it's just like, I guess the reason I don't know that answer is because that layer is not relevant to me. It's just like, what's relevant to me is getting the resources up and then bringing them down. And then making sure what happens in between those things, because this last thing I'll say on it. But what, what, what this, what this kind of looks like without this orchestration is it takes like two weeks to do a cache rebuild because somebody will be like, Hey, Murtaza, we need this done. And so he himself is the orchestrator. He's like, all right, I need to do this. I need to do this admin command. I need to do that. I need to do this. I need to do. And he's trying to do that across all these clusters at the same time. And that's a, that's an overwhelming thing, you know, so like, and so in the prod deploy, wouldn't I, I, I, I see it's, I see its case for like managing an express cluster, but for something like this, that requires more than just, you know, okay, bring this node up. It's, this is more like dynamic in that, like, things need to happen in between that the, the, the, that the prod deploy or simply was not able to do, and I would argue should not do like this should, this should, in my opinion, like the DCR should be our test child. And I've actually talked to Sergio about this. He's like, I want, I want to be able to control express from Argo CD and not have to ask Larry and SIF and all of the, you know, the whole gang to, to be able to deploy express in this. Sergio, who's the, on the platform team. Okay. So we're kind of working hand in hand and using VCR as a, as a guinea pig to, to deploy express in the way that it probably eventually needs to be done. Okay. Okay. Okay. And so I guess we didn't get to get, we didn't get to the other topic relating to DCR that I brought up to you last week. But we can, we can get to it at a later point if you got to go, but I did, I did find, I did find a way to satisfy both the requirements of keeping the, the, the, the,



---



```yaml
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: dcr
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
  labels:
    app.medallia.com/environment: production
    app.medallia.com/team: sre
spec:
  generators:
    - list:
        elements:
          - instance: eraunertest2
            namespace: 'tenant-123882-prod'
            expressVersion: '8d86720aab45f820e32439a9f5d8fbdb198f0bbd'
            enableSynchronizer: 'false'
            enableFarmers: 'false'
            nodeCount: '0'
            dc: den
            chartVersion: '0.0.1'
            configChangeset: 'master'
  template:
    metadata:
      name: dcr-{{.instance}}
      labels:
        app.medallia.com/environment: production
        app.medallia.com/team: sre
        express.medallia.com/cluster-name: '{{.instance}}'
    spec:
      goTemplate: true
      goTemplateOptions: ["missingkey=error"]
      project: dcr
      destination:
        namespace: '{{.namespace}}'
        server: https://kubernetes.default.svc
      source:
        helm:
          releaseName: 'dcr-{{.instance}}'
          valueFiles:
            - ../overlays/{{.dc}}/{{.instance}}/values.yaml
          parameters:
            - name: dc
              value: '{{.dc}}'
            - name: configChangeset
              value: '{{.configChangeset}}'
            - name: instance
              value: '{{.instance}}'
            - name: namespace
              value: '{{.namespace}}'
            - name: expressVersion
              value: '{{.expressVersion}}'
            - name: nodeCount
              value: '{{.nodeCount}}'
            - name: enableSynchronizer
              value: '{{.enableSynchronizer}}'
            - name: enableFarmers
              value: '{{.enableFarmers}}'
        path: apps/dcr/{{.chartVersion}}/
        repoURL: git@github.medallia.com:Atlas/deployment.git
        targetRevision: preprod
      syncPolicy:
        automated:
          selfHeal: true
          prune: false
          allowEmpty: true
```


```yaml
    spec:
      project: dcr
      destination:
        namespace: '{{.namespace}}'
        server: https://kubernetes.default.svc
      source:
        helm:
          releaseName: 'dcr-{{.instance}}'
          parameters:
            - name: instance
              value: '{{.instance}}'
            # ... other parameters ...
        chart: <CHART_NAME>
        repoURL: <HELM_CHART_REPO_URL>
        targetRevision: <HELM_CHART_VERSION>
```


```
def perform(self):
        self.switch_argo_app_to_deployment()
        self.update_deployment_repo()

    def update_deployment_repo(self):
        with self.k8s_client() as k8s_client:
            self.modify_argo_app_immediate(
                modify_function=self._start_history_update,
                extra_message=self.extra_message,
                k8s=k8s_client,
                force=False,
                wait_s=60 * 30,  # Adjust as necessary
                must_healthy=False,
                must_synced=False,
                poll_interval_s=60,
            )
```
