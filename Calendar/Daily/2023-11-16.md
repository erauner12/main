---
id: 9
up: "[[2023-W46]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20231116000100
modified: 20231117000100
aliases:
  - Thursday - November 16th 2023
linter-yaml-title-alias: Thursday - November 16th 2023
title: Thursday - November 16th 2023
week: "[[2023-W46]]"
yearly: "[[2023]]"
quarterly: "[[2023-Q4]]"
monthly: "[[2023-11]]"
daily: "[[2023-11-16]]"
month: "November"
weekday: Thursday
---

# Thursday - November 16th 2023

## Tasks

%% TCT_TEMPLATED_START 2023-11-16 00:00 %%

%% TCT_TEMPLATED_END 2023-11-16 23:59 %%
- ? Did these tasks align to your Goals?

# Rollover

# Daily Notes


So some kind of mass stop mass retry Operations plus individual Counterparts. Yeah, I didn't even think about that. Yeah Like without even that wouldn't be possible here right now. What it No, yeah Yeah, I feel like what what state I think an equivalent of like a Jira that we just Basically, like no, we don't need we're not actually gonna finish this one out like I'm not required I feel like a state of in that task would be useful. I say if you if you created another Say there was a button to retry to stop and if there was a resubmit then that retry would be To stop and if there was a resubmit then that resubmit not only should create another one with the same parameters but it should also Put that one into a state of hey like this isn't we're not gonna do anything with this one anymore So that if you were to filter all of those for that deployment Then it wouldn't show up there only the new one that it created would show up there or something yes, but Yeah, even this might be problematic in a sense that You won't retain the history. So anyone can come in and you know, stop and recreate a bunch of tasks and you simply won't Know You know what? You won't be having the history of that deployment, right? Because sometimes may You know may not appear in the list while some new tasks will be created So yeah I'm again, I'm I probably do this in phases like there'll be a first phase. We're gonna be able to submit the tasks in the Second phase we have the support in the UI for basic operations and then we'll see what's Well, yeah Yeah, we wouldn't want anybody to be able to like I don't know Take something that's old and try and just retry it or start it Like we'll do anything with it unless it's like a certain like amount of time right or something Yeah anyway


---

but it does say that. I need to get more clear about what exactly the donor node is used for. I know what it, I've seen it be used, I just don't know exactly. In this context, I don't, I need to get more, I need to understand it better before getting an answer. So let me find out, then I'll get back to you on that one. But I know from certain, from DCR for example, you have to disable all nodes at the beginning from being the donor node. If one is designated as the donor node, you have to disable all of them from being it if they are. And I know that if an FE node is down and it can't get back up, you need to designate a donor node that is working. And I think that it's somehow when that other, when the node comes up, it uses its caches or something like that. So I need to get clear about exactly what that is. And I'll get back to you on it. Okay then, anyway, I will troubleshoot it a bit more because now I remember that in the runs,


---

Let's see now, where are we? Okay, it's too far, right? Let's go to R. So, R. So it's like support growth, new provisioning request, yeah. This is R's. And we have to doâ€¦ Make provisioning request modifications. This is Jesper. Introduce the workflow, ArchiveDB. That's Jesper. I don't know why our names are here. Yeah, I guess because we're providing the feedback. Okay. And this is like the signation, okay. Yeah. This one, I guess we've been not like we're doing directly, but we've been doing passively. Yeah, yeah, it's okay. And there's not much to be done because this is like in very early stages. We just need to talk to the guys like some more times. Create design proposal, that's pretty much done, right? No, this is coming. This is me. But I was thinking maybe closer to Christmas, like we can start doing unit tests. Yeah. That's interesting, though, the GNU PG issue with the keys. And I was thinking about this. Why don't we just put it in pure storage as a secret? That's what I'm saying. I agree. It's an easy issue. There's so many things in our repo that we are the glue that's sticking all these things together. We need to take ourselves out of that position. Yeah, yeah, yeah. Okay. Yeah, but I think that's one option. Anyway, maybe this is good for a good candidate for next time. Use dedicated DCR synchronization. Yeah. So DCR has been stuck for a while. But as of the past week, we've made some progress. I think it's going to. Okay. Yeah, we need to. The things I want to focus on are specifically SRE router, but also improving upon DCR. The frame is already there. We just need to add some things to it to make it better. Mm-hmm. Okay. Because Luis told me that this order is actually the priority as well. Okay. So we have the design document. We can start at least maybe next week, get together and start putting. Maybe we can do it in the technical discussion. Start creating issues, what we need to do. Okay. That's actually a good point. Let's put it there. Next one is 21, right? 21.

---


MEC Deployment
- Iterating on this now

SRE Router
- Need to finish design document

Provisioning-ng (Unit tests)
- Dcr Unit Test (in december)

DCR
- Create issues


---


problem to go and like download the file, VARs file that was like related to exactly the issue that I that I have. I don't have a problem with that. What I was thinking the other day is like what I was kind of said that you can you can actually run remotely the deployer to the in the in on Kubernetes, not the deployer, sorry, you can run your debugger remotely on Kubernetes. But this kind of okay, so see now it changed this, it changed that. We're good. Nice. If I go to the resource, which, yeah, this back end is changing. Refresh it. It should be also there. It's still I didn't pick up the change yet because it's like it's like running right now. Okay. Yeah, like I agree. Like we don't always need it. But like, just making it incredibly easy to get those things and drop it right where you need to be, I think would be would be useful. I just want I want not have to ever have to log into the pod and get anything. I just want to be able to get it either from the UI or somewhere. I just I want it. I want it. I want to. But I agree with what you're saying. I just don't understand how to do that, like remotely debug it, like the pod itself. Don't worry. Yeah, that's I'm working on it. Like I hope I will make because the problem is that you need what you need. If you want to do that thing and you want to use that functionality from VS Code, there's like you need the service and a pod and a deployment and a service. Because what happens is like you create a tunnel between your your client and the service and you create a port forwarding tunnel or whatever to Kubernetes. And then what happens is you go to the target port and replace that pod with whatever you have in your workspace. And then when you're running debugging session, it's actually running on the pod. But the problem with the workflows is that it's a custom resource. It's a workflow and it cannot work. But I was thinking that we can introduce just a dummy service with a dummy deployment that will actually we will apply before we want to use this tool, this functionality, to Kubernetes. After all, it's a bit risky like from a security perspective, but it's like for dev and like for development and like it is a development environment like in Denver. So and the workspace is ours, so we don't expose things from other workplaces. So anyway, we could utilize this because I still didn't make it work. But yeah, I don't know. I think that that will be helpful. Because imagine you will have the whole mounted directory, this work there, you will have it available as you have it in Jesp. Because now what I do is like I go to Jesp and I copy over the file that I need. But imagine like you have it and your code is running and has all this stuff available and secrets and everything. Yeah, so you don't need to copy over secrets. That would be nice. That would be really nice because then we're less relied on our local. We're only needing our local for the development of a task and then once it's we're testing the task itself. But that's what I'm saying like you can even do local development and run it in Kubernetes this way. See, that would be cool. The first from the the first moment, let's say. So whatever you run, when you run, when you click on debugging session, it actually runs on the cluster and not on locally. So you don't need to do tunnels. You don't need to do anything. That would be cool. Yeah. Let's see. But anyway, this takes some time.


---



- My vision of this is only use the solution in guest for the one that doesn't have a JIRA because we are going to have an inconsistency between the JIRA and the task, if the task is going to be retrieved only from the API, right? So we need to start having this discussion around that. And it's going to take a while to have something that could work, right? At least that is what Kamil told me. But in the meantime, I would like to create a presentation for SIP where we have a definition of the scope and actual state of how the deployment is working right now and all the issues that we are having right now, as you mentioned. And then I would like to have like a final proposal, like, okay, we think that this should work like this. And then we can discuss about that. Because if we are going to have only another thing we see, it's going to be okay. I think, I thinkâ€¦ Yeah. Fuck that, man. It's not evenâ€¦ Yeah. That's such aâ€¦ Yeah. Like, we think it should be like this. Well, I thinkâ€¦ No, no, no. Like, let's just talk about the problem. Here's the problem. I don't think anything. All I think is that this is a problem. Like, no one's fault at all. Like, let's just talk about the problem. And I think a phased approach is what's necessary. And a part of that phased approach is leveraging ProvenG to get off the prod deployer. But the ultimate route to, like, is not to replace prod deployer with ProvenG. I think the ultimate destination of this is to get our deployments to a place where we don't need a ProvenG. But I'm not saying that we, like, ProvenG still will be useful for this. But I think we can do all of this shit with Helm charts. We're just overcomplicating it. But in order to get ourselves out of the rut that we're currently in, we need to take away from the prod deployer and put it in our position. So then we can control the phase. Okay. So the first phase is what we're essentially doing what the prod deployer is doing. But the second phase is instead we change the method to use a Helm chart, because that's what a Helm chart was designed to do. But I don't know. I don't want to get ahead of myself. I just want to I just I want I think I think we're doing like Barsamus has been looking at the mech deploy cluster tasks specifically and understanding it. And yeah, I think I think we're on our way towards, like, actually replacing the deployment like properly. Yeah. It's going to be a phased approach, though. Like, yeah. Yeah. Yeah. Yeah. And it's going to take time. Right. Because when I say that we need to have a scope, that means that maybe all the clusters that are running today in Atlas are not going to be inside that scope, right? Because we are only going to focus on the clients that run Kubernetes and the new ones. Right. So, yeah, it's very important to define that, because that means that we are going to have a period of time where we are going to still need to have two solutions. Right. Yeah. That is part of the process, because if not, you can't cover it. Right. And Proven-G can use multiple solutions. It's like that is the glue, whereas one is not a glue. It's its own thing. And this one is a glue that can use both at the same time. And I'm just like, anyway.


---



whenever the service is down, before the client notices it, or before the client even reports it. So, this is really aligned with what he said, right? So, yeah, that is why I would like to create a presentation, at least with the things that we know, right? Yeah, I think we're almost at a point where we can do that. The design document needs to be cleaned up a little bit more, but yeah. I'm going to work on finishing the design document with the feedback of Nikhil and the rest of the India team, and get started on the testing, like, probably next week. And also, we talked to Varsaman about this earlier today, but we're going to start actually off of the design document, creating all the gear that's necessary for the Epic. So, I'm working on it. And the other thing is provisioning NG. I've been picking up a few tasks, but I think Varsaman is mainly doing those, and then asking me when something doesn't make sense. And for better or for worse, like, even though it hasn't been documented, like, I have somewhat of an idea of what, like, for Metaplanet, why it's like that, you know, without the documentation. So, I've been weighing in there. And the last priority that I have that has actually, like, resulted in, like, we've had some, like, we had some movement on it lately is DCR. Yeah, I saw that in the comments. Just yesterday, we did it. We finally have a completely, no prod deployer, no, like, everything is controlled by us. We have a complete solution, like, in our control that I can put in the deployment repo and nowhere else. Nice. It's like, that's a game changer, like, to me. Like, we won't have to rely on an FE node or any of that shit. Like, we'll have our own, like, when we want a cache rebuild, we'll spin up our own set of services and then break them down, like, with a completely independent solution. Like, so, I'm going to create some gears for that so that next week when we go through the grooming process again, that we can prioritize those accordingly. Okay. Did you update the documentation related to DCR pump? No, not yet. Because right now, we don't even have it. Like, all I have from Sergio is a test image from QA. He just provided me the image. It doesn't even have an express E, nothing. It's just a, yeah. Okay.

---


SSO integration with MT, because they are like running a work-in-progress to integrate CheckMarket with an MT instance, and they want to present that for the next Medallia experience summit. So there are a lot of people working from the CheckMarket team to configure the SSO with the MT instance, but there is still a lot of work from the Express team to change the SSO configuration and the admin suite to have a switch button there for CheckMarket but besides that, they are trying to use something similar to what we have for LivingLens. So I was thinking that maybe we could help them with that. So I would like to understand how we are doing that, okay? I know that for sure that we configured the Medallia LivingLens and Medallia LivingLens SSO client user in the OAuth or Express, and then we configured the Medallia LivingLens user in LivingLens. So I know how to do it manually, okay, that is not a problem, but I would like to understand how we are doing that from the production engine, okay? Because at some point, I believe that we are going to, we will need to work on that, and I would like to be sure that we have everything from their side, okay? That the API that we need is in place, that we can, that we could configure the SSO, the OAuth in the Express instance with the global script as we are doing right now, but I would like to understand better what is going on there. So if you could help me with that, that would be awesome, okay? Because really, I can't read the code, but I'm not completely sure that I understand everything, because I can see that we have, for the digital, we have some templates, but then for LivingLens, we don't, so I want to be sure that whatever I'm doing is right, okay? Okay, yeah, sure, I'll, I don't know much about it right now, but whatever I can do to help you understand that. Oh yeah, I know, I know.

---
