---
up: "[[2024-W31]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20240801111310
modified: 20240802000100
aliases:
  - Thursday - August 1st 2024
linter-yaml-title-alias: Thursday - August 1st 2024
title: Thursday - August 1st 2024
id: 10
week: "[[2024-W31]]"
yearly: "[[2024]]"
quarterly: "[[2024-Q3]]"
monthly: "[[2024-08]]"
daily: "[[2024-08-01]]"
month: "August"
weekday: Thursday
---

# Thursday - August 1st 2024

```
instance-configure
- failed to commit/push prod-deployer made changes to viatris.xml via automation with: Command git push -q on fra1-prod-dep01 failed
instance-redeploy
- manual intervention needed on fra1-prod-dep01 in ~/deployer/clusterconfig-fra1 failed to sync/pull manifest with: Command git pull --no-edit on fra1-prod-dep01 failed
```

# Optimize API Server Workflow, Improve Testing Environment, Manage Kubernetes Clusters, Automate Deployment, Prioritize Integration Tests, Enhance Sandbox Stability, and Integrate Flex API

> Date & Time: 2024-08-01 09:09:05  
> Location: [Insert Location]  
> Attendees: [Speaker 1] [Speaker 2] [Speaker 3] [Speaker 4] [Speaker 5]

## 1. API Server Workflow Optimization

### Conclusion

A simpler client for the API server workflow is beneficial for specific use cases.

### Discussion Points

1. [Speaker 1]:The API server can consume information and generate workflows in a single step, but it has dependencies.
2. [Speaker 2]:A new client was created to simplify passing parameters directly to the API without type checking.
3. [Speaker 1]:The new client could potentially replace the existing one in the API spec repo.
4. [Speaker 2]:The Python client still has its merits, but for this use case, a simpler client is more suitable.

## 2. Testing Environment Improvement

### Conclusion

Improving the testing environment with a new stage and standardized processes is essential.

### Discussion Points

1. [Speaker 1]:A new environment for testing smaller chunks of code before pushing to production is needed.
2. [Speaker 2]:Testing locally is important, but running tests remotely provides more confidence.
3. [Speaker 2]:There should be a strict, standardized way to run scripts for new developers.
4. [Speaker 2]:Reproducing issues locally and debugging them quickly is crucial.
5. [Speaker 1]:Running a single step might be difficult due to dependencies on previous steps.

## 3. Kubernetes Cluster Management

### Conclusion

Managing Kubernetes clusters efficiently involves reducing pod sizes and potentially using dedicated clusters.

### Next Steps

- [ ] Create 30 test clusters across multiple data centers and simulate deployments. -- _[Speaker 2]_ _[Speaker 1]_
- [ ] Retrofit the Jira base trigger around the new setup. -- _[Speaker 2]_

### Discussion Points

1. [Speaker 1]:Submitting more workflows at once is necessary to identify major issues.
2. [Speaker 1]:Reducing the size of pod resources can help in submitting more workflows.
3. [Speaker 2]:Having a dedicated Kubernetes cluster for testing would reduce risks.
4. [Speaker 1]:We should start by reducing pod sizes and then consider a dedicated cluster.

## 4. Deployment Automation

### Conclusion

The deployment automation project should support both individual and mass deployment tickets, and a script should be created to automate this process.

### Next Steps

- [ ] Create a script to automate the creation of deployment tickets. -- _[Speaker 2]_

### Discussion Points

1. [Speaker 1]: The project SRE MEC requests allows for the creation of individual or mass deployment tickets.
2. [Speaker 2]:A script should be created to automate the creation of these tickets.
3. [Speaker 1]:The system should allow users to choose whether to create individual SRE config tickets for mass deployment.
4. [Speaker 3]:The deployment process should be able to handle multiple clusters simultaneously.

## 5. Integration and Unit Testing

### Conclusion

Integration tests should be prioritized, followed by unit tests for individual steps.

### Next Steps

- [ ] Set up integration tests first, followed by unit tests for each step. -- _[Speaker 2]_

### Discussion Points

1. [Speaker 2]:Integration tests should be prioritized before unit tests.
2. [Speaker 2]:Unit tests should be set up for each individual step.

## 6. Optimizing Deployment Process

### Conclusion

The deployment process should be optimized to allow for tracking individual deployments, handling failures, and providing a unified entry point.

### Next Steps

- [ ] Implement dynamic buckets and a unified entry point for deployments. -- _[Speaker 1]_ _[Speaker 2]_
- [ ] Test the deployment process with real MacDeploy cluster tasks. -- _[Speaker 1]_ _[Speaker 2]_

### Discussion Points

1. [Speaker 1]:The deployment process should allow for tracking individual deployments and their statuses.
2. [Speaker 1]:A unified entry point for deployments should be provided, regardless of the underlying technology.
3. [Speaker 1]:Dynamic buckets could be used to manage deployments across different data centers.
4. [Speaker 1]:The deployment process should be able to handle failures and retries for individual clusters.

## 7. Implementation and Improvement of Sandbox Environments

### Conclusion

The sandbox environment will be improved by hooking into the current Thrift APIs, making it more stable and similar to the DCR environment.

### Discussion Points

1. [Speaker 2]:The sandbox environment should be improved to be more stable and similar to the DCR environment.
2. [Speaker 2]:The current method using CLI for sandbox operations is not efficient.
3. [Speaker 2]:Larry is willing to help with the sandbox improvements.
4. [Speaker 2]:The first step should be to hook into the current Thrift APIs for sandbox service and orchestrator.
5. [Speaker 1]:The API for the user should remain the same even if the underlying code changes.
6. [Speaker 2]:Improving sandboxes will make them more stable and reduce the flood of alerts.

## 8. Prioritization of Tasks

### Conclusion

Tasks will be prioritized with a focus on Mac deployment and completing the SRE Router.

### Next Steps

- [ ] Plan and prioritize tasks with a focus on Mac deployment and completing the SRE Router.

### Discussion Points

1. [Speaker 1]:There is a need to prioritize tasks and do some planning to avoid jumping from one thing to another.
2. [Speaker 1]:The Mac deployment should be prioritized as it has been pending for a while.
3. [Speaker 2]:The SRE Router should be brought to the finish line as it showcases the framework's capabilities.

## 9. Integration of Flex API

### Conclusion

Flex API will be integrated to provide dynamic parameters and cluster information, with a focus on updating the schema regularly.

### Next Steps

- [ ] Implement a new task to the Slack integration piece using Flex API.

### Discussion Points

1. [Speaker 1]:Flex API can be used to provide a list of clusters and their information.
2. [Speaker 2]:The Flex API supports dynamic parameters and can be integrated with a Python script.
3. [Speaker 1]:The schema for Flex API needs to be updated often to stay current.
4. [Speaker 1]:Implementing a new task to the Slack integration piece would be a good practice.
5. [Speaker 1]:Working with Jira is simplified by mapping custom fields to their names instead of IDs.

---

```
Can you help me build out a set of unit test specifically for the new module I have introduced to the provisioning-ng codebase?

…

	├── run_workflow.py # entrypoint

	│

	├── config/

	│   └── workflow_config.py

	├── core/

	│   ├── init.py

	│   ├── workflow_runner.py

	│   ├── snippet_manager.py

	│   ├── step_executor.py

	│   ├── workflow_state.py

	│   ├── logging_manager.py

	│   └── task_parameter_manager.py

…

I have staged the tests:

```

    └── tests/

        ├── init.py

        ├── test_workflow_runner.py

        ├── test_snippet_manager.py

        ├── test_step_executor.py

        ├── test_workflow_state.py

        ├── test_logging_manager.py

        └── test_task_parameter_manager.py

```
I am building a module in my codebase (run_workflow.py) to provide more information on local integration testing. The existing code was `step.py`, and I am developing a layer on top of it to streamline the process of building, defining, and testing the code.

a point of friction in the current project/workflow is that we have to locally test one step at a time which I am trying to make easier by introducing task definitions,scenarios,  snippets (for local) and an easier means of taking the same exact parameters and plugging them to a remote server to be executed (SRE API client)

A new environment is needed to test smaller chunks of code before pushing to production.

While testing locally is important, running tests remotely provides more confidence, which is the purpose of the SRE API client.

There should be a strict, standardized way for new developers to run scripts.

Reproducing issues locally and debugging them quickly is crucial.

It's important to consider that running a single step might be difficult due to dependencies on previous steps. This will need to be kept in mind.

…

We should build out a set of pytest to make this new addition to the repo robust, and we should also update informative technical documention and usage as we go

…

there will be more additions to the code so at this point the tests should be simple and flexible to changes.

the new tests will serve as a means of quickly testing any additional changes we might introduce in the near future to this current code. I would like to prioritize extensibility.
```

```
Can you help me build out a set of unit test specifically for the new module, the workflow runner I have introduced to the provisioning-ng codebase?

…

	├── run_workflow.py # entrypoint

	│

	├── config/

	│   └── workflow_config.py

	├── core/

	│   ├── init.py

	│   ├── workflow_runner.py

	│   ├── snippet_manager.py

	│   ├── step_executor.py

	│   ├── workflow_state.py

	│   ├── logging_manager.py

	│   └── task_parameter_manager.py

…

I have staged the tests:

…

I would like the initial tests that we create to be comprehensive based on the inputs we have but also flexible enough to where we don't have to make so many changes everytime we change something

I am still early on in this module and would like to subscribe to a test driven development approach to ensure that I can iterate properly without breaking things.

but I do not want the tests to be so restrictive that it is difficult to make changes to the code so do with that what you will

…

I would like to use pytest for this, please reference pyproject.toml. we have not setup any unit testing in this repo yet, so now will be a good time setup somewhat of a skeleton for this.

…

I would like you to also keep in mind that I want to create specific usage documentation that we should update as we go.

specific usage/information (explanatory) pages should be in the docs folder:

tree doc/

doc/

└── developer_guide.md

…

while reference to these should be appended to the README.md in the root of the project.

…

Let's make sure the the "scenario"  inputs stay flexible as we will likely extend this piece greatly

…

I am going to propose this in a PR on monday and would like to help the reviewer understand the what/why/how to make the PR easier to consume. So let's keep that in mind.
```
