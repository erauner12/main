---
up: "[[2024-W31]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20240803000100
modified: 20240804000100
aliases:
  - Saturday - August 3rd 2024
linter-yaml-title-alias: Saturday - August 3rd 2024
title: Saturday - August 3rd 2024
id: 10
week: "[[2024-W31]]"
yearly: "[[2024]]"
quarterly: "[[2024-Q3]]"
monthly: "[[2024-08]]"
daily: "[[2024-08-03]]"
month: "August"
weekday: Saturday
---

# Saturday - August 3rd 2024

Alert that started it:

```
no_metrics_for_express_nodes (dc=sc4)
```

Parameters:

```
{
  "context": "5wT2W4vHXN",
  "datacenter": "sc4",
  "force": true,
  "instance": "wyndhamhotels",
  "killPodIfNotExiting": true,
  "node": "wyndhamhotels-fe2",
  "slack": {
    "channelId": "CEKF29CQG",
    "threadTs": "1722667893.510119"
  },
  "taskType": "MecBounceNode",
  "waitForReadySeconds": 18000,
  "waitForTerminationSeconds": 3600
}
```

```
	        wyndhamhotels-fe1                  : SYNCHRONIZED    visible    sync     e689.155 ready           5f5e3be35f5e3be3
	        wyndhamhotels-fe2                  : BROKEN          invisible  not sync  unreachable     org.apache.thrift.transport.TTransportException
	        wyndhamhotels-fe3                  : SYNCHRONIZED    visible    sync     e689.155 ready           5f5e3be35f5e3be3
	        wyndhamhotels-fe4                  : SYNCHRONIZED    visible    sync     e689.155 ready           5f5e3be35f5e3be3
```

```
2024-08-03 09:51:22,171 DEBUG tunnel running command kubectl port-forward --insecure-skip-tls-verify -n tenant-101399-prod --address=127.0.0.1 pod/wyndhamhotels-fe2 --kubeconfig=/tmp/tmp2b4_mk2a 47439:9100
2024-08-03 09:51:22,180 DEBUG tunnel waiting for local port 47439 to be live
2024-08-03 09:51:23,192 INFO tunnel stopping kubectl tunnel to pod/wyndhamhotels-fe2 in tenant-101399-prod
2024-08-03 09:51:23,193 DEBUG tunnel stopped kubectl proxy on port 47439
2024-08-03 09:51:23,194 DEBUG misc got exception failed to run admin command 'status' on http://127.0.0.1:47439/.admin with: HTTP Error code: 503, body:



HTTP ERROR: 503
Problem accessing /.admin. Reason:
    Service Unavailable
```

[SRE UI | Log bounce-node for task MecBounceNode - wyndhamhotels - Task automation-h4kk5wsud6mclhe](https://sre-provisioning-api.eng.medallia.com/ui/log/automation-h4kk5wsud6mclhe/bounce-node)

Then kicked this one off

[SRE UI | MecBounceNode - wyndhamhotels - Task automation-u2l0rkfkltmdggd](https://sre-provisioning-api.eng.medallia.com/ui/task/automation-u2l0rkfkltmdggd)

```
wyndhamhotels-fe2                                        2/2     Running     0          35s    10.182.21.3      sc4-r10-u12   <none>           <none>
```

Attempted to bring the pod back up, and this time it went into booting

```
wyndhamhotels-fe2 2/2 Running 0 35s 10.182.21.3 sc4-r10-u12 <none> <none>
```

```
wyndhamhotels-fe2 : BOOTING invisible not sync e689.155 waiting Inactive
```

During a recent incident with the `wyndhamhotels-fe2` node, L1 observed that the `MecBounceNode` task failed to properly handle a situation where the node was stuck in a BROKEN state and returning 503 errors after coming back up from the bounce.

The alert being responded to was: `no_metrics_for_express_nodes`

Current behavior:

1. The MecBounceNode task attempts to connect to the node via kubectl port-forward.
2. Connection attempts frequently time out or return 503 errors.
3. The task retries multiple times but ultimately fails without successfully bouncing the node.

Expected behavior:

1. The MecBounceNode task should implement a more robust retry mechanism with exponential backoff.
2. It should distinguish between different types of errors (connection timeouts vs. 503 errors) and handle them appropriately.
3. The task should attempt more aggressive recovery actions if initial retries fail.

Steps to reproduce:

1. Run MecBounceNode task on a node that is experiencing network issues or returning 503 errors.
2. Observe that the task repeatedly fails to connect or receives 503 errors, eventually timing out.

Logs:

```
2024-08-03 06:53:05,819 DEBUG tunnel running command kubectl port-forward --insecure-skip-tls-verify -n tenant-101399-prod --address=127.0.0.1 pod/wyndhamhotels-fe2 --kubeconfig=/tmp/tmpjmz8m5in 35815:9100
2024-08-03 06:53:05,828 DEBUG tunnel waiting for local port 35815 to be live
2024-08-03 06:53:36,868 INFO tunnel stopping kubectl tunnel to pod/wyndhamhotels-fe2 in tenant-101399-prod
2024-08-03 06:53:36,871 DEBUG tunnel stopped kubectl proxy on port 35815
2024-08-03 06:53:36,871 WARNING step_bounce_node Unexpected error while attempting to disable shutdown protection: failed to run admin command '{'cmd': 'allowOrDisallowShutdownProtection', 'protectionAllowed': 'false', 'format': 'json'}' on http://127.0.0.1:35815/.admin with: HTTPConnectionPool(host='127.0.0.1', port=35815): Read timed out. (read timeout=30). Proceeding with bounce anyway.

... (additional logs omitted for brevity)

2024-08-03 08:17:29,702 DEBUG misc got exception failed to run admin command 'status' on http://127.0.0.1:40871/.admin with: HTTP Error code: 503, body:

HTTP ERROR: 503
Problem accessing /.admin. Reason:
    Service Unavailable
```

Proposed solution:

1. Implement a more sophisticated retry mechanism with exponential backoff in the `bounce_k8s` method.
2. Add logic to distinguish between connection timeouts and 503 errors, adjusting the retry strategy accordingly.
3. Implement more aggressive recovery actions (e.g., force-deleting the pod) if initial retries fail. (if it does not seem that it is going to get better)
4. Improve logging to provide more detailed information about retry attempts and error types.

Potential Code changes:

[provisioning-ng/prov_platform/mec/MecBounceNode/step_bounce_node.py at delivery Â· medallia/provisioning-ng](https://github.medallia.com/medallia/provisioning-ng/blob/delivery/prov_platform/mec/MecBounceNode/step_bounce_node.py)

In `prov_platform/mec/MecBounceNode/step_bounce_node.py`, modify the `bounce_k8s` method:

```python
import time
from requests.exceptions import RequestException, HTTPError, Timeout

MAX_RETRIES = 5
INITIAL_RETRY_DELAY = 5  # seconds
MAX_RETRY_DELAY = 120  # seconds

def exponential_backoff(attempt):
    return min(INITIAL_RETRY_DELAY * (2 ** attempt), MAX_RETRY_DELAY)

def bounce_k8s(self):
    for attempt in range(MAX_RETRIES):
        try:
            self._disable_shutdown_protection()
            self._enter_maintenance_mode()
            self._patch_cr(up=False)
            self._wait_for_pod_termination()
            self._patch_cr(up=True)
            self.wait_node_ready()
            return  # Success, exit the method
        except (RequestException, HTTPError, Timeout) as ex:
            retry_delay = exponential_backoff(attempt)
            log.warning(f"Attempt {attempt + 1} failed: {ex}")
            log.info(f"Retrying in {retry_delay} seconds...")
            time.sleep(retry_delay)

    # If we've exhausted all retries, try more aggressive recovery
    log.warning("All retry attempts failed. Attempting aggressive recovery.")
    self._force_delete_pod()
    self._patch_cr(up=True)
    self.wait_node_ready(timeout=300)  # Wait longer for recovery

def _force_delete_pod(self):
    log.info(f"Force deleting pod {self.params.node}")
    self.k8s.core_v1_api.delete_namespaced_pod(
        self.params.node,
        self.vars.topology.namespace,
        grace_period_seconds=0,
        force=True
    )

def _wait_for_pod_termination(self):
    log.info("Waiting for pod to terminate")
    max_wait = self.params.wait_terminate_s
    start_time = time.time()
    while time.time() - start_time < max_wait:
        if not self._get_pod_if_exists():
            return
        time.sleep(5)
    raise StepBounceNodeException("Pod failed to terminate within the specified time")
```

This updated implementation should:

1. Uses exponential backoff for retries.
2. Breaks down the bounce process in smaller steps
3. Implements a more aggressive recovery method (force-deleting the pod) if all retries fail.
4. Improves logging to provide more context about what's happening during each step.

Additional tasks:

- Add integration tests to simulate various failure scenarios (timeouts, 503 errors).

1. Update docstrings to reflect the new behavior and configuration options.
2. Consider adding these new configuration options (MAX_RETRIES, INITIAL_RETRY_DELAY, MAX_RETRY_DELAY) to the task parameters so they can be adjusted per-run.
3. Implement better error reporting to Slack or other monitoring systems to provide real-time updates on retry attempts and recovery actions.

---



Old

```yaml
!!python/object:prov_platform.mec.MecBounceNode.bounce_node_abstract.BounceNodeVars
_completed: true
_initialized: true
_params: !!python/object:prov_platform.mec.MecBounceNode.bounce_node_abstract.BounceNodeParams
  allow_params_override: false
  context: 0rPgoNVDaF
  dc: den
  disable_safety_checks: false
  force: false
  instance: demodcrtest4
  jira_assignee: null
  jira_issue: null
  jira_related_issues: []
  kill_pod: false
  mark_completed_on_success: true
  mec_max_instance_age_in_days: 31
  mec_url: null
  node_name: demodcrtest4-fe1
  shared_context: null
  step_modifiers: {}
  task_type: MecBounceNode
  wait_ready_s: 900
  wait_terminate_s: 900
_task_unique_id: P6NQYBTF
account: null
clusterconfig_commit: !!python/object:lib.utils.Commit
  sha: null
  time_stamp: null
configuration_commit: !!python/object:lib.utils.Commit
  sha: null
  time_stamp: null
country_id: null
dc: null
deployment_commit: !!python/object:lib.utils.Commit
  sha: null
  time_stamp: null
deployment_security_commit: !!python/object:lib.utils.Commit
  sha: null
  time_stamp: null
display_name: null
first_user_email: null
first_user_first_name: null
first_user_last_name: null
instance_id: 124634
instance_type: !!python/object/apply:prov_platform.mec.mec_abstract.MecInstanceType
- PROD
ip_subnets: []
mec_display_name: null
mec_instance_age: null
mec_instance_dc: null
mec_instance_id: null
mec_tenant_id: null
mec_url: null
puppet_commit: !!python/object:lib.utils.Commit
  sha: null
  time_stamp: null
region: null
revreq_issue: null
sso_configured: false
tenant_id: null
topology: &id001 !!python/object:lib.express_topology.ExpressCluster
  backend: !!python/object:lib.express_topology.ExpressNode
    cluster_name: demodcrtest4
    cluster_type: &id002 !!python/object/apply:lib.express_topology.ExpressClusterType
    - kubernetes
    config_changeset: 4cdcbd5ad22933775a385747c13f21a63e5f1656
    dc: den
    dc_url: !!python/object/new:urllib.parse.ParseResult
    - https
    - demodcrtest4-be.den.medallia.com
    - ''
    - ''
    - ''
    - ''
    deployment_revision: null
    heap_memory_gib: 40
    host: null
    host_type: !!python/object/apply:lib.express_topology.ExpressHostType
    - be
    in_config_only: false
    ip: 10.197.10.112
    job_cpu: 1.0
    job_memory_mib: 63488
    metadata_source: &id003 !!python/object/apply:lib.express_topology.MetadataSource
    - kubernetes_custom_resource
    name: demodcrtest4-be
    namespace: tenant-124634-prod
    provisioning: false
    reachable_url: !!python/object/new:urllib.parse.ParseResult
    - https
    - demodcrtest4-be.medallia.com
    - ''
    - ''
    - ''
    - ''
    redeploy: &id004 !!python/object/apply:lib.express_topology.RedeployState
    - SKIP
    suffix: null
    up: true
    url: !!python/object/new:urllib.parse.ParseResult
    - https
    - demodcrtest4-be.medallia.com
    - ''
    - ''
    - ''
    - ''
    version: express-e689.155
  branch: master
  cluster: *id001
  cluster_type: *id002
  database: null
  dc: den
  dc_url: !!python/object/new:urllib.parse.ParseResult
  - https
  - demodcrtest4.den.medallia.com
  - ''
  - ''
  - ''
  - ''
  frontends:
  - !!python/object:lib.express_topology.ExpressNode
    cluster_name: demodcrtest4
    cluster_type: *id002
    config_changeset: 4cdcbd5ad22933775a385747c13f21a63e5f1656
    dc: den
    dc_url: !!python/object/new:urllib.parse.ParseResult
    - https
    - demodcrtest4-fe1.den.medallia.com
    - ''
    - ''
    - ''
    - ''
    deployment_revision: null
    heap_memory_gib: 40
    host: null
    host_type: &id005 !!python/object/apply:lib.express_topology.ExpressHostType
    - fe
    in_config_only: false
    ip: 10.197.10.113
    job_cpu: 1.0
    job_memory_mib: 63488
    metadata_source: *id003
    name: demodcrtest4-fe1
    namespace: tenant-124634-prod
    provisioning: false
    reachable_url: !!python/object/new:urllib.parse.ParseResult
    - https
    - demodcrtest4-fe1.medallia.com
    - ''
    - ''
    - ''
    - ''
    redeploy: *id004
    suffix: null
    up: true
    url: !!python/object/new:urllib.parse.ParseResult
    - https
    - demodcrtest4-fe1.medallia.com
    - ''
    - ''
    - ''
    - ''
    version: express-e689.155
  - !!python/object:lib.express_topology.ExpressNode
    cluster_name: demodcrtest4
    cluster_type: *id002
    config_changeset: 4cdcbd5ad22933775a385747c13f21a63e5f1656
    dc: den
    dc_url: !!python/object/new:urllib.parse.ParseResult
    - https
    - demodcrtest4-fe2.den.medallia.com
    - ''
    - ''
    - ''
    - ''
    deployment_revision: null
    heap_memory_gib: 40
    host: null
    host_type: *id005
    in_config_only: false
    ip: 10.197.10.114
    job_cpu: 1.0
    job_memory_mib: 63488
    metadata_source: *id003
    name: demodcrtest4-fe2
    namespace: tenant-124634-prod
    provisioning: false
    reachable_url: !!python/object/new:urllib.parse.ParseResult
    - https
    - demodcrtest4-fe2.medallia.com
    - ''
    - ''
    - ''
    - ''
    redeploy: *id004
    suffix: null
    up: true
    url: !!python/object/new:urllib.parse.ParseResult
    - https
    - demodcrtest4-fe2.medallia.com
    - ''
    - ''
    - ''
    - ''
    version: express-e689.155
  - !!python/object:lib.express_topology.ExpressNode
    cluster_name: demodcrtest4
    cluster_type: *id002
    config_changeset: 4cdcbd5ad22933775a385747c13f21a63e5f1656
    dc: den
    dc_url: !!python/object/new:urllib.parse.ParseResult
    - https
    - demodcrtest4-fe3.den.medallia.com
    - ''
    - ''
    - ''
    - ''
    deployment_revision: null
    heap_memory_gib: 40
    host: null
    host_type: *id005
    in_config_only: false
    ip: 10.197.10.115
    job_cpu: 1.0
    job_memory_mib: 63488
    metadata_source: *id003
    name: demodcrtest4-fe3
    namespace: tenant-124634-prod
    provisioning: false
    reachable_url: !!python/object/new:urllib.parse.ParseResult
    - https
    - demodcrtest4-fe3.medallia.com
    - ''
    - ''
    - ''
    - ''
    redeploy: *id004
    suffix: null
    up: true
    url: !!python/object/new:urllib.parse.ParseResult
    - https
    - demodcrtest4-fe3.medallia.com
    - ''
    - ''
    - ''
    - ''
    version: express-e689.155
  ip_subnets: []
  is_production: true
  name: demodcrtest4
  namespace: tenant-124634-prod
  reachable_url: !!python/object/new:urllib.parse.ParseResult
  - https
  - demodcrtest4.medallia.com
  - ''
  - ''
  - ''
  - ''
  redeploy: *id004
  url: !!python/object/new:urllib.parse.ParseResult
  - https
  - demodcrtest4.medallia.com
  - ''
  - ''
  - ''
  - ''
webapp_id: 401689
```


New

```json
{
	"_params": {
		"task_type": "MecBounceNode",
		"context": "AUTO-20240731010947",
		"shared_context": null,
		"step_modifiers": {},
		"jira_issue": null,
		"jira_assignee": null,
		"jira_related_issues": "[]",
		"mec_url": null,
		"mec_max_instance_age_in_days": 31,
		"mark_completed_on_success": true,
		"allow_params_override": false,
		"disable_safety_checks": false,
		"instance": "demodcrtest4",
		"dc": "den",
		"node_name": "demodcrtest4-fe1",
		"force": false,
		"wait_terminate_s": 900,
		"wait_ready_s": 900,
		"kill_pod": false
	},
	"region": null,
	"country_id": null,
	"dc": null,
	"first_user_email": null,
	"first_user_first_name": null,
	"first_user_last_name": null,
	"mec_instance_id": null,
	"mec_tenant_id": null,
	"mec_display_name": null,
	"mec_instance_dc": null,
	"mec_url": null,
	"mec_instance_age": null,
	"account": null,
	"sso_configured": false,
	"revreq_issue": null,
	"topology": {
		"name": "demodcrtest4",
		"dc": "den",
		"url": "https://demodcrtest4.medallia.com",
		"dc_url": "https://demodcrtest4.den.medallia.com",
		"reachable_url": "https://demodcrtest4.medallia.com",
		"frontends": [
			{
				"name": "demodcrtest4-fe1",
				"dc": "den",
				"url": "https://demodcrtest4-fe1.medallia.com",
				"dc_url": "https://demodcrtest4-fe1.den.medallia.com",
				"reachable_url": "https://demodcrtest4-fe1.medallia.com",
				"host": null,
				"ip": "10.197.10.113",
				"host_type": "fe",
				"redeploy": "SKIP",
				"cluster_type": "kubernetes",
				"cluster_name": "demodcrtest4",
				"namespace": "tenant-124634-prod",
				"suffix": null,
				"version": "express-e689.155",
				"config_changeset": "4cdcbd5ad22933775a385747c13f21a63e5f1656",
				"job_cpu": 1.0,
				"job_memory_mib": 63488,
				"heap_memory_gib": 40,
				"up": true,
				"provisioning": false,
				"deployment_revision": null,
				"metadata_source": "kubernetes_custom_resource",
				"in_config_only": false
			},
			{
				"name": "demodcrtest4-fe2",
				"dc": "den",
				"url": "https://demodcrtest4-fe2.medallia.com",
				"dc_url": "https://demodcrtest4-fe2.den.medallia.com",
				"reachable_url": "https://demodcrtest4-fe2.medallia.com",
				"host": null,
				"ip": "10.197.10.114",
				"host_type": "fe",
				"redeploy": "SKIP",
				"cluster_type": "kubernetes",
				"cluster_name": "demodcrtest4",
				"namespace": "tenant-124634-prod",
				"suffix": null,
				"version": "express-e689.155",
				"config_changeset": "4cdcbd5ad22933775a385747c13f21a63e5f1656",
				"job_cpu": 1.0,
				"job_memory_mib": 63488,
				"heap_memory_gib": 40,
				"up": true,
				"provisioning": false,
				"deployment_revision": null,
				"metadata_source": "kubernetes_custom_resource",
				"in_config_only": false
			},
			{
				"name": "demodcrtest4-fe3",
				"dc": "den",
				"url": "https://demodcrtest4-fe3.medallia.com",
				"dc_url": "https://demodcrtest4-fe3.den.medallia.com",
				"reachable_url": "https://demodcrtest4-fe3.medallia.com",
				"host": null,
				"ip": "10.197.10.115",
				"host_type": "fe",
				"redeploy": "SKIP",
				"cluster_type": "kubernetes",
				"cluster_name": "demodcrtest4",
				"namespace": "tenant-124634-prod",
				"suffix": null,
				"version": "express-e689.155",
				"config_changeset": "4cdcbd5ad22933775a385747c13f21a63e5f1656",
				"job_cpu": 1.0,
				"job_memory_mib": 63488,
				"heap_memory_gib": 40,
				"up": true,
				"provisioning": false,
				"deployment_revision": null,
				"metadata_source": "kubernetes_custom_resource",
				"in_config_only": false
			}
		],
		"backend": {
			"name": "demodcrtest4-be",
			"dc": "den",
			"url": "https://demodcrtest4-be.medallia.com",
			"dc_url": "https://demodcrtest4-be.den.medallia.com",
			"reachable_url": "https://demodcrtest4-be.medallia.com",
			"host": null,
			"ip": "10.197.10.112",
			"host_type": "be",
			"redeploy": "SKIP",
			"cluster_type": "kubernetes",
			"cluster_name": "demodcrtest4",
			"namespace": "tenant-124634-prod",
			"suffix": null,
			"version": "express-e689.155",
			"config_changeset": "4cdcbd5ad22933775a385747c13f21a63e5f1656",
			"job_cpu": 1.0,
			"job_memory_mib": 63488,
			"heap_memory_gib": 40,
			"up": true,
			"provisioning": false,
			"deployment_revision": null,
			"metadata_source": "kubernetes_custom_resource",
			"in_config_only": false
		},
		"database": null,
		"namespace": "tenant-124634-prod",
		"ip_subnets": [],
		"cluster_type": "kubernetes",
		"is_production": true,
		"branch": "master",
		"redeploy": "SKIP"
	},
	"ip_subnets": "[]",
	"tenant_id": null,
	"instance_id": 124634,
	"webapp_id": 401689,
	"display_name": null,
	"instance_type": "PROD",
	"db_name": null,
	"configuration_commit": {
		"sha": null,
		"time_stamp": null
	},
	"clusterconfig_commit": {
		"sha": null,
		"time_stamp": null
	},
	"deployment_commit": {
		"sha": null,
		"time_stamp": null
	},
	"deployment_security_commit": {
		"sha": null,
		"time_stamp": null
	},
	"puppet_commit": {
		"sha": null,
		"time_stamp": null
	}
}
```
