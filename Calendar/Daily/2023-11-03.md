---
id: 9
up: "[[2023-W44]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20231103101205
modified: 20231103172716
aliases:
  - Friday - November 3rd 2023
linter-yaml-title-alias: Friday - November 3rd 2023
title: Friday - November 3rd 2023
week: "[[2023-W44]]"
yearly: "[[2023]]"
quarterly: "[[2023-Q4]]"
monthly: "[[2023-11]]"
daily: "[[2023-11-03]]"
month: "November"
weekday: Friday
---

# Friday - November 3rd 2023

## Tasks

%% TCT_TEMPLATED_START 2023-11-03 00:00 %%

%% TCT_TEMPLATED_END 2023-11-03 23:59 %%
- ? Did these tasks align to your Goals?

# Rollover

# Daily Notes

What are you grateful for?

What are you looking forward to today?

What did you like about today?

---





Because, you know, Evan mentioned that they might be interested in doing some dev work on that. Yeah. Yeah. I think if we can, like, portion out, like, everything properly, then the portion that's directly, like, mapping the tasks to the, sorry, the, like, events to the tasks, I think that would be a good, like, place for them to help. It might. But still, I think that, you know, we, especially you and Varasames need to, because since it's going to be, maybe it's too early to go through that, but since it's probably, I mean, the SRE router is going to be probably written in Python, someone needs to design the architecture of how the program is going to be structured, design the interfaces, design the libraries, you know, make some kind of choice before giving something to someone to implement them, to implement something on their own. That would be a wrong way, I guess. I agree. Yeah, but again, it's maybe too early to go through this, so, yeah. Yeah, yeah, because, like…


…



I think we are going to impact the verification into the repo. If we're using the prod deployer like method, when I say that I mean if we're using the prod deployer via like a ProvenG call, then yes, that's how it works. We spin up a single session, like when we load the prod deployer with the CLI we designate only one cluster to be loaded with it so that we don't have to wait for the whole thing to load all the clusters when all we need to do is make changes for one of them. Like that's how we do provisions. Okay, but what we want right now is just to get rid of the prod deployer and have something else. So for that statement, what do you think that we should do? Well, I don't know for certain, but what I know is possible based on what Michal had set up and what we're looking at right now is that we can interact directly like in the same way that Larry from the prod deployer is interacting with the custom resources, which are like the backend and the frontend nodes. We can interact with those directly as well. For this deployment operation, like he has already implemented this. Like we can basically do exactly what he's doing. And the reason we had to do that, like when I say we, I mean him, I haven't modified this code. The reason that he did that is because it would have been like not like feasible to spin up a session for each cluster via the prod deployer in the same way that we are. Like it just wouldn't have, like it probably would have blasted the hosts because the deploy hosts. Yeah, I know. Sorry if I'm like, I'm just trying to be as like explicit as possible. So like I'm walking like myself through the process, like from what I remember and why the decision was made and whatnot to do it like this. Yeah, this is good. Great. I really appreciate this because I just want to understand if, I don't know if the SRE API should be like the deployer, the full deployer, okay. I don't know that. But I want to understand if we have the capability to do that because if we have, maybe we can implement a new API or a new service or we can take care of that. But that means that we should decouple the deployer from the SRE provisioning API, right? Because it's not the provisioning task. It's just a deployment. Yeah. It's kind of its own special thing right now, which we kind of tried to, like from what I know, like from what I could tell, we were trying to apply the same, okay, this is how we provision. Let's take that same flow and apply it to deployments. But I feel that there was hesitance from the RMT team when they saw that they're going to have to now like manage like hundreds and hundreds of JIRAs a week, which like is just not the case for provisioning. Like we have like a steady amount, but it's not like this huge influx. It's like deployment operations more of like, yeah, we need to do a lot at once, but we don't want to, we don't need the same accounting accountability, but we do need one place to see the progress of all of them. Like hence why I created like that document to kind of, like it's the best of both worlds in that we still leverage part of the code that was written to actually like execute like all of the clusters that are like specified in the JIRA, except we're just not creating more JIRAs. We'd be immediately going into the state of deploying it based on the cluster. Hold on. Based on whether or not it is K8s and simple deployment, Mesos or K8s and not simple deployment or an OCI. And like, I think this, this is like the most important client, like, you know, how many lines? This is the most important, like eight lines of code that there could be possibly, because this is what causes so much confusion. I think they're like, oh, what? Okay. It's this cluster. We need to do it here. It's that cluster. We need to do it here. We did that cluster. We need to do it here. Like that has to be like thought of every single time. And if we could just like confine the, like in my opinion, if we could consolidate the like, like every interaction to a proven G call, then no one would have to ever know. They would never have to think about that. And so, yeah, that's. And about the Express Operator, where Express Operator takes place here? Express Operator. Yeah. So like where it's relevant is, I mean, it's relevant in all of them. Right. But it's most relevant, I guess, like to what we're talking about in this redeployed Colos K8s instance method, because it's actually like performing the, it's, it's, it's do it's this here, for example, this patch CR. Actually going into the custom resource itself and saying, Hey, I noticed that your express version is this. I'm going to change it to this. I'm going to change it to a different one. Like, which is what is, which is the same thing is that the product player is doing. It's just doing it in a, in a vastly different way. We're going to change. We're going to change the express cluster, change the version. We're going to patch it. We're going to bounce it and wait for it to come back up, make sure that it's starting and pass it again. I'm not sure what the second patch is for. Cluster. No. Okay. It's true. Cluster. No. Okay. It's false. All those tasks are handled by the operator, right? Well, yeah. Well, the operator is, is, is providing us with the resources and we're just making changes to the resources directly. So we changed the resources and the operator is monitoring the resources. Yeah. The operator is still changed. Yeah. The operator is still maintaining the resources in this case. We're still using it. We're just, we're, we're, we're just doing in our own way, doing exactly what the product player is doing, which is for, but we also don't have, we don't have the same capability as the product player. We don't have all of the functions that it can have, but for this one function, which is the deployment we do, which is changing the express version out and then bouncing the nodes in a certain way. And we don't have the full control of the data as the product player has because we don't ask for that or because you know what? What do you mean exactly? You say that we don't have the same capability that the product player has with. Yeah. Well, like right. The product. Yeah. Like the product player has like all of this different, you know, macros and functions like binding the node and unbinding the node and do stuff like that. Like we're not, we, we don't, we don't, we haven't implemented all of those. But like for, as, as it relates to what we're talking about right now, what we have implemented is the deployment operation, which is swapping out the express version and then bouncing the nodes in a certain way. That's all I meant to say. Okay. Okay. So Okay. I guess like what it would help to, what it would, what it would help to understand is what the, like, like when we say, okay, we won't make deployments. Like what, what exactly, how do we want it to work? Can we get a consensus on how we would want it to work? So that if there are any changes that need to be made to this, like we can make them like to this logic, for example, like when do we want it? When do we want it to like, when do we want it to use the product player? And when do we want it to not use that? I think those are the, that's the, that's the real question. Yeah. Yeah. I guess that just for the missiles, right. We need to use. Or if like for whatever reason, for whatever reason, if we, I think this might be a flag. Let me see. Maybe that is.



Do a demo for CIF

We will be able to manage the deployment just for clusters that are running in Kubernetes and that's it, and production only, okay, for now. And that should be like our first goal. But of course, we already have most of that code already written, right? So maybe we can start from that and start working with them to try to understand for what we need CIF in the middle, right? Because if the operator is working well in Colossus, I don't know. Is there anything that we should need from them? From CIF, I guess. Yeah, I don't… If we talk about Colossus, okay, and Kubernetes, I would like to understand that we need something from them. So since we're effectively being our own prod deployer, like in this case, then no, I don't think we do need anything from them because we're just… Since from their perspective, they just see their express operator as an interface, well, Larry has his prod deployer and it's interfacing with it. So in this way, like for the deployment tasks specifically, we would be doing the same thing. Okay. Okay, let me… What I'm going to do is I'm going to prepare just a document with a couple of… with the scope and a couple of goals that I believe that we will need. And that we already have for monthly, maybe we already have that covered from the code that we have. Of course, maybe we are going to need to do some modifications or update the code based on the need that we have, but let me do that, okay? Because I have a meeting with Hermann next week and I want to understand everything that was happening here in the deployment side. Yeah, yeah. I can also, if you need me to do any like testing, I can do some of that as well and like provide the results of the test in like some kind of document that you can show. So, you know, whatever it is you need. Okay. Yeah. Okay. Yeah. And again, this is not so critical right now, but I would like to start working on something because I need to talk with… about this and he wants to prioritize this one, but I'm really… I really want to work in this router first, okay? Or at the same time, right, but I would like to have both projects this quarter because I feel like this is going to be awesome and it's going to be a really good tool that is going to be useful for a lot of people. Yeah, I agree. But I almost feel like the deployment stuff, most of it has been done. I think we just need to decide like on how we want to use it. And if like the JIRA approach is not like, like I might be making assumptions. Like, I don't know. I don't know why it was never adopted. I think that's what I want to know. Like, why was it? What hesitance created the lack of the, you know, the resistance to adopt? Do you know the answer to that or does anyone? No, no, no, no, not really. I know that they say he's using the task. I don't know to do what, but I know that he's using it. Oh, okay. I didn't know that. But yeah. Yeah. Yeah. Yeah. Yeah. Actually we have a couple of, I believe that he, last time he did Sydney, the entire data center. But I don't know how that go. Oh, nice. Okay. That's when I, yeah, I need to check with him. Okay, man. Okay. I have a lot. Thank you for that. I will work on this. Really. Sure. The planning for this quarter. I already spoke with or something about that. Please just take a look at the. That is nothing. Out of what we already follow. Just take it. Okay. Okay. I will. Okay. Thank you for your time. Have a good day. We can.



---


```shell
> show cluster itruffattest
...
itruffattest-fe4 | N       | Not ready | lightning | c794db64f7bc17fb3eefce28d0496a6eeaaab536 | express-e683.234 |                  | -          |
...
=> start itruffattest-fe4
itruffattest-fe4: OK (job aaed6018-2d69-464b-b5b6-3a0daa2c2ea8 created)
Job aaed6018-2d69-464b-b5b6-3a0daa2c2ea8 started.

Events:
  Type    Reason     Age   From               Message
```


```shell
Normal  Scheduled  88s   default-scheduler  Successfully assigned tenant-121901-prod/itruffattest-fe4 to den-r18-u09
  Normal  Pulled     85s   kubelet            Container image "virtual-docker.martifactory.io/medallia/giraffe/telegraf:1.19.3-3" already present on machine
  Normal  Created    84s   kubelet            Created container telegraf-sidecar
  Normal  Started    84s   kubelet            Started container telegraf-sidecar
```


```shell
./bouncenode itruffattest-be                                                                                                                   3s
This will force delete the pod 'itruffattest-be'. If you want to gracefully delete the pod, please type 'N'

Are you sure? Y Attempting to bounce the pod gracefully
Bouncing production node itruffattest-be in den for instance itruffattest with force flag set to true
```
- Bounced the `be` node:
	- [SRE UI | MecBounceNode - itruffattest - Task sreclient-1pjnu3hhlsltqyu](https://sre-provisioning-api.eng.medallia.com/ui/task/sreclient-1pjnu3hhlsltqyu)



```
A/A Status:
	    Classic Frontends
	        itruffattest-fe1                   : SYNCHRONIZED    visible    sync     e683.234 ready           0e4e426b0abab47f
	        itruffattest-fe2                   : SYNCHRONIZED    visible    sync     e683.234 ready           0e4e426b0abab47f
	        itruffattest-fe3                   : SYNCHRONIZED    visible    sync     e683.234 ready           0e4e426b0abab47f
```
- Unfortunately it seems that the new node is not showing up though after `be` was restarted



---


```
=> deploy-cluster itruffattest express-e683.234 config=231c45c2571edfafca50848221e7809c2a840c22
Job c1410ca5-f690-4f67-b9d0-66427d7d2078 started.
```
- [231c45c2571edfafca50848221e7809c2a840c22](https://github.medallia.com/medallia/configuration/commit/231c45c2571edfafca50848221e7809c2a840c22)

Instead did:  
[SRE UI | MecDeployCluster - itruffattest - Task prov-kzycof9ya2fa0bj](https://sre-provisioning-api.eng.medallia.com/ui/task/prov-kzycof9ya2fa0bj)
- `express-e683.234`
- `231c45c2571edfafca50848221e7809c2a840c22`



No deploy cluster is happening after the configuration changes.

So it seems that the `feX` node is not getting the right configuration (not the new one) when it is created.

So when I perform the `MecDeployCluster` , since the `be` node is not re-deployed first. Hence why it does not pick up the changes correctly:

```
"stepStateDetail": [
        "itruffattest-fe1: COMPLETED",
        "itruffattest-fe2: COMPLETED",
        "itruffattest-fe3: READY_FOR_DEPLOYMENT",
        "itruffattest-fe4: READY_FOR_DEPLOYMENT",
        "itruffattest-be: STOPPING"
      ],
```
