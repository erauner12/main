---
up: "[[2024-W16]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20240416000100
modified: 20240417104038
aliases:
  - Tuesday - April 16th 2024
linter-yaml-title-alias: Tuesday - April 16th 2024
title: Tuesday - April 16th 2024
id: 10
week: "[[2024-W16]]"
yearly: "[[2024]]"
quarterly: "[[2024-Q2]]"
monthly: "[[2024-04]]"
daily: "[[2024-04-16]]"
month: "April"
weekday: Tuesday
---

# Tuesday - April 16th 2024

## Local

```
export dc=smf1
export instance=sretestsmf1
export id=124478

kubectl get pods -n tenant-${id}-prod --context ${dc} -o wide | grep "${dc}-prod-db-${instance}"
```

```
pwd
/Users/erauner/repos/pg-scripts
```

```
kubectl delete -f production/kubernetes/manifests/${dc}/${dc}-prod-db-${instance}-pg-backup.yaml --context smf1
kubectl delete -f production/kubernetes/manifests/${dc}/${dc}-prod-db-${instance}-monitor.yaml --context smf1
kubectl delete -f production/kubernetes/manifests/${dc}/${dc}-prod-db-${instance}.yaml --context smf1
kubectl delete -f production/kubernetes/manifests/${dc}/${dc}-prod-db-${instance}-setup.yaml --context smf1

kubectl get pods -n tenant-${id}-prod --context ${dc} -o wide | grep "${dc}-prod-db-${instance}"
```

```
netapp_server=$(yq eval ". | select(.kind == \"StatefulSet\") | .spec.template.spec.volumes[] | select(.name == \"${dc}-prod-db-${instance}-backup\").nfs.server" ./production/kubernetes/manifests/${dc}/${dc}-prod-db-${instance}.yaml)

netapp_directory=$(yq eval ". | select(.kind == \"StatefulSet\") | .spec.template.spec.volumes[] | select(.name == \"${dc}-prod-db-${instance}-backup\").nfs.path" ./production/kubernetes/manifests/${dc}/${dc}-prod-db-${instance}.yaml | cut -d '/' -f -2)

echo $netapp_server
echo $netapp_directory
```

- Get the values for next step

## Remote

```
ssh ssh1-smf1
```

```
ssh smf1-r12-u10
```

- Or any host

```
export dc=smf1
export instance=sretestsmf1
export id=124478
```

```bash
viper delete-volume -v ${dc}-prod-db-${instance}
```

```
sudo mount 10.84.125.4:/mdbs_backup1 /mnt/mdbs/backups/mdbs_backup1 sudo rm -rf /mnt/mdbs/backups/mdbs_backup1/smf1-prod-db-sretestsmf1 sudo umount /mnt/mdbs/backups/mdbs_backup1
```

## Local

Delete branch

```
git push origin :prov-ng/provision_${instance}
```

```
exec-gesp-prov den SREPROV-216 retry
```

```
kubectl get pods -n sre-provisioning --context smf1 --no-headers | awk 'NR==1{print $1}' | xargs -I {} kubectl logs {} -n sre-provisioning --context smf1
```

[SRE UI | Log db-pgscripts for task MecProvisionInstance - sretestsmf1 - Task automation-xsphhzou2zpqf8q](https://sre-provisioning-api-dev.den.medallia.com/ui/log/automation-xsphhzou2zpqf8q/db-pgscripts)

---

the configuration PR it made is not correct:

[https://github.medallia.com/medallia/configuration/pull/21553/files](https://github.medallia.com/medallia/configuration/pull/21553/files)

has `None` in it multiple times.

---

need to compare to a different net new cluster config PR to see what it should look like.

`variables/smf1.medallia.com/sretestsmf1/express/clusterNodes.var`

- `http://systemdcrtest3-be.tenant-124486-prod.svc.k8s.yul1.medallia.ca`

`variables/smf1.medallia.com/sretestsmf1/express/dbHost.var`

- `yul1-prod-db-systemdcrtest3.tenant-124486-prod.svc.k8s.yul1.medallia.ca`

`variables/smf1.medallia.com/sretestsmf1/express/tenantId.var`

- `124485`

ex: [Added/Modified systemdcrtest3 via automation (#21481) Â· medallia/configuration@54535e7](https://github.medallia.com/medallia/configuration/commit/54535e7da89c80e5a478627d0998dd42fce222c4)

---

also need to figure out why it did that in the first place? (perhaps since lbaas did not finish)

- see what vars it is relying on in that particular step.
