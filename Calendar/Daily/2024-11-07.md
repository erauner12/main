---
up: "[[2024-W45]]"
tags:
  - periodic/daily
cssclasses:
  - cards
  - cards-cols-1
template: "[[Daily]]"
created: 20241107105615
modified: 2024-11-07T15:44:57-06:00
aliases:
  - Thursday - November 7th 2024
linter-yaml-title-alias: Thursday - November 7th 2024
title: Thursday - November 7th 2024
id: 10
week: "[[2024-W45]]"
yearly: "[[2024]]"
quarterly: "[[2024-Q4]]"
monthly: "[[2024-11]]"
daily: "[[2024-11-07]]"
month: November
weekday: Thursday
---

# Thursday - November 7th 2024

## Memos Personal

## Memos Work

## Working On


I agree that introducing a new client directly inside of prov-ng may not actually make sense at this point, but I wanted to explore the idea. Since you created a few clients, I decided to try it as well. But yeah, I'll move those k8s client specific methods to sre python lib (if they do not already exist)  
However, I disagree with adding more bloat to what is already a cluttered utils for what remains. Especially since all of the methods being introduced shared the same thing in common. The current utils already contains too many disparate helper methods, which is why I wanted to separate the k8s-specific helper methods into their own utility file. This approach initially made sense to me due to the watch functionality that required a connection, which several methods could utilize.  
this:

```
def __init__(self, kubeconfig: str):
        self.k8s_client = K8sClient(kubeconfig)
        self.k8s_client.connect()
        self.watch = watch.Watch()
```

In the current implementation, gitops clusters that use MecBounceNode rely on this approach to avoid unnecessary polling of information, opting instead to wait for it directly and then immediately updating the step states accordingly. However, we will still poll when there are no other options, which is what we will do for non-GitOps clusters that do not yet have probes. Both methods are supported without having to account for what ExpressClusterType that it is.

I will go ahead and move everything related to the k8s client to the sre python lib. However, I need to find a more suitable place than the lib/utils.py for the remaining Kubernetes helper methods that do not belong in the Kubernetes client directly, as they are built one layer above it from what I can tell. At this moment, I am unsure of the best place for these methods are yet. Though I'm curious what you think about it.  
I'll relay these comments in the PR so that I can get your opinion(s) directly in the comments (edited)

Evan Rauner  
:ooo: 2 minutes ago  
Perhaps the express_topology.py would be a better existing place for the pod specific logic

—

These are some reflections that I had based on some feedback that I received, from my coworker on my pull request, which has kind of prompted me to think that removing the Kubernetes client or the k eight's client from the provisioning n g, repo is probably the best move. So we should migrate that to, the SRE Python lib. But for the remaining is what I don't have the best answer for yet, which I wanted to get, like, your opinion, opinion on.




https://medallia.slack.com/archives/C0729AFDT3J/p1731000509873849?thread_ts=1730993500.748109&cid=C0729AFDT3J

https://medallia.slack.com/archives/C0729AFDT3J/p1731000509873849?thread_ts=1730993500.748109&cid=C0729AFDT3J

https://medallia.slack.com/archives/C0729AFDT3J/p1731001195143659?thread_ts=1730668066.385889&cid=C0729AFDT3J

https://medallia.slack.com/archives/C0729AFDT3J/p1731001195143659?thread_ts=1730668066.385889&cid=C0729AFDT3J





Here’s an updated draft for a Slack thread focused on the impact and urgency of this issue, along with relevant log snippets:

---

**Title**: "Urgent: Securitas Emperor OAuth Token Retrieval Failure Affecting SRE API Tasks"

**Message**:  
We're currently experiencing a critical issue where SRE API tasks are failing due to OAuth token retrieval issues with the Securitas Emperor service. This is impacting all provisions and tasks that depend on SRE API, leading to significant blockage in our workflows. Here’s a breakdown:

1. **Issue Details**:
   - All attempts to retrieve an OAuth token from `https://securitas-infra.eng.medallia.com/services.sre/oauth/token` are failing with a `503 Service Unavailable` response.
   - This issue is affecting all tasks and provisions routed through the SRE API, halting ongoing processes and blocking new ones.

2. **Relevant Log Snippets**:  
   Below are specific log entries demonstrating the token retrieval failure and its downstream impact:

   ```
   2024-11-07 21:26:19,641 DEBUG oauth2_session Requesting url https://securitas-infra.eng.medallia.com/services.sre/oauth/token using method POST.
   2024-11-07 21:26:19,647 DEBUG connectionpool https://securitas-infra.eng.medallia.com:443 "POST /services.sre/oauth/token HTTP/1.1" 503 62
   2024-11-07 21:26:19,648 DEBUG oauth2_session Request to fetch token completed with status 503.
   ```

   ```
   2024-11-07 21:26:19,650 DEBUG abstract_step updating step state to: {'stepName': 'init', 'stepState': 'FAILED', 'stepStateDetail': ['exception: Failed to retrieve OAuth token'], 'updatedAt': '2024-11-07T21:26:19.650153Z'}
   ```

   ```
   medallia_sre.securitas.SecuritasTokenException: Failed to retrieve OAuth token
   ```

This issue is preventing critical provisioning steps and other tasks from progressing. We need immediate assistance to restore SRE API functionality to unblock these provisions and ensure smooth task execution.

Let us know if you need additional logs or further details.
