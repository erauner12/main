---
up: "[[2024-W21]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20240523000100
modified: 20240523170125
aliases:
  - Thursday - May 23rd 2024
linter-yaml-title-alias: Thursday - May 23rd 2024
title: Thursday - May 23rd 2024
id: 10
week: "[[2024-W21]]"
yearly: "[[2024]]"
quarterly: "[[2024-Q2]]"
monthly: "[[2024-05]]"
daily: "[[2024-05-23]]"
month: "May"
weekday: Thursday
---

# Thursday - May 23rd 2024

need to remove: `snapshotsByNode` since static rebuilds are no longer a thing:

https://github.medallia.com/gist/erauner/f1bea061a1567911ff4fa069fb95d102

First noticed here:

```
2024-04-23 18:32:53,786 INFO tunnel stopping kubectl tunnel to pod/comcast-fe1 in tenant-101471-prod
2024-04-23 18:32:53,786 DEBUG tunnel stopped kubectl proxy on port 33521
2024-04-23 18:32:53,788 DEBUG misc got exception missing value for field "snapshotsByNode", sleeping for retry
```

- This is the error I am seeing that is making me start this.

Confirm with Sergio that we are no longer running anymore static rebuilds.

https://workmemos.erauner.synology.me/m/3uVYVnFQQmW84AHqNQEgeC  
UID: 3uVYVnFQQmW84AHqNQEgeC

---

Dcr instance configure step should pretty much always update with latest info  
https://workmemos.erauner.synology.me/m/m8cXkacr4dYHTNGCaUzuMN  
UID: m8cXkacr4dYHTNGCaUzuMN

---

Create a DCR specific Branch

should be: `SREPROVNG-606`

https://jira.medallia.com/browse/SREPROVNG-606  
https://workmemos.erauner.synology.me/m/XVcFSCQRoTBxhBrVtBN3Yu  
UID: XVcFSCQRoTBxhBrVtBN3Yu

---

https://medallia.slack.com/archives/C0729AFDT3J/p1716202337955129?thread_ts=1716060843.410229&channel=C0729AFDT3J&message_ts=1716202337.955129

Figure out how to exclude certain cases for the dcr alert when a node is just downloading caches not actually rebuilding  
https://workmemos.erauner.synology.me/m/8QhSvQakFF9EzAboZNeWTT  
UID: 8QhSvQakFF9EzAboZNeWTT

---

[Task Link](https://app.todoist.com/app/task/7976943384)

Create a jira for idempotency

https://workmemos.erauner.synology.me/m/oVWJ6mSFnd7eEqXye96uxa

Fix this

```
2024-05-07 21:38:27,747 DEBUG update_farm_replicas Post Update Check: Waiting for STS verisure-dcr to reach 1 ready replicas
2024-05-07 21:38:27,747 DEBUG k8s_client closing k8s client
2024-05-07 21:38:27,748 DEBUG misc got exception step '?' failed in '_wait_until_ready_replicas_reach_desired' with: STS verisure-dcr does not
 have 1 ready replicas yet. Current number of ready replicas: 20. , sleeping for retry
```

To be idempotent. Should tweak that slightly. If re-approved, if waiting for HU, assume if more nodes are already up that the HU completed.  
https://workmemos.erauner.synology.me/m/oVWJ6mSFnd7eEqXye96uxa  
UID: oVWJ6mSFnd7eEqXye96uxa

---

Makes me think as well. DCR might not need to wait the entire time. Perhaps it only needs to wait until it fails, and then another alert lets us know that it failed.

But then how would you capture the error in this case?

I think another downside would be that it would be hard to see all of the DCRs that are currently in flight are.  
https://workmemos.erauner.synology.me/m/nEkDjiF4p7c4WsbQNHYJJq  
UID: nEkDjiF4p7c4WsbQNHYJJq

---

Deprecate the dcr stop farm  
https://workmemos.erauner.synology.me/m/CRZWaH8XXoYUiHhtQcRDnt  
UID: CRZWaH8XXoYUiHhtQcRDnt

---

Make sure that DCR does not start unless farm is up

Check at the beginning of the trigger/wait step unless the farm is up

```
2024-04-01 18:08:50,343 DEBUG misc got exception step 'StepDCRFarmWaitForCompletion' failed in '_respond_to_dcr_state' with: DCR still waiting
 to start rebuilding the caches., sleeping for retry
```

https://workmemos.erauner.synology.me/m/kFoL56PYT4JZzzK6m2K8JT  
UID: kFoL56PYT4JZzzK6m2K8JT

---

Fix dcr path:

https://medallia.slack.com/archives/D024LL4PSKT/p1711998259072109  
https://workmemos.erauner.synology.me/m/CckGhFKafZxSLNkbpTaFWG  
UID: CckGhFKafZxSLNkbpTaFWG

---

Fix dcr vol

https://github.medallia.com/Atlas/deployment/pull/60560/files#diff-2f8658a479faa7fb59af805df88743fdc6e64ff06d7cb59bd71f317a797b745cR9  
https://workmemos.erauner.synology.me/m/9XbfpGSfp4GRj3riirJ3rc  
UID: 9XbfpGSfp4GRj3riirJ3rc

---

Take note at the beginning of DCR in the logs what the ETA is going to be  
https://workmemos.erauner.synology.me/m/7nW2uFCfJBPgGUoEpwMCtx  
UID: 7nW2uFCfJBPgGUoEpwMCtx

---

https://medallia.slack.com/archives/C065JNB1AE6/p1715957237392169?thread_ts=1715954166.159089&cid=C065JNB1AE6

Link Context:  
Work with Sergio to get doctor DCR script created  
https://workmemos.erauner.synology.me/m/n43p9wMsNYUsaWvzQ5TygG  
UID: n43p9wMsNYUsaWvzQ5TygG

---

Change the latest query to show you exactly which dcr clusters have had an ETA above 5d over the past year  
https://workmemos.erauner.synology.me/m/c7KBvqQCNBPzFzLrGUNc4Q  
UID: c7KBvqQCNBPzFzLrGUNc4Q

---

DCR: REBUILD_TO, REBUILD_NO_NEEDS_PUMPING  
ICR: REBUILD_FROM_TO  
Farmer: REBUILD_FROM_WORK_UNITS

REBUILD_FROM_TO is ICR, which would come after DCR or CR  
https://workmemos.erauner.synology.me/m/SpxA6NFxEaJzPCf8FmUZjb  
UID: SpxA6NFxEaJzPCf8FmUZjb

---

Over the next few days, if we start getting too many alerts that are not useful in any way, we can increase the amount of days

What I find interesting is that we could probably actually split this alert into two

One could identify which need their batch resized, and another could be specifically for DCR

But let's just have the one with a low number like 3 days so that we can see it all come in to see what we are working with  
https://workmemos.erauner.synology.me/m/SZDnRk47gz2cyz76tMt9p4  
UID: SZDnRk47gz2cyz76tMt9p4

---

Consider increasing DCR bucket size:

https://github.medallia.com/medallia/configuration/blob/sre-provisioning-api-server-delivery/templates/sre-provisioning-api-server/var/provision/api/config/generic/main.yaml#L29  
https://workmemos.erauner.synology.me/m/bph95nHFCEa3thzL2T8pwh  
UID: bph95nHFCEa3thzL2T8pwh

---

https://medallia.slack.com/archives/C070UU168N4/p1715858452911209?thread_ts=1715699548.222019&channel=C070UU168N4&message_ts=1715858452.911209

Link Context:  
Check all dcr that Murtaza ran  
https://workmemos.erauner.synology.me/m/8BaHytALPkKtcbDxRgPwcx  
UID: 8BaHytALPkKtcbDxRgPwcx

---

Create a jira to see the progress of all DCRs running at once  
https://workmemos.erauner.synology.me/m/RVvVKqi9NodCtqVFDJeid9  
UID: RVvVKqi9NodCtqVFDJeid9

---

But if cluster B with ETA 10 days and batch size of 25k, we need to use DCR  
https://workmemos.erauner.synology.me/m/XZbsNFhw3Nd3uuEFTMstW4  
UID: XZbsNFhw3Nd3uuEFTMstW4

---

Check samsclub for DCR:

https://medallia.slack.com/archives/C0729AFDT3J/p1715806985003459?thread_ts=1715798201.788549&cid=C0729AFDT3J  
https://workmemos.erauner.synology.me/m/PD63mtjxZGuq8WTTB9Z9Yh  
UID: PD63mtjxZGuq8WTTB9Z9Yh

---

Would be nice to have like a high level dashboard to see everything like you mentioned though

That would be incredible to see at the DC level to see how much resources we are taking up at a given time across multiple clusters

…

https://medallia.slack.com/archives/D02JV6KRTQF/p1715724608561329

…

---

https://medallia.slack.com/archives/D02JV6KRTQF/p1715722490065639

- What if I want to track >1 cluster’s progress at once?
- Can I see a list of potential DCR candidate clusters here? (Long-running CRs/high record count maybe)
- What’s being shown in the top left panel? % complete? If so might be good to set the unit for that so it shows the % indicator  
     https://workmemos.erauner.synology.me/m/f65au4LhBHoFFijCUxXpne  
     UID: f65au4LhBHoFFijCUxXpne

---

Fix this in DCR:

https://medallia.slack.com/archives/C0140KBUPGC/p1715716037316759?thread_ts=1715714454.898729&cid=C0140KBUPGC

```
2024-05-14 19:51:30,207 INFO deployer_cli CLI: sending: show jobs
2024-05-14 19:51:30,264 DEBUG deployer_cli CLI: output: show jobs
 Id                                   | Name                      | Time | Done | Status                   | Error |
--------------------------------------+---------------------------+------+------+--------------------------+-------+
 0c90bb8f-1e73-46de-bb94-2bdaffe428c0 | create                    | 0 s  | No   | Waiting on subtasks      |       |
 3b4eb2be-6357-4465-8a06-7d752c265325 |   Creating tamarriott-fe1 | 0 s  | No   | tamarriott-fe1: starting |       |
2024-05-14 19:51:30,265 INFO deployer_cli SHELL: sending quit
2024-05-14 19:51:30,674 DEBUG deployer_cli SHELL: response: quit
```

in some cases the node is not coming up after `create <fe node name>`  
https://workmemos.erauner.synology.me/m/QsefUZGxwA2xaXMZtqvH9i  
UID: QsefUZGxwA2xaXMZtqvH9i

---

Need guardian to protect DCR nodes

…

Talk to Ani about this  
https://workmemos.erauner.synology.me/m/m2mnvbWhnJJcngxowU27tx  
UID: m2mnvbWhnJJcngxowU27tx

---

Setup a DCR farm for @larry

`tamarriott` with 10 farmers.

```
./admin tamarriott
```

https://workmemos.erauner.synology.me/m/RbscSr4sRyAZdqY2LaURxB  
UID: RbscSr4sRyAZdqY2LaURxB

---

Make a dash that gets the previous Cache rebuild time for a single instance:

https://medallia.slack.com/archives/C8WPN4Q85/p1715706172484699?thread_ts=1714579736.361749&cid=C8WPN4Q85

Ex:

https://giraffe.eng.medallia.com/d/cc59f3f5-49c5-4a1c-ad57-9bf82188e8c1/dcr-progress?orgId=1&var-env=production&var-cluster=britishtelecom&var-express_node_filter=All&var-farmer_node_filter=All&var-company=All&from=now-12d&to=now&viewPanel=14

…

Should look over like the past year and see what the previous cache rebuilds took

…

What would be useful is to adjust this query on a tenant to tenant basis, to look at a single cluster over the past year and see how long it's previous cache rebuilds took.

…

This information would be useful to know right after the alert triggers.  
https://workmemos.erauner.synology.me/m/ekNLBc4hTEC4HQqx4zFy34  
UID: ekNLBc4hTEC4HQqx4zFy34

---

Adjust the DCR alert to only include clusers that are under a certain percentage by the time it reaches 2 day mark  
https://workmemos.erauner.synology.me/m/Kzpq6gn4ktjLo7xAMURxuQ  
UID: Kzpq6gn4ktjLo7xAMURxuQ

---

See why DCR alert (s) are not firing

https://medallia.slack.com/archives/C070UU168N4/p1715701504064549?thread_ts=1715699548.222019&cid=C070UU168N4  
https://workmemos.erauner.synology.me/m/mMgKanVKbVyf62VRbTsmJ4  
UID: mMgKanVKbVyf62VRbTsmJ4

---

https://medallia.slack.com/archives/D02JV6KRTQF/p1715694783968399

Link Context:  
Respond to Adam about DCR

…

Recording rule examples:

Recording rule:  
https://github.medallia.com/medallia/giraffe-alerts/blob/master/loki-rules/production_services/express/pump-recording-rules.yaml

Alert that uses it:  
https://github.medallia.com/medallia/giraffe-alerts/blob/master/rules/production_services/replicated-rules/pump_errors_detected.yaml  
https://workmemos.erauner.synology.me/m/CVESYcTqnjckS7GCCLU7NZ  
UID: CVESYcTqnjckS7GCCLU7NZ

---

fix `systemdcrtest1`, bring up FE1

https://medallia.slack.com/archives/C05LNMF72QY/p1715690411972759?thread_ts=1715688902.909909&cid=C05LNMF72QY

Link Context:  
Fix it  
https://workmemos.erauner.synology.me/m/dkPfsKCrg8NHSUcHnpvvd4  
UID: dkPfsKCrg8NHSUcHnpvvd4

---

Create step to propagate Caches after DCR is complete.

Instead of asking L1 to propagate the caches, go ahead and do this yourself after bringing the DCR nodes down.  
https://workmemos.erauner.synology.me/m/9uw6W86yejDdnyz5WtjCT4  
UID: 9uw6W86yejDdnyz5WtjCT4

---

Create a recording rule for dcr so we can have more than 30 days of data

Asked about this here:

https://medallia.slack.com/archives/C8WPN4Q85/p1715639060179129?thread_ts=1714579736.361749&cid=C8WPN4Q85

…

How often are the deployment cycles. I want to figure out with the metrics how we can always see how long the last cache rebuild took

…

Create a jira for this  
https://workmemos.erauner.synology.me/m/Zywodf6aYqk7ELrYoyDrHX  
UID: Zywodf6aYqk7ELrYoyDrHX

---

Address the issue that prevents you from being able to start a DCR against an express instance that is already running a regular CR.

Specifically without needing the disable safety checks

This is because, this likely going to be most often how you start DCR.

```
{
      "stepName": "init",
      "stepState": "FAILED",
      "stepStateDetail": [
        "exception: step 'StepInit' failed in '_perform' with: step failed with failed to run admin command '{'cmd': 'ExpressVersion', 'facet': 'TAG', 'format': 'json'}' on https://britishtelecom-fe4.medallia.eu/.cmdCenter with: HTTP Error code: 503, body: <!DOCTYPE html>\n<html dir=\"ltr\" lang=\"en\">\n<head>\n<meta charset=\"utf-8\" />\n<title>Medallia: Unavailable</title>\n<style>\n\t*{margin:0;padding:0;}\n\tbody{background: #F4F7FC url(https://cdn.medallia.com/survey-engine/survey_unavailable_bg.jpg)\n\t\ttop center no-repeat;font: 13px/1.3 helvetica,arial,sans-serif;text-align:center;}\n\t.wrapper{width:600px;margin:0 auto;text-align:left;padding:105px 0;}\n\t.content{padding:0 0 0 170px;}\n\ta{color:#0A5FBD;}\n\th1{font-size:40px;letter-spacing:-1px;line-height:.9;margin:0 0 15px;}\n\tp{color:#555;font-size:18px;margin:0 0 18px;}\n\tp.note{font-size:13px;}\n\tb{color:#000;letter-spacing:-.3px;}\n\tabbr{border:0;}\n</style>\n</head>\n<body>\n\t<div class=\"wrapper\">\n\t\t<div class=\"content\">\n\t\t\t<h1>Medallia is unavailable for a few minutes.</h1>\n\t\t\t<p>An issue came up that we are now addressing. Rest assured, we&rsquo;ll restore access to your customer&rsquo;s feedback <abbr title=\"As Soon As Possible\">ASAP</abbr>.</p>\n\t\t\t<p><b>The Medallia Team</b></p>\n\t\t</div>\n\t</div>\n<script type=\"text/javascript\">\n\t/* analytics */\n\tvar _gaq = _gaq || [];\n\t_gaq.push(['_setAccount', 'UA-23536725-3']);\n\t_gaq.push(['_setDomainName', '.medallia.com']);\n\t_gaq.push(['_trackPageview']);\t\n\t(function() {\n\t\tvar ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;\n\t\tga.src = ('https:' == document.location.protocol  'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';\n\t\tvar s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);\n\t})();\n\n</body>\n</html>\n, headers: {'Connection': 'close'}"
      ],
      "updatedAt": "2024-05-13T20:56:32.050498Z"
```

https://workmemos.erauner.synology.me/m/6nx62TcEmJETKKPaFiNeed  
UID: 6nx62TcEmJETKKPaFiNeed

---

Create jira for this

…

Make change to helm chart that will allow the node count to stay retained

So that all you need to do is flip false and true on the farmers

This is for dcr  
https://workmemos.erauner.synology.me/m/JgXvQjkRAamqitELHXRELX  
UID: JgXvQjkRAamqitELHXRELX

---

https://medallia.slack.com/archives/C065JNB1AE6/p1715008216855509?thread_ts=1714502169.533949&cid=C065JNB1AE6

Take DCR down before ICR begins  
https://workmemos.erauner.synology.me/m/BrBVVJPoUD3WVvhxH6t6MA  
UID: BrBVVJPoUD3WVvhxH6t6MA

---

See why the DCR alert is not firing yet?  
https://workmemos.erauner.synology.me/m/e5cirtFPTTHDFKfpiWFYmZ  
UID: e5cirtFPTTHDFKfpiWFYmZ

---

Identify DCR tenants and create a batch PR to add tenants

- We could try with all above 50
- Will pull anything over the last deployment cycle that took over 2 days.  
     2 days was an arbitrary number that I chose
- Or all with more than 100m records

https://medallia.slack.com/archives/C065JNB1AE6/p1715116028272989?thread_ts=1715017068.231429&cid=C065JNB1AE6  
https://workmemos.erauner.synology.me/m/T94Xi9JF9VTuMgrwvG8yTx  
UID: T94Xi9JF9VTuMgrwvG8yTx

---

Need to be able to hold DCR buckets by the DC

Currently only able to have buckets across all dcs. Would be beneficial to have bucket by dc

Ask Kamil on this

C  
https://workmemos.erauner.synology.me/m/5rcMQ2GBB9yVz6NDgoRpmt  
UID: 5rcMQ2GBB9yVz6NDgoRpmt

---

We need to create a resource quota for DCR to prevent resource contention.

This should span multiple namespaces. Per dc only this many resources can be used in the following namespaces  
https://workmemos.erauner.synology.me/m/NrHXJeDhAKP92atoby7JDV  
UID: NrHXJeDhAKP92atoby7JDV

---

Insert a message like:

"This tenant does not have dcr"

So that in the deployment repo we only have what clusters we need.  
https://workmemos.erauner.synology.me/m/8KaYUzNR5tFhZGvmzRoQtk  
UID: 8KaYUzNR5tFhZGvmzRoQtk

---

Create jira for prod operator documention of DCR  
https://workmemos.erauner.synology.me/m/X8aRNeDxZJyz53b62yiG78  
UID: X8aRNeDxZJyz53b62yiG78

---

Create a jira for identifying which clusters to onboard to DCR  
https://workmemos.erauner.synology.me/m/Fiz2oefD2XPmxaBekHYYyP  
UID: Fiz2oefD2XPmxaBekHYYyP

---

[Task Link](https://app.todoist.com/app/task/7976935630)

https://medallia.slack.com/archives/C065JNB1AE6/p1715202564906289?thread_ts=1715190487.449109&channel=C065JNB1AE6&message_ts=1715202564.906289

Link Context:  
Try out new groovy script that Sergio made for dcr  
https://workmemos.erauner.synology.me/m/iuses5D3hJXPkgyoMWhPsj  
UID: iuses5D3hJXPkgyoMWhPsj

---

https://medallia.slack.com/archives/G8YR81G49/p1715250842066669?thread_ts=1714691192.722409&channel=G8YR81G49&message_ts=1715250842.066669

Link Context:  
Check verifisure dcr is stalled  
https://workmemos.erauner.synology.me/m/TTgvuEbHfoudU7sXLpdVJ8  
UID: TTgvuEbHfoudU7sXLpdVJ8

---

Create a jira to turn the vars. Yaml into a json for storing state that is persisted in the workflow

Vars all in on place, no need to have a type for each one

Turns vars. Yaml into a json

Only store values in the vars. Json

It's time. So if you know the back-end is this type of object, and let me show you what you can do. Because this is a data class, right? OK. So and we set task. Is this a data class? No, it has none, right? What you can do is you can use asDict to serialize it, which will give you the equivalent of a dict, right? And then, OK, let's make it also, let's create it. So say, I don't know what the types were. No, str, str, list and dict, OK.

```
task = TaskOutput (taskType="foo", context="bar", a_list=(1,2,3], a_dict=("a":1, "b":2})
```

…

Vars. Yaml ex:

```
cat ~/work/prov-ng/api/dcr/systemdcrtest2/DcrStartFarm/TEST-1/vars.yaml.bak
!!python/object:prov_platform.dcr.DcrStartFarm.start_farm_abstract.StartFarmVars
_completed: false
_initialized: true
_params: !!python/object:prov_platform.dcr.DcrStartFarm.start_farm_abstract.StartFarmParams
  allow_params_override: true
  context: TEST-1
  dc: yul1
  dcr_custom_config_sha: 732988bcccb09e7239be984005758d62f89e9196
  dcr_custom_express_version: express-e687.176
  dcr_farm_memory_in_gb: 80
  dcr_force_cache_rebuild: true
  dcr_leader_frontend_node: fe1
  dcr_number_of_farms: 3
  disable_safety_checks: false
  instance: systemdcrtest2
  jira_assignee: null
  jira_issue: null
  jira_related_issues: []
  mark_completed_on_success: true
  shared_context: null
  step_modifiers: {}
  task_type: DcrStartFarm
_task_unique_id: OCUQ4VLF
clusterconfig_commit: !!python/object:lib.utils.Commit
  sha: null
  time_stamp: null
configuration_commit: !!python/object:lib.utils.Commit
  sha: null
  time_stamp: null
deployment_commit: !!python/object:lib.utils.Commit
  sha: null
  time_stamp: null
deployment_security_commit: !!python/object:lib.utils.Commit
  sha: null
  time_stamp: null
display_name: null
instance_id: 124294
instance_type: !!python/object/apply:prov_platform.mec.mec_abstract.MecInstanceType
- PROD
ip_subnets: []
puppet_commit: !!python/object:lib.utils.Commit
  sha: null
  time_stamp: null
tenant_id: null
topology: &id001 !!python/object:lib.express_topology.ExpressCluster
  backend: !!python/object:lib.express_topology.ExpressNode
    cluster: *id001
    config_changeset: 732988bcccb09e7239be984005758d62f89e9196
    dc: yul1
    dc_url: !!python/object/new:urllib.parse.ParseResult
    - https
    - systemdcrtest2-be.yul1.medallia.ca
    - ''
    - ''
    - ''
    - ''
    deployment_revision: null
    heap_memory_gib: 40
```

Instead of having to keep the python type in the yaml like this, we should turn it into a json and only provide values in the json.

Then specify how the values should be input into a generic type. Each class will take the input values from the json a little bit differently.  
https://workmemos.erauner.synology.me/m/dx5hEg6YmWD6VYnukfL2na  
UID: dx5hEg6YmWD6VYnukfL2na

---

How to tell when DCR is stalled from the workflow  
https://workmemos.erauner.synology.me/m/dAhDvfYMY8ZdZvqYURNkqN  
UID: dAhDvfYMY8ZdZvqYURNkqN

---

https://medallia.slack.com/archives/D05GGQQ13C3/p1715175515109019

Link Context:  
Send Jobin more/better DCR documentation  
https://workmemos.erauner.synology.me/m/jfpCT6mUfe5HVJgJP7ddBV  
UID: jfpCT6mUfe5HVJgJP7ddBV

---

https://jira.medallia.com/browse/SRECONFIG-2337

Fix this

```
2024-05-07 19:54:32,195 CRITICAL step step failed with exception: StartTag: invalid element name, line 2, column 2 (, line 2), cause: None, context: None, exiting.
```

https://sre-provisioning-api.eng.medallia.com/ui/log/automation-ake5pmumen4seti/dcr-farm-instance-configure

…

fails in `dcr-farm-instance-configure` step

When xml is checked  
https://workmemos.erauner.synology.me/m/hsNJum7BtCZUG2RmAKUnHG  
UID: hsNJum7BtCZUG2RmAKUnHG

---

https://medallia.slack.com/archives/D044XU1E0UE/p1715019429665999?thread_ts=1715018701.712189&cid=D044XU1E0UE

Start adding record count to each DCR instance  
https://workmemos.erauner.synology.me/m/NRmzzUjCippyukr45cuabr  
UID: NRmzzUjCippyukr45cuabr

---

[Task Link](https://app.todoist.com/app/task/7970082102)

All the dcr tenants you cannot find cache size for  
https://workmemos.erauner.synology.me/m/XdeBeB9S6xi74rz6FeqCkW  
UID: XdeBeB9S6xi74rz6FeqCkW

---

[Task Link](https://app.todoist.com/app/task/7970080358)

Restart bbva as dynamic:

```
kubectl exec -it bbva-dcr-0 -n tenant-101055-prod -c express --context=lon -- /bin/bash
...
bash-4.4$ cat /express/workdir/dcr.properties

bash-4.4$ cat /express/workdir/slug.properties
compiler.query.cache.size = 5000
slug.cache.format = CANONICAL
corruption-detector.start-on-boot = false
pump.batchsize = 5000
```

---

https://medallia.slack.com/archives/C070UU168N4/p1714673609003579?thread_ts=1714576599.485679&cid=C070UU168N4  
https://workmemos.erauner.synology.me/m/Tzdd7TK7rFhxFJdotPmYou  
UID: Tzdd7TK7rFhxFJdotPmYou

---

[Task Link](https://app.todoist.com/app/task/7970060224)

Make another DCR add tenant PR for

Generali  
Bunnings  
Woolworthsgroup  
https://workmemos.erauner.synology.me/m/4RFV8NvXkgwN8UmmUXc2i7  
UID: 4RFV8NvXkgwN8UmmUXc2i7

---

[Task Link](https://app.todoist.com/app/task/7966664817)

https://medallia.slack.com/archives/C0140KBUPGC/p1714845793302099?thread_ts=1714214663.018649&channel=C0140KBUPGC&message_ts=1714845793.302099

Link Context:  
Start bbva dcr up again  
https://workmemos.erauner.synology.me/m/GozcvonpAXx4PX3S2FuKpC  
UID: GozcvonpAXx4PX3S2FuKpC

---

This is a bug that needs to be fixed in prov ng code

DCR needs to be able to hit the `https://` in order to do an ssl client

```
2024-04-25 19:48:31,070 DEBUG misc got exception step 'StepDCRFarmWaitForCompletion' failed in '_fetch_dcr_process_status' with: Express admin
 error: failed to run admin command '{'cmd': 'processStatus', 'format': 'JSON'}' on http://127.0.0.1:42691/.admin with: ('Connection aborted.'
, RemoteDisconnected('Remote end closed connection without response')), sleeping for retry
```

https://workmemos.erauner.synology.me/m/jAYL4RaoHZf6f5JCd3nR3c  
UID: jAYL4RaoHZf6f5JCd3nR3c

---

[Task Link](https://app.todoist.com/app/task/7966656542)

Create an epic for DCR in this quarter  
https://workmemos.erauner.synology.me/m/A3M8QmBmvBMAjJ6kmi9J66  
UID: A3M8QmBmvBMAjJ6kmi9J66

---

Decomm the following DCR clusters

```
lumanity
zurichspain
kiva
digitalglobe
```

https://workmemos.erauner.synology.me/m/UNqQNnNKHkxVU2kGHgxP8L  
UID: UNqQNnNKHkxVU2kGHgxP8L

---

Should we be provision dcr for clusters based on how long they were taking or how much cache size they have:

```
Updated add_cache_size/deployment/apps/dcr/overlays/yul1/systemdcrtest2/values.yaml with cache size: 4
Updated add_cache_size/deployment/apps/dcr/overlays/fra1/telefonica/values.yaml with cache size: 55
Updated add_cache_size/deployment/apps/dcr/overlays/fra1/telekom/values.yaml with cache size: 30
Updated add_cache_size/deployment/apps/dcr/overlays/fra1/deliveroo/values.yaml with cache size: 105
lumanity: Tenant not found
Cache size not found for lumanity
zurichspain: Tenant not found
Cache size not found for zurichspain
Updated add_cache_size/deployment/apps/dcr/overlays/fra1/entain/values.yaml with cache size: 68
Updated add_cache_size/deployment/apps/dcr/overlays/lon/vf/values.yaml with cache size: 193
Updated add_cache_size/deployment/apps/dcr/overlays/lon/bbva/values.yaml with cache size: 232
Updated add_cache_size/deployment/apps/dcr/overlays/lon/veon/values.yaml with cache size: 187
Updated add_cache_size/deployment/apps/dcr/overlays/lon/libertyglobal/values.yaml with cache size: 143
```

Vs

```
      (
        max by (cluster) (
          (
            express_cache_rebuild_parsed_surveys_total_count{application="express", env="production"}
            / express_cache_rebuild_expected_surveys_total_count{application="express", env="production"}
          ) * 100
        ) < 100
      )
      and
      (
        max by (cluster) (
          sum by (cluster) (
            express_cache_rebuild_duration_total_ms{application="express", nodeName=~"[^-]+-fe\\d+"}
          ) / (24 * 60 * 60 * 1000)
        ) >= 3
      )
```

Last 30d  
https://github.medallia.com/medallia/giraffe-alerts/pull/3219

…

Determine this is who I think I'm going to go ahead and add ahead of time, if they do not already exist that is:  
https://workmemos.erauner.synology.me/m/LeQPv8ripFQvEpACns3YAU  
UID: LeQPv8ripFQvEpACns3YAU

---

Make a script to find the cache size for each cluster in DCR currently

Need to find the script that Sergio made for you to do this

---

calculating ephemeral for `cacheSize`

```
import com.amazonaws.services.s3.model.ObjectListing
import express.bl.cluster.topology.ExpressTopologyManager
import express.bl.datamodel.company.Company
import express.db.hose.NodeInitializationContext
import express.db.hose.S3CacheStoreUtils

def nodeToInspect = ''
def numFarmers = 20

def sw = new StringWriter()
def pw = new PrintWriter(sw)
try {
    if (!nodeToInspect) {
        nodeToInspect = ExpressTopologyManager.getInstance().getBaselinePlusRegisteredClusterConfig().getAllPumpTargets().first()
    }

    def s3Properties = S3CacheStoreUtils.getS3ClientConfig()
    def s3Bucket = S3CacheStoreUtils.getS3Bucket(s3Properties)
    def s3 = S3CacheStoreUtils.createS3Client(s3Properties)
    def GB = 1 << 30

    pw.println('<table><tr><th>Node</th><th>Company</th><th>Cache Size</th><th>Farmers</th><th>Farmers Ephemeral</th></tr>')
    for (def company : Company.getPumpedCompanies()) {
        def s3Folder = S3CacheStoreUtils.getCacheTypeKeyPrefixForNode(nodeToInspect, company, NodeInitializationContext.NODE_DIR_NAME)
        def s3Prefix = S3CacheStoreUtils.getCurrentFolderAbsolute(s3Folder, s3, s3Bucket).get().toPrefix()

        def accumulate = {objects, totalSize ->
            objects.objectSummaries.findAll({ it.key.endsWith('.cache') }).inject(totalSize, { ts, file -> ts + file.size })
        }
        ObjectListing objects = s3.listObjects(s3Bucket, s3Prefix)
        long totalSize = accumulate(objects, 0)
        while (objects.truncated) {
            objects = s3.listNextBatchOfObjects(objects)
            totalSize = accumulate(objects, totalSize)
        }

        pw
                .append('<tr>')
                .append('<td>').append(nodeToInspect).append('</td>')
                .append('<td>').append(company.getURLname()).append('</td>')
        pw.printf('<td>%.2f</td>', totalSize / GB)
        pw.printf('<td>%s</td>', numFarmers)
        pw.printf('<td>%.2f</td>', totalSize * 2 / numFarmers / GB)
        pw        .append('</tr>\n')

    }
    pw.append('</table>\n')
} catch (e) {
    pw.append("<br><br><br>All Broke: $e<br><pre>" + com.google.common.base.Throwables.getStackTraceAsString(e) + '</pre>')
}
sw
```

https://workmemos.erauner.synology.me/m/c3GBhkqznTtx8hcQ2A5U53  
UID: c3GBhkqznTtx8hcQ2A5U53

---

Where is local static configred for dcr?

https://medallia.slack.com/archives/C070UU168N4/p1714676824744489?thread_ts=1714576599.485679&cid=C070UU168N4  
https://workmemos.erauner.synology.me/m/bVQgnBeaiYDpxvNwqPjJvg  
UID: bVQgnBeaiYDpxvNwqPjJvg

---

But I think  
`slug.properties` older versions

`dcr.properties` newer versions

…

So really we just need to find out what version is where it changes.  
https://workmemos.erauner.synology.me/m/WS2tV9B4At7neC9cBMyS8H  
UID: WS2tV9B4At7neC9cBMyS8H

---

Check why this jira did not fail when this cluster restarted DCR:

https://medallia.slack.com/archives/C070UU168N4/p1714671348834529?thread_ts=1714576599.485679&cid=C070UU168N4

```
deserialize

    FAILED [started: 4.21 h ago] [elapsed: 1s 33ms]

    java.lang.RuntimeException: Could not deserialize dataset
```

https://workmemos.erauner.synology.me/m/CiztEDQ3Y9NDS282rnyJR8  
UID: CiztEDQ3Y9NDS282rnyJR8

---

Should add more static IPs or a range to dcr so we are not constrained  
https://workmemos.erauner.synology.me/m/Q3iGD8ywfikJ39NnEWHhM4  
UID: Q3iGD8ywfikJ39NnEWHhM4

---

Yeah I'd prefer to take it out honestly, it's an unnecessary burden. The only tenants we want doing DCR should be in the repo and if we don't they should be taken out.  
The only changes we should be making in the workflow:

- Argo application files (helm chart updates)
- Values files (update with latest cacheSize etc)  
     https://workmemos.erauner.synology.me/m/BY6JCr9NcZvtQqvBBFKNhv  
     UID: BY6JCr9NcZvtQqvBBFKNhv

---

To avoid unnecessary troubleshooting mid deployment on net new DCRs. Will prepare for next deployment by taking the data from the query over the past 30 days and staging  
https://workmemos.erauner.synology.me/m/C8M9sRjNVQR8TPRfqfDzB2  
UID: C8M9sRjNVQR8TPRfqfDzB2

---

Should we have a maximum for ephemeral storage?

This is for dcr

Ex:

```
nodeCount: 1
cacheSize: 75
```

```
        resources:
          limits:
            memory: 116736Mi
          requests:
            cpu: 2
            ephemeral-storage: 150Gi
            memory: 116736Mi
```

https://workmemos.erauner.synology.me/m/5jM4XNrd3GTbuNFvcsbA4L  
UID: 5jM4XNrd3GTbuNFvcsbA4L

---

Onboard the following clusters to DCR:

https://medallia.slack.com/archives/C022XV98JH4/p1714590819850379?thread_ts=1714162456.374759&cid=C022XV98JH4

Fra1

```
bankintercx
makeyourmands
```

Can

```
tdbank
```

https://workmemos.erauner.synology.me/m/7csKt33BhfcuyKpcS4M36z  
UID: 7csKt33BhfcuyKpcS4M36z

---

For starting the script to copy helm chart make it easier

```
mkdir -p apps/dcr/helm_0.0.5
cp -R apps/dcr/helm_0.0.2/ apps/dcr/helm_0.0.4/
git commit -m "copy helm_0.0.4 to helm_0.0.5"
```

https://workmemos.erauner.synology.me/m/Mu5uyJHrtfJgmtt7kgTviW  
UID: Mu5uyJHrtfJgmtt7kgTviW

---

Fix DCR template failing to take down node

Cannot go to zero nodes properly, need to fix it.

```
Error: template: dcr/templates/nodes/dcr.yaml:25:30: executing "dcr/templates/nodes/dcr.yaml" at <div (mul $.Values.cacheSize 2) $nodeCount>: error calling div: runtime error: integer divide by zero Use --debug flag to render out invalid YAML
```

https://argocd.fra1.medallia.eu/applications/dcr-entain?conditions=true&resource=  
https://workmemos.erauner.synology.me/m/kmjEb5tQgFp4GHmzBJ6eTd  
UID: kmjEb5tQgFp4GHmzBJ6eTd

---

Please test the new delivery image on a DCR cluster and check if it's giving metrics. Note that `yul1` will fail due to the cmdCenter issue.

If there's another cluster available, feel free to use it.

Here are the links:  
[Slack Thread](https://medallia.slack.com/archives/C05LNMF72QY/p1714439844229509?thread_ts=1714439831.769889&cid=C05)  
https://workmemos.erauner.synology.me/m/AozAZuXJfL6UeF7H7MdnRZ  
UID: AozAZuXJfL6UeF7H7MdnRZ

---

Should DCR farmers be cleaned up when ICR begins

---

https://medallia.slack.com/archives/C065JNB1AE6/p1714502182101349?thread_ts=1714502169.533949&cid=C065JNB1AE6  
https://workmemos.erauner.synology.me/m/SZTTUPQukJ4Trg9aBU2swW  
UID: SZTTUPQukJ4Trg9aBU2swW

---

https://medallia.slack.com/archives/C8XCPL3UN/p1714473310134999?thread_ts=1714472612.351199&channel=C8XCPL3UN&message_ts=1714473310.134999

Link Context:  
Start a dcr for this instance  
https://workmemos.erauner.synology.me/m/SsXw5PB4meqHFhafXKrfnf  
UID: SsXw5PB4meqHFhafXKrfnf

---

Need to get this fixed in `init` step of DCR:

```python
        msg = f"Target version {target_version} is "
        # store the newer version between the leader_version and the target_version
        if get_newest_express_version(self.leader_version, target_version) != target_version:
            msg += (
                f"lower than Leader version {self.leader_version}. "
                f"Please update the `dcr_custom_express_version` parameter to {self.leader_version} or higher."
            )
            raise StepInitException(msg)
```

https://workmemos.erauner.synology.me/m/LMHuHfvKiHEhcNPrBiX7Ac  
UID: LMHuHfvKiHEhcNPrBiX7Ac

---

Get DCR on latest image

---

https://github.medallia.com/medallia/configuration/blob/sre-provisioning-api-server-delivery/templates/sre-provisioning-api-server/var/provision/api/config/DcrStartFarm.yaml

It is currently on

```
202402061227-SREPROVNG-410-792e6e2
```

https://jira.medallia.com/browse/SREPROVNG-410

…

So pretty much what you need to do is get this on the delivery image.

…

so go to the delivery branch and run a test against ` systemdcrtest2` in `yul1`  
https://workmemos.erauner.synology.me/m/9ekPPWdiW4QMHw3KgDKfny  
UID: 9ekPPWdiW4QMHw3KgDKfny

---

For DCR

We need to decide how we are going to handle getting

Option 1: make it all dynamic in the code

Make the configuration PRs

https://medallia.slack.com/archives/D044XU1E0UE/p1714399553729529?thread_ts=1714221447.406029&cid=D044XU1E0UE

…

Should we run a groovy script across all instances to see what instances are not dynamic already?  
https://workmemos.erauner.synology.me/m/4B8CvRfwp5ZsYzkrZai6tp  
UID: 4B8CvRfwp5ZsYzkrZai6tp

---

Start making a DCR confluence page with all of the things you are finding that you need to do.

Present this to Murtaza tomorrow.  
https://workmemos.erauner.synology.me/m/LaVErG4TPENHjSudWdqLtu  
UID: LaVErG4TPENHjSudWdqLtu

---

Network policy needs to be fixed, this is how

https://medallia.slack.com/archives/C02LBCF9YE8/p1713967696698979?thread_ts=1713964419.607089&cid=C02LBCF9YE8

This was the fix

```yaml
Spec:
  Ingress:
  - From:
    - PodSelector:
        MatchLabels:
          Express. Medallia. Com/cluster-name: entain
          Express. Medallia. Com/instance-type: dcr
  PodSelector:
    MatchLabels:
      Express. Medallia. Com/cluster-name: entain
  PolicyTypes:
  - Ingress
```

…

https://github.medallia.com/Atlas/cloud-deployment/blob/master/apps/express/helm_2.0.2/templates/network_policies/intra-cluster.yaml

To here:  
https://github.medallia.com/Atlas/deployment/tree/master/apps/express/helm_0.0.3/templates/network_policies

https://workmemos.erauner.synology.me/m/db7W6rvWrPt9kx3dJNoGxR  
UID: db7W6rvWrPt9kx3dJNoGxR

---

Report when dcr has successfully started

Or when it gets stuck to slack

---

Test

---

CreateAppendFromClipboardButton

---

Test  
https://workmemos.erauner.synology.me/m/H7WrSiB2Jn5b5ft55D3sAG  
UID: H7WrSiB2Jn5b5ft55D3sAG

---

Make a script to perform updates to all DCR instance in deployment repo  
https://workmemos.erauner.synology.me/m/Xy9FWNZ2fZWrzzFgKsE7Qu  
UID: Xy9FWNZ2fZWrzzFgKsE7Qu

---

DCR should be able to support if the FE node is already down before starting:

Ex:

```
./pods bbva
NAME                                        READY   STATUS      RESTARTS   AGE     IP               NODE          NOMINATED NODE   READINESS GATES
bbva-be                                     2/2     Running     0          56d     10.149.9.1       lon-r12-u19   <none>           <none>
bbva-fe2                                    2/2     Running     0          4h50m   10.149.9.3       lon-r10-u15   <none>           <none>
bbva-fe3                                    2/2     Running     0          59d     10.149.9.4       lon-r13-u28   <none>           <none>
bbva-fe4                                    2/2     Running     0          53d     10.149.9.5       lon-r12-u30   <none>           <none>
```

Context:

https://medallia.slack.com/archives/G8YR81G49/p1714161571079929?thread_ts=1712353359.430909&cid=G8YR81G49

Asking about it here:

https://medallia.slack.com/archives/D070GNHN1HD/p1714162404455929

Test  
https://workmemos.erauner.synology.me/m/VZMmAt2Sc5R5vhHx3GVbBc  
UID: VZMmAt2Sc5R5vhHx3GVbBc

---

What I am trying to get kamil to help with is make the SRE API the connection between a single slack thread and a single jira for all updates for a single DCR instance  
https://workmemos.erauner.synology.me/m/5rqZouf8tr4kapMZroUcYp  
UID: 5rqZouf8tr4kapMZroUcYp

---

Going to create a jira to make sure at the beginning of DCR, the latest is always chosen to avoid issues like we ran into with ephem. No need for these PRs

Ok  
https://workmemos.erauner.synology.me/m/XCtv9xEPRwgApbfvcwtLNB  
UID: XCtv9xEPRwgApbfvcwtLNB

---

Need to create some documentation on how to troubleshoot DCR

The main audience will be prod operators  
https://workmemos.erauner.synology.me/m/LEQpxAs9PCKCariXEzcyfg  
UID: LEQpxAs9PCKCariXEzcyfg

---

K8s_scripts that opens up every relevant browser tab related to DCR:

BE  
FE Cache rebuild Status  
FE Prrocess Status  
https://workmemos.erauner.synology.me/m/G6SYjEVxxcmiH4HeUPb73N  
UID: G6SYjEVxxcmiH4HeUPb73N

---

Dcr bug

Fix this:

```
2024-04-24 18:24:41,952 DEBUG misc got exception step '?' failed in '_wait_until_ready_replicas_reach_desired' with: STS wfc-dcr does not have
 1 ready replicas yet. Current number of ready replicas: 20. , sleeping for retry
```

It should know if there are more, than it should go forward.  
https://workmemos.erauner.synology.me/m/hKfgAEi2GCT8V8GM5zBRqY  
UID: hKfgAEi2GCT8V8GM5zBRqY

---

Report DCR progress to slack  
https://workmemos.erauner.synology.me/m/fs434LdCS8oiTykvJqsDoG  
UID: fs434LdCS8oiTykvJqsDoG

---

ask @kamil to make DCR slack updates go to the #eng -deployment channel.  
https://workmemos.erauner.synology.me/m/B44r3JC6jcwgW6adnuhRoL  
UID: B44r3JC6jcwgW6adnuhRoL

---

Is it possible to configure a statefulset this way in argocd?

Is there a way to set a max for STS replicas instead of a specific number

So that we can set that and adjust replicas via kubectl instead without changing what is in git.

We would provide something like a maximum and a minimum number of replicas in the helm chart instead of a specific amount of replicas

```
Kubectl scale statefulset wfc-dcr --context sea1 -n tenant-101470-prod --replicas=0
```

```
Kubectl scale statefulset wfc-dcr --context sea1 -n tenant-101470-prod --replicas=20
```

```

```

Kubectl scale statefulset wfc-dcr --context sea1 -n tenant-101470-prod --replicas=40

```

```

Kubectl scale statefulset wfc-dcr --context sea1 -n tenant-101470-prod --replicas=30

```
https://workmemos.erauner.synology.me/m/ZX3XjQeHGcGisAT9TCaXAG
UID: ZX3XjQeHGcGisAT9TCaXAG

----------------------------------------
Start workplan for updating helm chart:

```

Cp -R apps/dcr/helm_0.0.2/ apps/dcr/helm_0.0.4/

```


https://workmemos.erauner.synology.me/m/2YigiNoAHfL4X9YZ7v3WJp
UID: 2YigiNoAHfL4X9YZ7v3WJp

----------------------------------------
## Restarting DCR Rebuild with New Configuration on FE Node

### Step 1: Cancel the Current Rebuild
- Cancel the rebuild process in the admin panel.
- Use the command:
```

./admin entain-fe1

```

### Step 2: Force a Cache Rebuild on the Next Boot
- Force the cache rebuild to happen on the next FE node restart.
- In the admin console, use the following commands to stop the FE node:
```

Ssh ssh1-fra1  
ssh -A -p2222 medallia@fra1-prod-dep01  
Cd deployer  
Opts='-Dclusters=entain' ./cli

```

### Step 3: Apply the New Configuration and Restart FE Node
- Remove the FE node using the command:
```

Remove entain-fe1

```
- Bring the updated farmers up with ephemeral data.
- Use the following command to restart FE:
```

Kubectl scale statefulset entain-dcr --context fra1 -n tenant-102868-prod --replicas=0

```

### Step 4: Helm Template Deployment
- Apply the new configuration using the following command:
```

Helm template foo apps/dcr/helm_0.0.2/ --values apps/dcr/overlays/fra1/entain/values. Yaml | kubectl apply -f - --context fra1

```

### Step 5: Monitor Node Status
- Monitor the termination and startup status of the nodes:
```

Entain-dcr-0 1/2 Terminating 0 30h  
Entain-dcr-1 1/2 Terminating 0 30h  
…

```

- Check for DCR farm nodes in the admin panel:
```

./admin entain-be

```

### Step 6: Verify Cache Rebuild
- Once the nodes are rebooting, ensure that the FE node status is `BOOTING`:
```

A/A Status:  
Classic Frontends  
Entain-fe1 : BOOTING invisible not sync e689.121 waiting Inactive

```
- Navigate to the synchronizer to check if it has started:
- URL: http://localhost:9100/.admin
- Command: `Cache Rebuild Status` -> `Velocity` or `JSON`

- Confirm the rebuild status with the following JSON response:
```

{  
"snapshotsByNode": [{  
"nodeId": "entain-fe1",  
"directUrl": " http://entain-fe1.tenant-102868-prod.svc.k8s.fra1.medallia.eu:9100/.admin" ,  
…  
}],  
"processSnapshots": [{  
…  
}],  
"completed": false  
}

```

### Step 7: Final Check
- Ensure the FE node is running:
```

./pods entain

```

- Check for any additional logs or errors that might indicate problems during the reboot process.
https://workmemos.erauner.synology.me/m/gTtv6Y7FA8doVpDdyxPYft
UID: gTtv6Y7FA8doVpDdyxPYft

----------------------------------------
With entain for example. I made major changes to the chart and before I leveled the whole farm with:
Kubectl scale statefulset entain-dcr --context fra1 -n tenant-102868-prod --replicas=0
What happens when you make a change to the chart in the middle of a rebuild is this:
```

Entain-be 2/2 Running 0 20d  
Entain-dcr-0 2/2 Running 0 169m  
Entain-dcr-1 2/2 Running 0 169m  
Entain-dcr-2 2/2 Running 0 169m  
Entain-dcr-3 2/2 Running 0 169m  
Entain-dcr-4 2/2 Running 0 170m  
Entain-dcr-5 2/2 Running 0 169m  
Entain-dcr-6 2/2 Running 0 169m  
Entain-dcr-7 2/2 Running 0 169m  
Entain-dcr-8 2/2 Running 0 169m  
Entain-dcr-9 2/2 Terminating 0 169m

```
It's just going to start with one at a time, swapping out each node with the new config
This is why we were not able to make the ephemeral changes in the middle
But I'm going to add a new step in the workflow to basically check what the helm chart version is, and if it is not on the latest, before bringing the farm up. Change to the latest. So we we can mitigate this situation
https://workmemos.erauner.synology.me/m/FfJLSA4yaFcVJFFmh4RJhb
UID: FfJLSA4yaFcVJFFmh4RJhb

----------------------------------------
Work on python change to always use the latest configuration for DCR at the beginning of every run
https://workmemos.erauner.synology.me/m/3xXS7GDUQoF7SEpCoFCepG
UID: 3xXS7GDUQoF7SEpCoFCepG

----------------------------------------
When you make a change to the chart:

```

Entain-dcr-0 2/2 Running 0 169m  
Entain-dcr-1 2/2 Running 0 169m  
Entain-dcr-2 2/2 Running 0 169m  
Entain-dcr-3 2/2 Running 0 169m  
Entain-dcr-4 2/2 Running 0 170m  
Entain-dcr-5 2/2 Running 0 169m  
Entain-dcr-6 2/2 Running 0 169m  
Entain-dcr-7 2/2 Running 0 169m  
Entain-dcr-8 2/2 Running 0 169m  
Entain-dcr-9 2/2 Terminating 0 169m

```
https://workmemos.erauner.synology.me/m/Lq8fwhC82eG5aftbgf2qPE
UID: Lq8fwhC82eG5aftbgf2qPE

----------------------------------------
Use this as a reason to explain why you need a better DCR process:

https://github.medallia.com/Atlas/deployment/pull/62784/commits
https://workmemos.erauner.synology.me/m/Kysx6EZq6uff3qTjuZfMbu
UID: Kysx6EZq6uff3qTjuZfMbu

----------------------------------------
```

❯ helm template foo apps/dcr/helm_0.0.4/ --values apps/dcr/overlays/sea1/wfc/values. Yaml | grep ephemeral  
Ephemeral-storage: 12Gi

```

```

❯ helm template foo apps/dcr/helm_0.0.4/ --values  
Ephemeral-storage: 15Gi

```

```

❯ helm template foo apps/dcr/helm_0.0.4/ --values apps/dcr/overlays/fra1/entain/values. Yaml | grep ephemeral  
Ephemeral-storage: 10Gi

```
https://workmemos.erauner.synology.me/m/BjKwVvndPnVUnKRxT9CM7S
UID: BjKwVvndPnVUnKRxT9CM7S

----------------------------------------
Restarting DCR Rebuild with new configuration on FE node

- Cancel the rebuild
- Force rebuild in admin
- Stop the FE node to apply new configuration
- Bring up farmers with updated ephemeral
- Bring FE node back up to start Dynamic

https://medallia.slack.com/archives/C070UU168N4/p1714067358840739?thread_ts=1714040066.191409&cid=C070UU168N4



```

./admin entain-fe1

```

`Cancel Cache Rebuild`

`Force Cache Rebuild` on next boot

...


Go on deployer host to stop the FE node
```

Ssh ssh1-sea1

```

```

ssh -A -p2222 medallia@sea1-prod-dep01

```

```

Cd deployer  
Opts='-Dclusters=wfc' ./cli

```

...

```

Remove entain-fe1

```

```

entain-fe1 2/2 Terminating 0 23h 10.42.25.129 fra1-r17-u16 <none> <none>

```

```

Kubectl scale statefulset entain-dcr --context fra1 -n tenant-102868-prod --replicas=0

```

```

Helm template foo apps/dcr/helm_0.0.2/ --values apps/dcr/overlays/sea1/wfc/values. Yaml | kubectl apply -f - --context sea1

```

```

Statefulset. Apps/entain-dcr scaled  
entain-dcr-0 1/2 Terminating 0 30h 10.42.208.157 fra1-r16-u05 <none> <none>  
entain-dcr-1 1/2 Terminating 0 30h 10.42.208.167 fra1-r15-u17 <none> <none>  
entain-dcr-2 1/2 Terminating 0 30h 10.42.209.37 fra1-r11-u05 <none> <none>  
entain-dcr-3 1/2 Terminating 0 30h 10.42.204.223 fra1-r10-u05 <none> <none>  
entain-dcr-4 1/2 Terminating 0 30h 10.42.209.38 fra1-r12-u14 <none> <none>  
entain-dcr-5 1/2 Terminating 0 30h 10.42.204.129 fra1-r17-u24 <none> <none>  
entain-dcr-6 2/2 Terminating 0 30h 10.42.208.150 fra1-r15-u21 <none> <none>  
entain-dcr-7 1/2 Terminating 0 30h 10.42.208.163 fra1-r17-u10 <none> <none>  
entain-dcr-8 2/2 Terminating 0 30h 10.42.208.162 fra1-r13-u16 <none> <none>  
entain-dcr-9 1/2 Terminating 0 30h 10.42.208.159 fra1-r14-u18 <none> <none>

```




```

        DCR Farm nodes:
            Entain-dcr-0                       : ?               ?          ?        E689.121 hidden          Inactive
            Entain-dcr-1                       : ?               ?          ?        E689.121 hidden          Inactive
            Entain-dcr-2                       : ?               ?          ?        E689.121 hidden          Inactive
            Entain-dcr-3                       : ?               ?          ?        E689.121 hidden          Inactive
            Entain-dcr-4                       : ?               ?          ?        E689.121 hidden          Inactive
            Entain-dcr-5                       : ?               ?          ?        E689.121 hidden          Inactive
            Entain-dcr-6                       : ?               ?          ?        E689.121 hidden          Inactive
            Entain-dcr-7                       : ?               ?          ?        E689.121 hidden          Inactive
            Entain-dcr-8                       : ?               ?          ?        E689.121 hidden          Inactive
            Entain-dcr-9                       : ?               ?          ?        E689.121 hidden          Inactive

```

Wait for nodes to come up in BE admin:

```

./admin wfc-be

```

Looking for `hidden`

```

        DCR Farm nodes:
            Wfc-dcr-0                          : ?               ?          ?        ??? Hidden          Inactive
            Wfc-dcr-3                          : ?               ?          ?         Unreachable     org. Apache. Thrift. Transport. TTransportException
            Wfc-dcr-4                          : ?               ?          ?         Unreachable     org. Apache. Thrift. Transport. TTransportException
            Wfc-dcr-5                          : ?               ?          ?        ??? Hidden          Inactive
            Wfc-dcr-6                          : ?               ?          ?         Unreachable     Inactive

…

```

```

Dep-simple-node-config entain-fe1 express-e689.121 31779e25a529ec39caa9d83252b92a3c48fbb41c  
Job a3545d56-b4d3-47b5-a783-bd6ad9f9e352 started.

```

```

./pods entain

```

Check if running
```

wfc-fe1 2/2 Running 0 27s 10.53.36.2 sea1-r10-u21 <none> <none>

```


```

./admin entain-be

```
- Check admin to wait for `BOOTING`

```

A/A Status:  
Classic Frontends  
Entain-fe1 : BOOTING invisible not sync e689.121 waiting Inactive

```

Once it is in `BOOTING`, Check the synchronizer to make sure it has started:
```

./admin entain-fe1

```

Navigate to: http://localhost:9100/.admin

Command: `Cache Rebuild Status` -> `Velocity` or `JSON`

...

```

{  
"snapshotsByNode": [{  
"nodeId": "entain-fe1",  
"directUrl": " http://entain-fe1.tenant-102868-prod.svc.k8s.fra1.medallia.eu:9100/.admin" ,  
"company": "entain",  
"cacheRebuildStrategy": {  
"type": "REBUILD_TO",  
"stampFrom": 0,  
"stampTo": 2101426468132,  
"batchIdTo": "180502-7e75c793-entain",  
"setType": true,  
"setStampFrom": false,  
"setStampTo": true,  
"setBatchIdTo": true,  
"setProcessId": false  
},  
"totalExpectedRecords": 179900155,  
"currentRecords": 2270000,  
"currentDuration": "2.63 m",  
"batchSize": 10000,  
"currentBatches": 227,  
"totalExpectedBatches": 17991,  
"rebuildProgressPercentage": 1.261811,  
"rebuildETA": "3.45 h"  
}],  
"processSnapshots": [{  
"enrollerId": "DCRFarm:entain: entain-fe1:entain: 7cc396fa-93c8-4b39-a1b2-b8f5f6bce978",  
"companyURLName": "entain",  
"startTime": "4/25/24 5:41 PM",  
"elapsedTime": "7.83 m",  
"unitsPending": 2500,  
"unitsProcessing": 10,  
"unitsProcessed": 21,  
"totalUnits": 2531,  
"avgProcessingTime": "3.02 m",  
"minProcessingTime": "2.03 m",  
"maxProcessingTime": "4.23 m",  
"redoCount": 0,  
"nodeId": "entain-fe1",  
"creatingWorkUnits": true,  
"percentageDone": 0.82971156,  
"percentageLeft": 99.17029,  
"lostToRedoTime": "0 s",  
"nodesInvolved": ["entain-dcr-3", "entain-dcr-2", "entain-dcr-1", "entain-dcr-0", "entain-dcr-7", "entain-dcr-6", "entain-dcr-5", "entain-dcr-4", "entain-dcr-9", "entain-dcr-8"],  
"sortedNodesSnapshots": [{  
"nodeId": "entain-dcr-0",  
"totalProcessingTime": "6.47 m",  
"percentageOfProcessed": 14.285715,  
"directNodeURL": " http://entain-dcr-0.entain-dcr:9100/.admin" ,  
"avgProcessingTime": "2.27 m",  
"unitsProcessed": 3,  
"minProcessingTime": "2.03 m",  
"maxProcessingTime": "2.27 m",  
"processing": true  
…  
"completed": false  
}

```
https://workmemos.erauner.synology.me/m/FaLhVh6hSPLcBnFjaqFkRg
UID: FaLhVh6hSPLcBnFjaqFkRg

----------------------------------------
Figure out why farmers do not start back up when they come back up:

https://medallia.slack.com/archives/C070UU168N4/p1714139800458269?thread_ts=1714040066.191409&cid=C070UU168N4


```

        DCR Farm nodes:
            Entain-dcr-0                       : ?               ?          ?         Unreachable     org. Apache. Thrift. Transport. TTransportException
            Entain-dcr-1                       : ?               ?          ?         Unreachable     org. Apache. Thrift. Transport. TTransportException
            Entain-dcr-2                       : ?               ?          ?         Unreachable     org. Apache. Thrift. Transport. TTransportException
            Entain-dcr-3                       : ?               ?          ?         Unreachable     org. Apache. Thrift. Transport. TTransportException
            Entain-dcr-4                       : ?               ?          ?        E689.121 error           Inactive
            Entain-dcr-5                       : ?               ?          ?         Unreachable     org. Apache. Thrift. Transport. TTransportException
            Entain-dcr-6                       : ?               ?          ?         Unreachable     org. Apache. Thrift. Transport. TTransportException
            Entain-dcr-7                       : ?               ?          ?         Unreachable     org. Apache. Thrift. Transport. TTransportException
            Entain-dcr-8                       : ?               ?          ?         Unreachable     org. Apache. Thrift. Transport. TTransportException
            Entain-dcr-9                       : ?               ?          ?        E689.121 error           Inactive

```



I'm really curious why admin continues to recognize the nodes as:

```

            Entain-dcr-1                       : ?               ?          ?         Unreachable     org. Apache. Thrift. Transport. TTransportException

```

Despite the pod coming back up?

```

entain-dcr-1 1/2 Running 0 21h 10.42.208.157 fra1-r15-u21 <none> <none>

```

Is there anything we could check on that before restarting?

---

Because I would think that if the heap lost control, the container would restart and get back to work


...


```

Error: express. Db. Hose. Cacherebuild. Distributed. DistributedCacheRebuilder$MinimumNodesAvailableNotReachedException: Not enough workers to enroll, at least 2 requested with 1 available

```
https://workmemos.erauner.synology.me/m/LaPqwUSVpubMabFhxZs8xH
UID: LaPqwUSVpubMabFhxZs8xH

----------------------------------------
Bug that needs to be fixed in prov ng code


Get datacenter working on DCR

Datacenter:
Automatic


Look at adjust heap
https://workmemos.erauner.synology.me/m/jU7dqpmUxdapXxmVCGhS87
UID: jU7dqpmUxdapXxmVCGhS87

----------------------------------------
Put this is prod operator DCR documentation

Deliveroo doing ICR

ICR basically just means it's done and you are waiting for the node to become `SYNCHRONIZED` again

```

2024-04-25 19:49:17,264 DEBUG misc got exception step 'StepDCRFarmWaitForCompletion' failed in '\_respond_to_dcr_state' with: DCR has completed  
, waiting for status to reflect from BE node., sleeping for retry

```
https://workmemos.erauner.synology.me/m/iGXPUqYcfMc8L5RsoCuP4f
UID: iGXPUqYcfMc8L5RsoCuP4f

----------------------------------------
Dcr improvement

Need to be able to catch when in this state for too long, so that DCR does not just keep waiting for nothing

```

2024-04-24 18:24:41,657 DEBUG misc got exception step 'StepDCRFarmWaitForCompletion' failed in '\_respond_to_dcr_state' with: DCR still waiting  
To start rebuilding the caches., sleeping for retry

```

- `DCR still waiting
 To start rebuilding the caches`


...

What we need to do is basically give the trigger 2-3 more tries instead of 1. And only exit early if we get `rebuilding the caches` for x amount of time
https://workmemos.erauner.synology.me/m/4kvf3YdGAowHcytXLrdqJk
UID: 4kvf3YdGAowHcytXLrdqJk

----------------------------------------
https://jira.medallia.com/browse/SREPROVNG-283

---

Need to figure out why TLS clients do not run DCR

https://medallia.slack.com/archives/C022XV98JH4/p1713970169435189?thread_ts=1713903809.311749&cid=C022XV98JH4

https://medallia.slack.com/archives/C070UU168N4/p1713967231252419?thread_ts=1713965714.163739&cid=C070UU168N4

https://medallia.slack.com/archives/C02LBCF9YE8/p1713967343479439?thread_ts=1713964419.607089&cid=C02LBCF9YE8


---

Need to fix wfc helm with helm template


https://github.medallia.com/Atlas/deployment/pull/62541

Disabled it.


---


Going to try the same thing as yesterday:

https://github.medallia.com/Atlas/deployment/pull/62429/files

```

--override nodeTypeContents. Var=roles=DISTRIBUTED_REBUILDER%0Abase-endpoint%20%3D%20 {{ $protocol }} %3A%2F%2F${POD_IP}%3A {{ $port }}

```

```

          - Name: POD_IP
            ValueFrom:
              FieldRef:
                FieldPath: status. PodIP

```

```

Kubectl exec -it wfc-dcr-0 -n tenant-101470-prod --context sea1 -c express -- sh  
sh-4.4$ curl --connect-timeout 3 ' https://localhost:9110/.admin?cmd=status ' -k  
Hidden

```

- Notice `https` and `9110`

```

sh-4.4$ curl --connect-timeout 3 ' http://localhost:9100/.admin?cmd=status ' -k  
Curl: (7) Failed to connect to localhost port 9100: Connection refused

```


```

    Readiness:            exec [sh -c curl --silent --connect-timeout 3 ' http://localhost:9100/.admin?cmd=status ' | grep --quiet -e 'hidden'] delay=30s timeout=5s period=10s #success =2 #failure =3

…  
Events:  
Type Reason Age From Message

---

Warning Unhealthy 34s (x1628 over 3h59m) kubelet Readiness probe failed:

```


So let's switch that around.

Check BE with this:
```

kubectl port-forward --insecure-skip-tls-verify -n tenant-101470-prod --address=127.0.0.1 wfc-fe1 9100:9110 --context sea1

```

```

Kubectl exec -it wfc-dcr-0 -n tenant-101470-prod --context sea1 -c express -- sh  
sh-4.4$ curl --connect-timeout 3 ' https://localhost:9110/.admin?cmd=status '  
Curl: (60) SSL certificate problem: unable to get local issuer certificate  
More details here: https://curl.haxx.se/docs/sslcerts.html

Curl failed to verify the legitimacy of the server and therefore could not  
Establish a secure connection to it. To learn more about this situation and  
How to fix it, please visit the web page mentioned above.

```

The difference might be `-k`

---

```

while true; do if curl --connect-timeout 3 ' https://localhost:9110/.admin?cmd=status ' -s -k > /dev/null; then echo "Ready"; else echo "Not Ready"; fi; sleep 5; done

```
https://workmemos.erauner.synology.me/m/nssNcyGdHF9F6J6UEUG8qM
UID: nssNcyGdHF9F6J6UEUG8qM

----------------------------------------
We need to add more ephemeral coldswap for the


```

    Requests:
      Cpu:                500m
      Ephemeral-storage:  87795171328
      Memory:             411779989504

```

We need to add more like the FE nodes have.


Farmers need more. What FE nodes have divided by amount of the farmers. And add a bit more.

...

What base amount can I set in the meantime. We need a new default.

Latest cache for that company


Caches using groovy script:
Ex: `wfc/wfc-fe1/wfc/node`

(160 * 2) / 20 = 16

...

Let's run this script against every cluster to get an idea.

---

Should setup a DCR specific alert for ephemeral space

```

Groups:

- Name: memory-alerts  
     Rules:
    - Alert: HighMemoryUsage  
         Expr: (container*memory_usage_bytes{namespace="tenant-101574-prod", pod=~".*-dcr.\_"}) / (1024^3) > 80  
         For: 5m  
         Labels:  
         Severity: critical  
         Annotations:  
         Summary: "High memory usage for DCR containers"  
         Description: "Memory usage has exceeded 80 GiB for more than 5 minutes."

```

---

Go to s3 and go the latest cache, ephemeral should be this


https://workmemos.erauner.synology.me/m/nnjfBn32GnsPxYaRoeAMoz
UID: nnjfBn32GnsPxYaRoeAMoz

----------------------------------------
Something I'm going to mention to Varsamis

...


I have a bunch of small changes I need to make with DCR.

I know that these items are not on the sprint but I'm going to go ahead and start creating/implementing them.

I want to start getting it pulled together and I'm afraid that in the past I've tried to fit too much into a single PR and end up getting stuck as a result.

So just bear with me.
https://workmemos.erauner.synology.me/m/DXpq3KYA8swPen6dzo83wQ
UID: DXpq3KYA8swPen6dzo83wQ

----------------------------------------
Need to figure out why TLS clients do not run DCR

https://medallia.slack.com/archives/C022XV98JH4/p1713970169435189?thread_ts=1713903809.311749&cid=C022XV98JH4
https://workmemos.erauner.synology.me/m/EfRZ8ZYNRGuznYvoLnsroX
UID: EfRZ8ZYNRGuznYvoLnsroX

----------------------------------------
Need to get DCR to latest delivery image
https://workmemos.erauner.synology.me/m/8S7D7jzt2w9t8P65hRyGmm
UID: 8S7D7jzt2w9t8P65hRyGmm

----------------------------------------
Add a reference on how to logs DCR bugs to the dashboard:

https://pacific.medallia.com/display/~erauner/DCR+Dash
https://workmemos.erauner.synology.me/m/mo4ozbjrs8xZ3wruonGaLz
UID: mo4ozbjrs8xZ3wruonGaLz

----------------------------------------
https://medallia.slack.com/archives/C0140KBUPGC/p1713966854230379?thread_ts=1713911630.901719&cid=C0140KBUPGC

```

Copy-caches tamarriott-fe1 tamarriott-fe2  
Copy-caches tamarriott-fe1 tamarriott-fe3  
Copy-caches tamarriott-fe1 tamarriott-feX

```

This is how you copy caches ( propagate them) after a DCR is complete
https://workmemos.erauner.synology.me/m/ARCZ7nAfo3Tfg4YDiGLiLn
UID: ARCZ7nAfo3Tfg4YDiGLiLn

----------------------------------------
For sergio

Had a separate question:

What other cluster other than `marriott` and `t-mobile` should I create a DCR app for?

This was brought up by platform yesterday
https://workmemos.erauner.synology.me/m/aLiTnhBTpFrUtCyUWodaQB
UID: aLiTnhBTpFrUtCyUWodaQB

----------------------------------------
I want to learn how to know when a cluster needs a DCR and how we can tell from express that it actually completed successfully (other than seeing that the synchronized is no longer in booting)
https://workmemos.erauner.synology.me/m/f8RLdfHZkDKBmdw9oYNEa4
UID: f8RLdfHZkDKBmdw9oYNEa4

----------------------------------------
Compile large list of clusters and create dcr Argo apps at the same time
https://workmemos.erauner.synology.me/m/J9Sc96K8kmQrCVtLyKGDSE
UID: J9Sc96K8kmQrCVtLyKGDSE

----------------------------------------
https://medallia.slack.com/archives/C0140KBUPGC/p1713957428306909?thread_ts=1713946782.628899&channel=C0140KBUPGC&message_ts=1713957428.306909


Create a jira to address this DCR bug related to the express versions
https://workmemos.erauner.synology.me/m/jDXJrsMmHQFRGAi3ZUhZEf
UID: jDXJrsMmHQFRGAi3ZUhZEf

----------------------------------------
Get a list together of every cluster that is unique that needs DCR
https://workmemos.erauner.synology.me/m/DXBpgbRUhwUAJjaUrkjY4S
UID: DXBpgbRUhwUAJjaUrkjY4S

----------------------------------------
```

curl -I " http://config-service.config-service/getConfig?application=express&configVersion=d490fe98d7acad05f369f1dd6a536f2503c2607e&environment=sc4.medallia.com&ignoreSecrets=false&overrides=nodeId.var%3Dmarriott-dcr-0%2CnodeTypeContents.var%3Droles%3DDISTRIBUTED_REBUILDER%250Abase-endpoint%2520%253D%2520http%253A%252F%252Fmarriott-dcr-0.marriott-dcr%253A9100&service=dcr.express&tenant=marriott"  
HTTP/1.1 200 OK  
Date: Tue, 23 Apr 2024 20:14:25 GMT  
Content-Type: application/json  
Content-Length: 163766

```

This is how you test a single configuration.

In this case: `d490fe98d7acad05f369f1dd6a536f2503c2607e`
https://workmemos.erauner.synology.me/m/ZuoWM2ScnbkVSyN64VePW6
UID: ZuoWM2ScnbkVSyN64VePW6

----------------------------------------
Moving forward, just so everyone is aware. DCR technically actually needs a specific config changes

Unless:
Latest config in master is broken
A specific tenant has some changes that the DCR nodes also need

Otherwise, only express version is needed in the request, and that's only if it's an deploment. If it's not a deployment, just CR on existing version, we don't need to supply the express version either.
https://workmemos.erauner.synology.me/m/Bg63JWGUya773si7aARCdR
UID: Bg63JWGUya773si7aARCdR

----------------------------------------
```

Create a jira to bounce a node differently
