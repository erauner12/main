---
id: 9
up: "[[2023-W48]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20231129000100
modified: 20231130000100
aliases:
  - Wednesday - November 29th 2023
linter-yaml-title-alias: Wednesday - November 29th 2023
title: Wednesday - November 29th 2023
week: "[[2023-W48]]"
yearly: "[[2023]]"
quarterly: "[[2023-Q4]]"
monthly: "[[2023-11]]"
daily: "[[2023-11-29]]"
month: "November"
weekday: Wednesday
---

# Wednesday - November 29th 2023

## Tasks

%% TCT_TEMPLATED_START 2023-11-29 00:00 %%
* Recurring
    - [x] Start Laundry - Morning ✅2023-11-29
    - [x] Gratitude - Morning ✅2023-11-29
    - [x] Start Rice Cooker ✅2023-11-29
    - [x] Check Calendar for what events are occuring ✅2023-11-29
    - [x] Go Exercise - Morning ✅2023-11-29
    - [x] Start Cook/Eat Breakfast ✅2023-11-29
    - [x] Fill Up Huel ✅2023-11-29
    - [x] Fill Up/Drink Water Bottles ✅2023-11-29
    - [x] Take Vitamins ✅2023-11-29
    - [x] Brush Teeth - Morning ✅2023-11-29
    - [x] Start Cooking/Eating Lunch ✅2023-11-29
    - [x] Go for Run ✅2023-11-29
    - [x] Catch up on Email Inbox - Morning ✅2023-11-29
    - [x] Meditate ✅2023-11-29
    - [x] Check Snoozed Emails - Evening ✅2023-11-29
    - [x] Read Highlights ✅2023-11-29
    - [x] Finish Dishes ✅2023-11-29
    - [x] Brush Teeth - Evening ✅2023-11-29
    - [x] Put in retainer ✅2023-11-29
    - [x] Reading ✅2023-11-29
* Recurring Weekly -=-
    - [x] Go see Auggie ✅2023-11-29  
%% TCT_TEMPLATED_END 2023-11-29 23:59 %%
* ? Did these tasks align to your Goals?

# Rollover

# Daily Notes


or is it dynamic right now? It's a standard port, so I think it's gonna be dynamic, but there is a service which should be static. I wonder if like alert manager will be able, like, cause right now, for example, I'm just sending the data to a static IP address. I wonder if I can similarly just send that to like a Kubernetes like created like DNS name or something like, or if we need something additional to be able to point it there, but I'll figure it out. I think for now we should be good either with the port IP or the service IP, but definitely at the, you know, eventually we will definitely need some proper DNS record for that. Okay, okay, I can, I'll take a look at the, what you've done so far and try to send it that to there. Okay.


---

* Summary of Zoom transcript:
    * Discussion about using Helm chart version and extracting it from the application to set it in the values.
    * Mention of a change in the next version of Argo CD to make the Helm chart an artifact.
    * Proposal to designate a different target revision for Helm 001 and have all instances of the application use the new chart with values from the deployment repo.
    * Need for an application set per DC due to not using templating for application sets.
    * Possibility of convincing the owner of the repo to use templating.
    * Suggestion to use a script to handle changes safely instead of making multiple PRs.
    * Consideration of using waves and pre/post hooks for changes in the future.
    * Concerns about going against GitOps methodology and relying on scripts for changes.
    * Discussion about the difficulty of implementing this for GovCloud instances.
    * Mention of a CR that controls resources for a single instance and is ephemeral.
    * Upcoming changes to eliminate the need for an FE node in DCR.
* Most actionable items:
    * Further explanation and discussion of the proposed changes to DCR and how it fits into the overall MC cluster.
    * Preparation of a diagram to better understand the role of DCR and its tasks.
    * Consideration of how these proposed changes may affect the blast radius and management of resources.
    * Discussion on the use of waves and pre/post hooks for changes in the future.

%%  
use the Helm chart version, utilize that field, that means that we will take, extract the Helm chart from the application and kind of set it in the values, right, or I'm confused. So it will be now part of the overlays, that's what I want to say. As it relates to that change that I was talking about last time, which was making the Helm chart an artifact, which is not possible until the next version of Argo CD, then we would just change where that path is, where Helm 001, we would just designate a different target revision and that target revision, like every single instance of that application could then be like using that new chart with the values that are sitting in the deployment repo. So it's kind of like, I mean, as far as I can see, like completely independent of what we're talking about now, but I think it should be like similarly possible. But we would need an application set per DC, that's something else to consider, like because we're not using templating at the deployment, like we're not using templating for the application sets themselves, like we're using customized for that and overlays, so we would need an application set for each DC, unless we convinced like whoever the owner is of the repo to like to, but that's, I mean, I don't, I don't think that's a problem per se, it's just, yeah, we're going from, you know, alright, so one more thing just to mention, just for like, is like, I consider this to be preferable for the instance that, you know, how we almost changed the network policy and fucked everything, I think that this would help with that, because then we could sync those changes without having, because like, almost every time we have to make a change, right? Like somebody like Mertaz would be like, I want to do this safely. So make a PR in this order, in that order. Well, instead of having to make like several PRs, which again, I agree, like a script would be like, handle a much better, we don't have that script right now. And what seems like, would be easier is okay, I'm gonna make that change once self heal is set to false. And then I just have Argo CD CLI or something just iterate through. Yeah, but that's, that's another thing, right? Because if we have everything as it is in cloud, let's say, in Kubernetes, in the cloud, in OCI, so if we have everything, as a Kubernetes resources, and not the way we do it today, then we will have the ability to do waves, we're going to define waves in the in all this behavior that you're describing that Mertaz will say, yeah, first the backends and the frontends and one by one, it's gone, we can implement with these waves and pre and post hooks. And that's one thing. And the other thing, which is like kind of problematic with what you're saying, is like, you're kind of going against the GitOps methodology. Because like, that's exactly what GitOps is saying, like, we want everything to be defined, our infrastructure to be defined as software somewhere. And then we have the expected the kind of in a declarative way, right? Yeah, because we have what's expected in our Git repository. And we expect that to be reflecting in reality. Yeah. But the way you're suggesting is like, we are taking away this part and like, we would have something in our Git repository. But we're not 100% sure that this is what will be done in an automatic way, right? Something needs to happen in between. And we need to write scripts to do that. Like we need to have things with I think, in the long run, could be unreliable. Like, in a sense, it was like, who what will make you will write another script to tell you what's the status of all the applications right now? Yeah, I mean, yeah, it's, it would effectively be the same thing as what the prod deployers doing. The only difference would be that the manifest wouldn't be static, it would be, it would be templated. You know, but it would similarly be, yeah, not like not what it says that it is until that application is synced. And as far as I can tell, that's what, like, is desired. But maybe not, like, I don't, I'm not saying it's desired, like it for the future way that we deploy Express. But right now, that seems to be what's desired, because that's what how it was built with a product player. So in that, the reason for that, as far as I can tell, is, is like, because they want to have control over when, say, this instance gets the change, and that instance gets the change they want, they want, they want like a granular control over that. And so I feel like this would be the closest thing to that. But yeah, I agree. It's not it's not, it's not, it's not. Well, I guess it's like the happy compromise between what a true GitOps would be and what we're doing right now. But okay, yeah. Anyway, I think, I don't know now, if it's like, something worth investing to tell you the truth, or changing into so much, changing into that pattern, because like, also, the other thing is like, no matter what you do, even if you disable it, yeah, okay, I understand that it's not going to be start updating all the instances all together, or whatever. Yeah. But imagine that if it's kind of like also changing the way we manage the blast radius, let's say, because like, you make a change in one point, and whatever you think after that, it will be affected. Let's say why, if you just have single applications defined, you make a change to that single application, and the change will only affect that specific instance. Yeah. You know, but on the other hand, because you're disabling the self heal, this is not such a big risk, let's say, because you're disabling, but again, you're taking away one of the basic functionalities that we and I like, personally, because it makes the resources to be self healed, like, so if somebody goes and does a manual change, you say, No, yeah, that's what I have in GitHub. And that's what's allowed. Trust me, I like it, too. I love it. In fact, but I just from what I just seems like, with Express, that's a difficult thing to like, like, well, I think it is possible, but we would have to set things up in such a way that it would reflect, like, exactly what we want, every time. We don't have it like that right now. Let's consider something else. If you do that, you know, in normal instances, let's say, what are you going to do with GovCloud instances, because there, you cannot do what you're suggesting, because there, you cannot change one thing, and 100 things starts getting updated. Even if it's a script or whatever, you need to have accountability, what did this change and how and like have it in a ticket and so on. And somebody approves that pull request that makes a change to that specific resource. So we're going to get into it's a can of worms, I think, to tell you the truth, if you consider GovCloud as well. Yeah. Yeah, I haven't. I don't know. Look, if you if it's like something, because because in the, on the other hand, if this is something like a common resource, between instances, like because I don't fully get the big picture. Yeah, this is fitting. Yeah. But if it's like something like Redis, for example, right, but it's it's shared between multiple instances, then it makes sense to me to have a single point that you control things with that. That is CR specifically, it does make sense. That is kind of what it is. Yeah, but but the way that it's being done is not like, say, all the resources are not in one namespace. Like technically, they could be but because of the way that like mech is segmented, I just decided to like align it to the mech instance itself. But it's like a single single instance of this. Yes, but it's ephemeral in that the only thing that is existing after it is done is the staples, the two now two staple sets that have zero replicas. The process of like using it, like is only for a cache rebuild. And as soon as that's done, ideally, the automation will turn both of those replicas are so that it's not it's not running. Okay, okay. Okay. So it's just sitting there without any resources. Yeah, like it. Well, the resources are there. The one like the staple sets are there. But the only when they're being used, or do they have like actual pods being like produced by them? Yeah. Okay. Okay. Anyway, let's take another look like, maybe spend some time explaining how this DCR works. Yeah. And what is what is actually creating a DCR instance? Because from what I've seen, we have to do two tasks, right? Start and stop. Yeah. And those tasks are like what you are referring to. So the resources are there, and you just start them and stop them. Yes. But here's here's one thing I'm about to change as a result of what's being introduced. What I've been working on for the past week is this new node that's a sync node, because in the previous DCR, we had to rely like not only on the staple set that I introduced, that's running like a like a dynamic version of Express that it like it's, but also we had to rely in the static version of DCR on this on the FE node that's actually running, like in control by the prod deployer. So now we've actually like eliminated the need for like an FE node. And both nodes that I need for DCR, both the synchronizer node, which is an FE node, it's a dynamic FE node, both the synchronizer node and the farmers themselves, I'm controlling. So I'm controlling everything and I'm not following again, like, you know what, I would suggest like, can you add a topic for the next technical discussion and have a nice diagram that you explain how, where is DCR fitting in according according to a MC cluster? And what is his job? And like, let's explain that so I understand it. Because right now, I cannot follow what you're saying. Yeah, like, I don't understand and say also how, how, how things are done, like maybe we can do it in 20 minutes or something. Yeah, actually, I have a I have a page right now that I'll send you but I think there's more. I'll prepare something more. But this will give you half of the equation here. Okay, okay. Yeah, it doesn't matter. We can just go through that page if you have something ready in case of diagram, but you can explain what it's doing and like how it's doing and so on. How does it fit in the whole big picture? Yeah, I can. We'll look at it next. Okay. Okay, but other other topic in the document you have it

%%

---
