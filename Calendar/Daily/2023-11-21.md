---
id: 9
up: "[[2023-W47]]"
description: ""
publish: false
starred: false
status: ""
type: note
tags:
  - periodic/daily
cssclasses:
  - "cards"
  - "cards-cols-1"
obsidianUIMode: source
obsidianEditingMode: live
template: "[[Daily]]"
created: 20231121081117
modified: 20231122104153
aliases:
  - Tuesday - November 21st 2023
linter-yaml-title-alias: Tuesday - November 21st 2023
title: Tuesday - November 21st 2023
week: "[[2023-W47]]"
yearly: "[[2023]]"
quarterly: "[[2023-Q4]]"
monthly: "[[2023-11]]"
daily: "[[2023-11-21]]"
month: "November"
weekday: Tuesday
---

# Tuesday - November 21st 2023

## Tasks

%% TCT_TEMPLATED_START 2023-11-21 00:00 %%

%% TCT_TEMPLATED_END 2023-11-21 23:59 %%
- ? Did these tasks align to your Goals?

# Rollover

# Daily Notes




Okay, can we link it to this? Yeah, let me find it. Epic It is Okay, I found it 357, okay Okay, then we need one for creating Kafka topics All right, but what is gonna write in the Kafka topic, can we write directly? Yes, there's a trigger. Okay. Okay, great. Okay, so We just need to decide what instance of Kafka we'll be using Something existing. So should we do it something vague like investigate the Best practice to use Kafka Yes, I can update the the ticket afterwards like saying Check whether there is some existing instance that we could possibly use or create a custom one Yeah, I think There I will just say that there are Kafka and there is a Kafka fleet that you can use in from what I know That would definitely be better I Don't know how to Learn how to put a gear inside of it You don't need to you just need to go see down here. It says epic link So you provide the one? Like it's a proven thing. Oh, it's a sorry roacher. I think it Really yeah, it searches using the Epic link also searches through the or in the Garden folder or in the header so you don't need to put the You don't need to know the Ah, so I decided out there, okay Okay Then we said I Think we need another one For the left side and it's the creating the Argo events resources like event sources triggers and sensors so that we Basically connect all the so configure configure. Let's say it's a reverence. Hey, sorry Yeah Something like that. Yeah, okay You It's like even sources sensors triggers figures I Didn't get it Even sources Answers why is it serious? Just put it, you know, it's for my we can we can Okay, I hate the JIRA formatting You know, yeah, I always switch to text and just format it is down I Use a plug-in to always convert it. I was just writing markdown and And Then sensor sensor Sensor another C Sensor, okay Thanks You


---



- Need to consider how to structure the process of pulling and processing events from Kafka
- Suggested creating a single box for everything in the first phase, but keeping in mind the possibility of splitting it in the future
- Proposed creating a ticket for phase one, focused on SRE router features or foundations, without worrying about the database or APIs
- Agreed to name the consumer for the Kafka queue
- In the first phase, the single box would handle consuming, validating, creating event workflow, generating mapping, and potentially more
- Need to determine if and how to split the process in the future

%%
Start thinking about this, right? I'm not in this phase. What do you think? Well, I think we need at least something, because we'll be, suppose that we have the event in Kafka that's going to be pushed there by events, and we need something to pull it and process it. So I don't know how you want to structure the whole thing, whether you want to use these structures that I outlined in the. Look, it's, as I said, we can actually make this one box in the beginning, but with keeping in mind that we can split it in the future. So that has to do with the way we're going to structure the packets and the way we're going to install it later in. OK, so we can create the ticket saying phase one, you know, SRE router features or foundations, because I wouldn't bother with the database right now and with some kind of APIs. Do you agree to name this consumer, because we're going to consume the Kafka queue? Well, yes. Well, it should do like a bit more than just consume the events based on the diagram. But yeah, in the first phase, we could have a single box for everything, for consuming, for validating, for creating the event workflow, things like that, generating the mapping. And then we'll see if and how we're going to split that.

%%

---

Get for it for that. Yeah, I Think we can start we can start by because again early development stage is like we can even start with JSON file Inside the code base and like then we can just extract it from that and like make it the external configuration For prevent mapping, you know this because now we want to map this one, right? Yeah We can also be free of like what the structure is gonna be how what data we're gonna keep there and this stuff What do you think? Well, I think that we first need to make sure that the event mapping is you know that we understand Understand it this way the same way to me it's mapping an event to some kind of Set of actions. Yeah And it's definitely not just the SRE API action or task. It's it should be set of yeah Yeah, you know arbitrary actions, you know ordered in You know in different order. Yeah for for for, you know different events So that one event may start with slack another event may start with creating a Jira ticket, you know, things like that Hey, I think that so this is gonna be defined But whatever the signature of this Kafka message will be because I guess this will have a type In it and record and the type will be the key to this mapping I think definitely but the the event mapper should work with an event not with a Kafka message because that that's why the event manager Is there? Yeah So yeah, just to be sure It's gonna read the the Kafka message and transform into transform it into some kind of internal object or whatever which will be called event probably and You know, it's not going to be a Kafka message anymore once passing the event manager Well, yes. Yes. Yes. I'm just saying like normally what this is like a Jason A Compressed Jason Object a file and that's it and then you think of you can either first thing is like you you decompress it you make it Into your you put it into your data structure and you start doing things. I'm just saying that one of the things so Simply this can I add here? Here nice Bigger than I expected Okay Hey Evan, you wanted to mention something sorry One thing so this might be obvious, but it's the the event But it's what's inside of that event like that particular like action Sorry, not action that particular alert inside of the event that I think that we're interested in for the mapping, right? Because like let's say it's like that Alert that it say you say we get a list of alerts inside of the payload like each Alert inside of that event is what we're interested in in the sink Well Yes, that's what I want to mention that's a you know fundamental thing that we should agree on whether a single Kafka message may Contain Multiple alerts or if it's gonna be a one-to-one mapping So we I didn't realize that we could control that it with the Kafka at the Kafka layer like that I thought well, I think that it's gonna be like decided How we are gonna gonna be pushing the messages using Argo events So suppose that the event source is gonna accept some kind of event or alert from some alert manager or Kingdom or you know ad hoc or whatever and then it's gonna generate Probably one Kafka message for each That's a lot better than I thought there was gonna be Nice the whole point Get event and like if something happens to your to your Backend in a sense of your consumer you have still have the messages so you don't lose But it needs to be one message one event Just didn't realize that that was possible because like in testing I just was listening directly to the web hook and sometimes the web hook would send a lot at once like But I guess that's why we're introducing this that makes sense nice what we're gonna have to come up with is the structure of the of the message right because we as I mentioned we need to have several pieces of information inside that type of the alert Then the art itself may be some kind of other metadata So that's gonna be It should be defined using some kind of Schema or something I meant when I said signature, but they call it signature. Yeah. Yeah, so we need another We need another ticket Right. Yeah, and and however the creator I Just want to do mention that we need a ticket for


Adhoc event type
- Will contain the data

---



which have the task type. And then we have specific parameters that belong to that particular task type. And that could be the alert, while the envelope could have some defined set of fields. One of them will be the type of the alert. Another one will be some kind of timestamp or whatever. So yeah, but the schema should be well-defined so that we can share it with the teams and, you know. But I think for sure we know now that we need a, I don't know, we can call it event type, or I don't know, whatever, what it will be a string we want. Then I don't remember how do they look in the provisioner if we want to keep some parity. Like platform or something? Not really, but more because normally what you do is like you say, OK, event, and the first key is data, for example, in event-driven stuff. And then it's all the stuff you want to have there. OK, sorry. No, I'm lost. For example, this is like a params file. Let's have this context. Task type we can keep, right? And we can keep all this. Wouldn't the task type be like inside of the mapping? No, yeah, let's not confuse that. Anyway, but that's the whole point of this task, I guess. Yeah. So let's just say. I like the idea of storing not only what event, I guess, but also where it came from. Yes, it's a webhook, but eventually if we discover some additional type of event source, yeah, then to continue to know that as well, I think, would be beneficial. I think so, probably. Yeah. Anything else? But that doesn't, at least you can configure on events, right, in Argo events, how this schema will be. Well, I would say that I would expect that you can define the structure of the Kafka message that you're going to be pushing to Kafka. So I suppose that we should be able to define custom fields. Or not just fields, but custom objects or structures. Yes, again. Anything else you can think about now, at least? I don't think that we can be, we need to be too specific at this point. We'll see once we know how the individual pieces of Argo events are put together, like the sensors, triggers, and event sources, we will know what and how we can basically structure the Kafka message. So I would leave it open for now and leave it just this. Oh, yeah, and then some type of ID associated with it as well. Yeah, probably some kind of ID. Yeah, we may find out that we need something more, like some kind of nonce or nonce validity, because that's what we dealt with in the API server as well. Like, suppose that the system goes crazy and starts pushing events in a crazy manner so that it's going to start pushing hundreds of events per second. Probably, we'll need something to filter out the duplicate events right in the event source or something like that. I know that Argo events has a support for that. But if not, we need to have something like a validity check. But that's maybe too early now. Just to, I just wanted to highlight that we're probably going to need something more here. OK. In the data payload, the nonce, I mean. I meant in the envelope. But in the data payload, that's open. We will see what we're going to need there. Definitely, there's some kind of details about the alert itself. The question is whether we want to have the data field, again, mapped to some kind of set of different kinds of schemas. So that, for example, for alert manager, the data object should include this set of fields. For ad hoc type of event, it's going to include this and other sets of fields. But is it fields? I was thinking, that's why I said data here is an object. Yeah, but it's going to consist of different sets of fields. Yeah, yeah. Ah, yeah. What I meant, whether this object thing here should be, again, defined by some kind of schema, or multiple schemas, probably, depending on the event type. But I think that will be, again, configurable in the events, right? Yeah, I think so. But if we say that we will be following some kind of schema, we should generate the messages accordingly. Yeah, I think we need just to, as you say, just do a POC with what we can figure out today. And then we will see what's missing.


---


I think or no for fixing a repo like mixing making changes because you already have a repo for that. Yeah, I made it. I made a repo for this. It's it's in there. Let me get it real quick. It's a medallion. It's a rerouter. And I added you guys as admins to it. So we can we can change all of the Yeah, yeah. But you had the issue for that, like, because this we need to change it again, like, yeah. Or should we include it just in this one that we're going to do the consumer processor? Or do you think we need to break it and have another one? Yeah, that's what I actually wanted to ask you about. Yesterday, when you were talking about creating modules, like, would those be separate repos themselves? Like, no. Or how would Yeah, how would that work? Yeah, so look, it's up to us, like, how we can do it. You can have it like you get you have here, you can actually define it's going to be source or something. Right. And then you can have a another directory that will have your docker file, docker files if you need to, to package it, but then whatever we're going to do in the pipeline, for example, in the Jenkins file, we can instead of like packaging the code into a into an image, we can push it to the pipe by index. Yes. Artifactory. I like it. And then in our docker file, we just install it as a dependency. Yes. Like, we will see and we can have like multiple Jenkins files, right? One that will actually push the package, as I said, to the index and the other Jenkins file will build the images or the image and this is done just because as we said, if we want to split it, split those two at some point, we will have this, we will create probably a different repository or keep the existing one, but we will build two separate images that will have the same dependency, which will be our package. Yeah. I like that. That's how it's going to be working. I like that. And then like where we can keep it in one repo, but I mean, because I'm just a fan of having it all in one place, but still having the ability to decouple things. Yeah, you can have it. We can still have all of this in one repository, but it's up to us like now how we're going to handle it. But I think that's a better way, just in case we want to split things. So maybe we should just create a Jira for like, I mean, the repos aren't even created, but restructuring like how we… That's what I meant. Okay. Okay. Yeah. Anyway, it's enough. We'll fix it later. Okay.


---


I was kind of thinking that, okay, let's start with Jira, it's not a database, but I do think that if there is a specific event, and that's what I was telling to the guys that were asking us, so I think that if it has to do with a specific event, maybe the task performed for a specific event will require a Jira to be created, and that will be defined in what we're doing in the mapping, like in the event mapping. So one step of that could be create a Jira ticket, but I don't believe that we will create for everything that comes in, we will create an equivalent Jira ticket. I don't think this is efficient, but saying this, I understand the guy that was insisting on this, because he's from GovCloud, and in GovCloud, you cannot touch a thing without having proof of why you did it. That's why he was so requiring the Jira tickets. I think that the use cases for these two storages, so to say, the database, be it a Redis or something else, and Jira, so the use cases for these two is a bit different. To me, or at least, I think that Jira is for some kind of historical auditing, what has been created, what errors we have processed, something like that, so that the management or whoever is interested in going through that can do that, while the database is for another use case, like having a way how to pull some kind of metrics or statistics or reconstruct some alerts when there's a crash or do whatever, and by saying that, I mean that we probably will have different data stored in the database than what is stored in Jira. So, yeah, I would consider that we won't necessarily be creating Jira ticket for everything, but we will be creating a database entry for everything, while this definitely shouldn't be the case from the phase one or in the phase one, because there probably won't be a database in phase one. I agree. We can plug it in later, and from my perspective, it's like a NoSQL database is easier for us because we don't need to manage it. We don't need to do… If we want to add a field, we just add a field, like we don't need to do things like, I don't know, database scripts that will add the field in all the existing records and stuff like that that you need to do in the relational. But anyway, I agree with Kamil that we can do it in phase two, add the database support. Okay. Well, one more thing I wanted to mention, and I came to a similar conclusion about the Jira, is that even though I would rather not create one for each, I understand the rationale for doing so, but at the same time, using some kind of time series database to store similar information, whether or not it gets used, I feel it could be beneficial for us eventually to be able to visualize the metrics. Look, that's another thing, because we say, now you said time series and so on, but that's a different thing, let's say. Kamil before mentioned metrics, but I think we can, from now, add a Prometheus exporter so we get metrics. We get how many events we got and how many we processed successfully, unsuccessfully, and this stuff. We can add a Prometheus exporter to the whole thing. We may quickly find out that we don't need that database at all. Maybe. Yeah. Because if it's for reporting, we can use metrics, but if it's for, as they said, like auditing, then we will create a Jira, but it's per event. Yeah. Per event. Yeah. I agree. I would really, before introducing a database, I would really like to have a clear use case for that. Because, yeah, if we had a Jira ticket for auditing, Prometheus exporter for metrics, then probably the database is no longer needed, but yeah, I may be mistaken, but we'll see. It seems like the NoSQL, you would need that for, let's say the case of, okay, you're processing a single event and you want to keep track of it until it has been processed, and then you remove that record from, let's say, the Redis. I think something like that would be necessary, would it not? Yeah. But you are assuming now that, okay, I think we're assuming, you're saying that because you're assuming that this is not going to be a linear flow. Like I mean, when it hits the, what we're going to do with it, it's not going to be like the workflows that we have, that we expected to go from one step to the other and like nothing will break in between. Yeah. Something like that. I'm assuming that one event, like, yeah, is not like going to correspond necessarily to one task. Yeah. Okay. But let's say, look, now we're trying to get a POC. When we do that, we will see how it works. And then we can extend it and say, okay, now we need, really, we need a database because we need to keep track of what is happening with the action, let's say, and so on. So let's keep it simple now because now we kind of have a, we kind of can have a flow going because we have, we will have the events, we'll have the queue in between, we'll start working on this guy, which is going to consume and the process. And then we will have the action, which can be just simply writing on the Slack message. Yeah. And that's the simplest thing we can do. And then we will see how we want to expand it. Okay. All right. Based on, you know, based on various criteria.

---

No, like we don't need it for a POC. We can add it later. I think that for this POC, I think that we're good with the list of tickets that you've already created. OK, yeah, I agree. There is no database. There is no Prometheus, no bells and whistles. So yeah, maybe we can add one or two tickets. But I would still rather get focused on the most obvious flow in the first place. How about this part? Because now I just looked at it. What was this supposed to be? Well, it has two use cases, the two arrows that goes in, which is the user access, so that there might be a group of people who might be interested in seeing what alerts are there in the system, what's in process, what's ongoing, what has been resolved, things like that. We may use Jira for a subset of that. But overall, we probably need some API. And the other arrow that goes from the event workflow was that suppose that you have a workflow that creates a Slack message, starts a Slack thread. You probably, in some subsequent actions in the event workflow, might also want to have that Slack thread available so that they can write their own messages into that Slack thread. And for that, you need to have some kind of way how to reach some details about the currently processing task so that the idea was that, hey, I'm a step in the event workflow. And I want to get the details about the current task that I should be executing or I should be processing. And I want to know the Jira ID, or I want to know the Slack thread, something like that. And for that, we'll probably need some kind of API or maybe some other piece that would handle the passing the information from one step to another. OK. Yeah. OK. I get it. Yeah, I don't know now that messes up the plan for the packets. But OK, we can have a server component there and implement something. OK. OK. But should we? I think that this does not necessarily have to be there in the first phase because if we have just a way how to post a Slack message, create a Jira ticket, and maybe spawn the, or submit the, sorry, API task, we probably will be good even without some kind of API that's going to be available there for the individual steps to get the information about the event. But yeah, let's keep this open. Do we need a task now that you said it, that you mentioned it? We need a workflow task, right? We have that because I suppose that the event that's going to be there in the first phase, and Evan, please correct me if I'm wrong, that's going to bounce a MAC cluster, right? Or MAC, no, sorry. Yeah. And we already have that task. Do we? OK, OK. Yeah, and it's not intrusive. We can test it easily. That's what I was thinking, like if we need something just to test, but we can bounce our instances like all the time. So yeah, OK. OK. And this can work without a Jira ticket. Yeah. We can bounce without a Jira ticket, like in all that. Well, you can reach the API directly without a Jira ticket, just, you know, posting the new. Ah, OK, OK, we're going to hit the API. Yeah. OK, I think that's good enough then. Like we can. Definitely we have something to work on. Yeah. OK, so let's.


---


being able to use that synchronizer, that dynamic synchronizer as a donor and those kind of things. Is that using the work units and the pre-balance with the? No, we actually run it with the static but with a dynamic operation. Yes, and what's interesting about that, we never tested that with the dynamic, that is without the work unit, sorry, and it worked. I mean we had two retries that were problems with the static specifically, so when we moved to the dynamic, to the DCR balance, I think that it should work fairly easy, but since this is what we will be running in any case in Walmart in the foreseeable future, that's why we started with this static version, okay, not with the DCR balance. But the way things are going, if we get every detail about the deployment and all that right, then that means that we are very, very close to being able to get rid of the deployers all together and just use Argo CD and Kubernetes. Yeah, that'd be freaking awesome. Yes, okay, so from what I saw from the last notes, let me see, I saw that there is this what looks like the health check or the probe, I don't know what that is called. Yeah, readiness probe, yeah. Okay, so if this doesn't say waiting, that means that it's not going to be able to resolve the fully qualified name? I think that's the case and the reason I actually recognize that is that I think the readiness probe is designed to prevent traffic from having to go to this node until it's technically ready, which in this case, we don't have to have the readiness probe or we can change the readiness probe to be whatever it is, but it's not going to allow that endpoint to be created in the Kubernetes like definition of an endpoint, the IP, it's not going to be able to use it until that health check is resolving as yep, good to go. We might need to drop that because what I think is happening is that you have the synchronizer is coming up and it's telling, okay, I am part of the dynamic topology, okay, and then the pump running in the backend is trying to go because it says, okay, this node is here in the topology, great, so is it ready? It shows us ready in the topology, in the database, it already talked to me, so I need to send it something and the code that does that, if it has some kinds of problems, like for example, not being able to resolve the name, it's going to mark it as broken and then it's going to suicide. That makes sense, yeah, because like I think the requirements of this particular process are just not really aligned with like what this is designed to do, this readiness probe, so I think, yeah, you're right, we probably just need to remove it and use like a different way of validating that it came up properly, like specific to checking the express cluster, like the backend node or something. What do you think? You can just look at the .admin page status, right, that will tell you the page is up. Like that's what I'm saying, like, I mean, I try to avoid doing that in every area that I possibly can using Argo CD, but if I am forced to, then I will, like it's not, we can, it's just I try to avoid it just as a rule of thumb where I can because I'm trying to make as much of it external to the orchestration as possible. The problem is that the, this dynamic nodes, they will register dynamically very early in the initialization and probably by that time, this status command will not even work. Right the health check is going to keep failing because it's not ready and it's in a waiting state. And hence, even though it is part of the dynamic topology, and that's when the backend is going to keep marking it as broken. So we're kind of in a chicken and egg situation. It's not a problem with the configuration, but more of a problem with the way the health check is working. So Kubernetes is going to mark the node as not ready. And then Express will think that it's actually ready because it's part of the dynamic topology. Right. That's what you're hinting at Saguio. Yeah. So the thing is, I'm trying to think what's the best way. I'm looking in Kubernetes documentation. At that time, at that time, if the backend wants to connect with that host through thrift, which is how they talk, they are going to be able to, it's going to work. So if we just, I don't know if you can just remove the readiness probe completely. Yeah, I can. It's just a line, you know, it doesn't need to be there, but it's usually to the benefit of almost every other scenario than this one. For this particular scenario, I mean, that is something that we can try. I'm also going to check Kubernetes documentation to see if there's some option that would, I mean, I'm not, I'm not thinking there will be, but it's worth a shot to see if there's a way to allow it to work before the readiness probe is technically done. Oh, you mean to be able to resolve that address even before the readiness probe is ready? Yeah, I mean, I would imagine that they made it that way on purpose, but if there is an option, then I'm going to, I'm going to, let me see if it exists, but I don't think there will be, but we'll see. It wouldn't make sense. I mean, if you will say publish this service when the readiness probe is ready, then yeah, don't say ignore the readiness probe. I think technically the service would exist and it's just the endpoint itself. It's not going to get, well, no, maybe the endpoint exists as well, but it's not connecting those until the readiness probe is a success. So let me see. Well, what I would do is just test that if without the readiness probe, this works. This is something that I wanted to do for some time now, because one of the first things that is enabled in an express node is the thrift service. So if we managed to put the readiness probe into thrift, I tried to do that. It's not easy because we modified the protocol. So you would need to hit, not with curl, but with one program, one Java application that will just hit the readiness, I'm sorry, status command. That wouldn't be impossible. I mean, I think I've never used anything other than curl with the readiness probe, but I would assume what we can just designate any binary like on the container. Yeah. Yeah. It would be execute this program and this program, what it does is it connects via thrift. And if it can connect, it says, okay. And then it would give us an exit code that like would say whetherIt would give you this exact same thing. I mean, that exact code is duplicated in thrift. But since we, right now, since the node says, okay, you can access me in this name and this name is not published until the readiness probe says, okay, it doesn't matter what we put here, it's going to fail. Okay. So I guess I can remove it for now just for testing. But if you could provide me with that, whatever it is, then we can test that. I don't think I tried to just use curl because I thought, okay, if I can identify what the initial package is and what the expected response package is, but it's a very strange protocol. It's a connection that is stateful and it keeps like a dictionary of things that already sent and they are not sent. So if you send the same command twice, it may work the first time and fail the second one because the state of the connection has seenI think in the readiness probe, I don't know. I think you can configure a wait time for it to actually start, like executing that whatever the command is that's indicating that it's ready. I think you can configure a static like wait time for that. Would that help or no? Yeah. Let's, no, no, no, no, because we are in the same situation because as I said, what we can probably do is like add that kind of timeout to the registry. That is, I register now, but don't talk to me until 30 seconds from now. I don't know. Something like that. We can do something like that, but that is, I really don't want to go that way. We can probably change the code to make it more complex. So if it is trying to, if the backend is trying to talk to a dynamic node and it fails because of this, just, it's okay that it fails, but just make sure it's not marked as broken in that case. Okay. Yeah. I think we need to try that. Right now the board is even not coming up, so we need to see what's going on. Yeah. No. So what I would do is try taking out the readiness probe or just replace curl by true and, and then we will see if what makes better sense. Okay. Okay. Can you just check the logs, the dynamic synchronizer is kind of stuck in a crash loop, so it's not going to get ready no matter what. And the logs are not ready. Well if we change the, this thing is going to, next time it's going to change the readiness probe and this readiness probe failed, will not fail. Well like, could we also in this case be leveraging like the, there's a readiness probe, but also a liveness probe that will like continue to check if the application is alive. And the problem is that you would expect that as soon as it says ready, liveness is going to be also correct. Yeah. Okay. We, we can, and I think that if we are going the way of these dynamic nodes, we do need to do a, to have a reliable readiness probe and make sure that when the readiness probe is true, the one that we define that it's only that moment that the node is registered or after that, the node is registered. Right. We don't want the node to be ready before it's actually ready in a sense. Yeah. Yeah. Because it will start doing things which it's probably not supposed to. And we have no idea on that state, because what if… I think that we, so far what we try to do is try to come up with this readiness probe and liveness probe, given the tools that we already have. And this is a different case, we need different tools. Well, it looks like there's also a success threshold and failure threshold that allows the probe to report, like it must report success like this many times or failure this many times before it decides to do something as a result of it. Like I'm just looking… Oh, like the dead man's switch. Yeah. But so actions here are, we are going to try without the readiness probe and we are going to fix the readiness probe. Okay. To fix the readiness probe, that is from my side. Okay. Okay. And probably what I would do is I will defer the registration in the dynamic topology until we know that the readiness probe is going to be successful. Okay. Sorry, I'm just asking this again in case this has already been said, but like you said, if we're using this like thrift, like we're using thrift and maybe like not curl to, whenever we eventually create this, you're saying that sometimes it'll say success and then other times it'll say it's not success, even though it already did that or no? No, no, no. Because of the, it has a stateful protocol and that is why it might run okay the first time, but the second time and from there on, it will not be okay. So there is some, there are some challenges. So we cannot just copy the wire, the packages that were sent in a successful invocation of that service. Yeah. That would be very easy to do with curl, but we need to do an actual app, a small application that will actually connect using the protocol. It has to have all of that stack. We cannot just use curl. So either of those will work. Also, also Sergio, do we need the farmers to be up before the dynamic node is going to be ready? That was another question I wanted to ask too. Yeah. It depends on what we are trying to do. What we are trying to do now is see if this node is going to come up with the cache that is in S3. For that, we don't need the, we don't need the farmers, but we need to think a little bit how we are going to operate this because most of the time we don't want this, this synchronizer to come up if it doesn't have the farmers. But for this particular case where we are trying to check the donor, the donor functionality is a different case. So to answer your question, Murtaza, I don't see a situation where it will be useful to have a synchronized.

---


**JIRA Title**: DCR Extend Length of Workflow

**Description**:

This JIRA focuses on enhancing the workflow process within our Dynamic Cache Rebuild (DCR) system. The primary goal is to streamline the startup process of DCR farmers and to extend the workflow for better management and oversight of DCR operations. Key points discussed include:

1. **Dynamic Synchronizer Integration**: Explore the feasibility of using the dynamic synchronizer for history updates. This would allow for simultaneous startup of multiple DCR farmers, eliminating the need to sequentially start and update them.
2. **Workflow Enhancement for DCR Farmers**: Modify the current workflow to continue running until the DCR process either completes or fails. This change aims to provide clearer visibility into ongoing DCR operations and to distinguish between DCR processes that have not started versus those that are in progress or have failed.
3. **Configurable Farmer Thresholds**: Implement code changes to allow configuration of minimum and maximum farmer limits in the JSON properties. This includes setting a default minimum threshold (suggested: 2 farmers) and a maximum limit (suggested: 100 farmers) to accommodate varying customer needs.
4. **Readiness and Liveness Probes**: Improve the accuracy of readiness and liveness probes, particularly in scenarios where the startup process is paused unexpectedly. This is critical for ensuring system reliability and for handling potential issues during the DCR process.
5. **Synchronizer as a Data Guardian**: Post-DCR, consider leaving the synchronizer node active as a guardian of the dataset. This would enable options like fast recovery and facilitate cache updates with newer versions maintained by the synchronizer.
6. **Testing and Implementation Phases**: Conduct testing to validate the synchronizer's capabilities in reading from S3 and acting as a donor. Plan to implement the enhancements in phases, starting with the crucial aspect of extending the workflow duration.
7. **Handling Synchronizer Downtime**: Develop strategies to address potential downtimes of the synchronizer, ensuring minimal impact on the DCR process.
8. **Manual Override and Monitoring**: Include features for manual intervention and monitoring, allowing for flexibility in managing the DCR process and adapting to unforeseen situations.

This JIRA aims to significantly improve the efficiency and manageability of our DCR process, aligning it more closely with our operational needs and customer requirements.

---

Feel free to modify or add any specific technical details that may have been missed in this summary.


%%

If this is just going to be a dynamic synchronizer, why will we need that to be up with no farmer? Well, I have a use case for that and I mentioned it inside the chat. I haven't got a response yet But I just mentioned it but I think and this is related to what I mentioned to both of y'all like individually about the Like right now we have to to be able to start DCR farm We need to start one farmer and let it do its history update Which could take X amount of time like a long time or a short time either way And then we allow the rest of them come up. So effectively what that looks like Externally orchestrated is one replica and then when that one replica is ready Then we turn on the rest of the replicas But what I what I'd like to be able to do and I don't know like the dynamic synchronizer would be able to help this Situation but if we could use the dynamic synchronizer and to do the history update itself then Whenever we actually turn on the DCR farmers, no matter if it's two three or a hundred Replicas we don't have to worry about this situation where we need to like bring one up have it do its thing and then bring The rest up we could just turn it to full however many that we need and then adjust that accordingly like I Just I want to I want to I want to try to avoid the scenario that we have to Turn one farmer on and then turn the rest of the farmers on I want to be able to just turn all of them on at once Okay, that's a good point that's a very good point Let me answer that question two parts first with DCR farm as it is now the static DCR farm and then with a Dynamic DCR farm that is DCR balanced With the DCR farm It checks at the beginning of the process how many farmers are available and then it uses those for the For the for the rebuild what we could do is Require a minimum number. I think we already have that. Yeah, that would be two Yeah, but that's that's configured that that is not I know that for DCR balance that is configured. I don't know how it is for For DCR farm. I will need to check but if we can do that and put it in such a state that it was just going to be waiting for the minimum number of Farmers to be available then we probably can do that That is get the synchronizer up and it will magically wait until we have I don't know 40 farmers up Okay Would it do the history update as well or no Yes. Yes. Okay. Cool. Sorry. It is obvious. I'm sorry I'm not familiar enough with it, but that makes more sense. That makes sense to me like what you just described Oh, yeah, that would work now with DCR balance that's already built into DCR balance That it's going to wait for the minimum number of Farmers and the good thing is that if you keep suppose that you want I don't know 100 farmers and your meat is 40 What will happen is if you if we do this The the synchronizer goes up it does the history update it checks it doesn't have the minimum minimum number of Farmers so it's going to wait You start getting the farmers up when it reaches 40. It's going to start doing the DCR And Farmers will keep coming up and in the DCR balance. They are going to be added to that same process Yeah, yeah, see that would be cool because like then if you could then right there isn't the whole point So if this is going too slow, we can just add more to that after it hits the minimum and We could add more farmers to the to the topology and if we had like some kind of like Liveness probe that was on these farmers then if one of them did go down it would just bring it back up like So that we wouldn't have to intervene on that, you know Yeah, that that is how DCR balance works already. So if we manage to restart the farmers and all that That will already be taken into consideration by DCR balance Now DCR farm once it started with 40 farmers, it doesn't you cannot add more farmers Okay Even if you add farmers They are not going to be part of the of the DCR Okay, that's the only difference and that is so we can We do need to tweak the code in order to do that because right now it has this fallback It goes and check that there is a cache file in S3 if there is none it checks if Ignite has already a cache if Ignite has no cache or is this there's no Ignite It checks how many farmers they are if there are not enough farmers. It will go ahead and do the cache rebuild the non-distributed cache rebuild so we need to allow configuration so that instead of moving on to the Falling back all of this way if it's supposed to be doing DCR and it doesn't have enough farmers It doesn't fall back. It just waits for enough farmers to come up So that code that is a code change that we need to do Also, where is the minimum number of farmers to be defined? It's in In the JSON properties So How many do we need to define for this then to start like we if we keep minimum as 40 That's a two that's a bit too many, right? So you so maybe that number should be like Five or three or something like that Right now the default minimum and we never changed it is two Right, so if so now what you will change the code to be is If we have it will keep waiting for two farmers Right, and that means a synchronizer can be in the wait state where it's gonna complete the history of date And then once it gets the two farmers up, it's gonna start DCR Is that how it's gonna work? And then as more come in the as more come in to the number of replicas So that we have defined say 20 It's gonna allow those 20 those remaining 18 to come up and then balance the work on those remaining nodes, right? That's how Mm-hmm up to a maximum. There's a maximum as well Right, the maximum can be hundred or five hundred or something like that. I think right now it's like 60, but yeah Yeah, so maybe we want to keep it at 100 or 200 so that we can go up to 100 nodes Because that's what we need for the bigger customers. Like we have been playing around with at least 100 power nodes Sorry, not 100, no, I'm wrong. 20 to 40 was the max. 20 to 40 was the max With 60 to 80 60 to 100 gigabytes of heap on them can we make can we can we use the override to Define the minimum and the maximum per express instance like as we like determine like, okay We want only this, you know, three or four express instances to be able to have more than 100 farmers So we'll configure their maximum like in the deployment repo yeah, yeah, it's Actually, you can you can do it in the property files or in the JSON's But I don't need that. It has to just be one limit which everyone can use We don't really need to have that separate for different customers Okay, it's gonna be I mean The way the way our configuration works you cannot do it like that I mean you have the default and if nobody else has an override Then that's going to be the value Right, but the default should just work for us for every use case, right? Like if you have a minimum at two that means it's just gonna wait for two farmers and every every DCR Cluster will have at least two farmers or more for DCR to start because it's otherwise pointless with just two nodes. It's doing nothing I mean, it's not gonna be a much big difference in in speed for cash We will by just running a DCR on two farmers or five farms, right? We try to reduce the time for cash rebills on the big customers. So at a minimum they would need Eight to ten farmer nodes with at least limit of 60 to 80 gigabytes, right? so our default of two being minimum is gonna serve our purpose in that case and a default of say keeping hundred as the maximum is also gonna serve our purpose because Until today we haven't had the need for more than 20 to 30 farmer nodes on our cluster, right? And even in the future we don't need that many more because we are trying to keep the data set size down Or reduce that on our big customers. So 30 to 40 farmer as being our max is what I foresee for now So if we have a limit of hundred as being our max that should solve all our needs in the default settings And then we don't really need to keep Overriding those at all, right? I mean there can be a use case in the future, which we don't foresee But but we can think of that when that time comes right now the default limit should just be fine Yeah, and then like in order and so knowing like what Express cluster like did the last time you could just look at the Get history and see what the replica set Was set to in the values file and then you know exactly what it did last and you can know what to set it to again correct Okay, cool, well, I guess can we Is it pop like I guess pending like the code change that you said? Could we like begin testing of this stat in the static DCR? And Can we begin testing like bringing the synchronizer up node first and then the rest of the replicas No, that is not going to work until we have this code change I Don't know how complicated it will be Because we would have to just pause the startup in a place where nobody is expecting it to pause so Which makes the this thing about the liveness probe or the readiness probe more critical Because now we cannot just trust that eventually is going to come up even if we say it's ready right away We need to be That needs to be More accurate So I would still like to go with the tests for Checking if the synchronizer can read from the s3 and if it can be used as a donor Basically because we don't have much more to test and in the meantime, I will be working in the readiness probes and This waiting for the minimum number of farmers, okay, cool. I Like that idea and we already have tools that are going to say, okay Manual intervention required waiting 30 minutes that mechanism. We have it already. Mm-hmm. So And we know where to look for instructions on how to move forward yeah So something else I wanted to let you guys know that I'm working on I created a Jira for the sprint is to Elongate the workflow wait, like right now when it gets the farm up it turns down and just off but What I feel like is how it should actually work is Until the actual like DCR is completed whether it completes or fails Then then the workflow should stop so that we can know like in one place What work what DC ours are currently running and which ones are failed? Like, you know right now if it failed it just means that it didn't come up at all Like it didn't start whereas, you know I think it'd be more useful to know which is in flight still and which Has not finished yet or which failed then we can know like how to bring it back up like or when to bring it Back up or try it again So, yeah, and I think we also need to Probably change that


---


pump running in that synchronizer node and keeping the data sets live. Then we have options. We have the option of doing a donor, a fast recovery with that thing, or doing another write cache and doing a copy cache with a newer version that it already kept up to date. Okay, okay, cool. Probably they just put another step in the workflow. So if everything starts, you do the DCR, you do the cache write, you kill the farmers and you leave the synchronizer being a guardian of that data set. Yeah, okay. And then you do have another step where you know you don't need it anymore and this way we already took the data set out of that synchronizer. Okay, I think I'll probably do it in phases and for now just focus on just making sure that it ends, the farmers come down and then we leave the synchronizer up and we can make it better later. But for now, I think that would solve the issue ofThat is more than enough. Okay, okay, cool. I'll work on that. One thing to probably consider is that what do we do if that synchronizer goes down for whatever reason, this DCIV update or whatever? Do we want to say, okay, we lost it or we want to get it back up, read its S3 file, keep it updated again with the pump and all that? Yeah, I don't know. I can make it as difficult as possible for someone to do that with the PDB but beyond that, if they can override it, I don't know. Well, that's why I want to test all of these things becauseAnd then, why do we need to keep it up and running for more than a day because that's just adding more complexity to the whole process, right? Once we have it written out andIt depends on, I mean, until we have the whole process automated where we do the DCR, we propagate that cache by a deployment in the rest of the nodes and then we know for sure that we can kill it. I'm only thinking of, since we don't have an automated second part that is a propagation of the cache, then we probably need a manual way to takeWe'll start on the next FE by the tool. It's just that it's not going to continue for all the other nodes. So like, let's assume we have a cluster with four FEs, okay? And we start DCR farm on WFC or some cluster, right? This is going to complete, DCR is going to complete, the caches get copied to the first FE1 in the cluster using the synchronizer. That's going to come up and then probably we need manual deployments to take that forward. But once the first FE has already got these caches and is up, that means we already have the data set from the rebuild caches ready for all the other, right? And then there'sThat will work, yeah. The only, the two cases I was considering is if there's a delay between the time the cache write happens and the first, and that FE is bounced to make sure that we keep that data set as current as possible for as long as possible. But you have a better idea than me. So just the drawback of that is if that time is too long from cache write to FE1 coming up is that it may have to do ICR, correct? There's no other impact to that node. No. WellOkay, now let's assume in your situation, like you're suggesting, is we keep the synchronizer running and there is this long delay when we are trying to write caches out. But then what's the next step? Is that again, write caches out and copy them to the FE1 and then bring up the FE1? Yeah. But then again, so you're to just counter that argument is write caches take hours of some of these bigger customers, right? So that delay will always be there of a few hours. Like let's say Walmart and CVS are taking four hours today to write caches. Yeah, I'm not thinking, I mean, since we don't have that part of the automation thought out as yet, that's why I propose that for now until we know exactly what we want to keep it open-ended and not implement a logic that we are probably not going to use or it's not going to serve our purpose. Okay, okay, that makes sense. Yeah, so then for now we'll just keep it running. We can kill it manually in our test situations. And as we get more data from the testing, we can harden the process for the synchronizer. Yeah, yeah. And then like, I mean, just in the same way that you can set the replicas to whatever from the helm chart, I can set like a Boolean, like bring it up, bring it down. So we could just manually bring that down when we're comfortable with, but we'll have been able to, in the workflow, turn the rest of the farmers down in the meantime. So we don't have all this compute running all the time.

%%


Readiness probes are a feature in Kubernetes, used to determine if a container is ready to serve traffic. The settings you mentioned are indeed part of the readiness probe configuration, and here's a breakdown of all the standard options available:

1. **`initialDelaySeconds`**: The number of seconds after the container has started before the probe is initiated.
2. **`periodSeconds`**: How often (in seconds) to perform the probe. Default is 10 seconds. Minimum value is 1.
3. **`timeoutSeconds`**: Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1.
4. **`successThreshold`**: Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1.
5. **`failureThreshold`**: When a probe fails, Kubernetes will try `failureThreshold` times before giving up. Giving up in case of readiness probe means the Pod will be marked Unready. Defaults to 3. Minimum value is 1.
6. **`httpGet`**: Performs an HTTP GET request to the specified path on the container's IP address. You can specify the port, host, scheme, and HTTP headers.
7. **`tcpSocket`**: Tries to open a TCP socket to the specified port of the container. The container is considered ready if the probe can establish a connection.
8. **`exec`**: Executes a specified command inside the container. The container is considered ready if the command exits with a status code of 0.

These probes help to ensure that traffic does not get routed to a container that isn't ready to handle requests.


- There's an initial wait of 30 seconds.
- Then the probe checks the readiness every 10 seconds.
- Each check waits for a response for up to 5 seconds.
- If it fails 3 times in a row, the container is marked as not ready.
- To recover, the probe must succeed 2 times consecutively.
